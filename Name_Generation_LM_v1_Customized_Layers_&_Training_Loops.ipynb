{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizhieffe/language_model/blob/main/Name_Generation_LM_v1_Customized_Layers_%26_Training_Loops.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1dIZNdUbjFg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJ1M-ed4b2vI"
      },
      "outputs": [],
      "source": [
        "USE_GPU = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3ytFnYUpzMI"
      },
      "source": [
        "# Setup GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjGwfWZzp0mF",
        "outputId": "95a8ff48-0840-4433-9ec6-c22e18468324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "if USE_GPU:\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  assert device != 'cpu', \"GPU is not available\"\n",
        "else:\n",
        "  device = 'cpu'\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzq1BqOrcNcS"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVLA0NP5cO9_",
        "outputId": "843291a8-fd03-4bbc-fd68-22717e87cd2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-09-30 00:15:56--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt’\n",
            "\n",
            "names.txt           100%[===================>] 222.80K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-09-30 00:15:57 (9.03 MB/s) - ‘names.txt’ saved [228145/228145]\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# download the names.txt file from github\n",
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
        "\n",
        "# read in all the words\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "words[:8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQ0NWyEjcWzd",
        "outputId": "e1950bee-2556-4e74-aab7-f0e8d6f92fee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "32033"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMBB3_mZcYcI"
      },
      "source": [
        "# Build vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djgEPheAcai0"
      },
      "outputs": [],
      "source": [
        "chars = sorted(list(set(''.join(words))))\n",
        "\n",
        "stoi = {c:i+1 for i,c in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "\n",
        "itos = {i:c for c,i in stoi.items()}\n",
        "\n",
        "assert len(stoi) == len(itos)\n",
        "assert len(stoi) == 27\n",
        "\n",
        "vocab_size = len(stoi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM8RU8VQdQjN"
      },
      "source": [
        "# Create DS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUMTWVkIdR7S"
      },
      "outputs": [],
      "source": [
        "block_size = 7 # Context length: how many chars do we take to predict the next one?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjq6bpCZdkhA"
      },
      "outputs": [],
      "source": [
        "def build_dataset(words):\n",
        "  X = []\n",
        "  Y = []\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for c in w + '.':\n",
        "      iy = stoi[c]\n",
        "      X.append(context)\n",
        "      Y.append(iy)\n",
        "      context = context[1:] + [iy]\n",
        "\n",
        "  X = torch.tensor(X).to(device)\n",
        "  Y = torch.tensor(Y).to(device)\n",
        "  return X, Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGX20hCXirCj",
        "outputId": "d4c626c0-282b-45eb-cbd6-8a44969485f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total size = 32033, n1 = 19219, n2 = 25626\n",
            "Xtr.shape=torch.Size([137024, 7]), Ytr.shape=torch.Size([137024])\n",
            "Xdev.shape=torch.Size([45601, 7]), Ydev.shape=torch.Size([45601])\n",
            "Xte.shape=torch.Size([45521, 7]), Yte.shape=torch.Size([45521])\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "\n",
        "n1 = int(0.6 * len(words))\n",
        "n2 = int(0.8 * len(words))\n",
        "\n",
        "print(f'total size = {len(words)}, n1 = {n1}, n2 = {n2}')\n",
        "\n",
        "Xtr, Ytr = build_dataset(words[:n1])\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])\n",
        "Xte, Yte = build_dataset(words[n2:])\n",
        "\n",
        "assert Xtr.shape[0] == Ytr.shape[0]\n",
        "assert Xdev.shape[0] == Ydev.shape[0]\n",
        "assert Xte.shape[0] == Yte.shape[0]\n",
        "\n",
        "print(f'{Xtr.shape=}, {Ytr.shape=}')\n",
        "print(f'{Xdev.shape=}, {Ydev.shape=}')\n",
        "print(f'{Xte.shape=}, {Yte.shape=}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oARP3rckeVFe",
        "outputId": "13de7b6d-49d4-49f2-e3bb-0cbb04c9759b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "....... ---> y\n",
            "......y ---> u\n",
            ".....yu ---> h\n",
            "....yuh ---> e\n",
            "...yuhe ---> n\n",
            "..yuhen ---> g\n",
            ".yuheng ---> .\n",
            "....... ---> d\n",
            "......d ---> i\n",
            ".....di ---> o\n",
            "....dio ---> n\n",
            "...dion ---> d\n",
            "..diond ---> r\n",
            ".diondr ---> e\n",
            "diondre ---> .\n",
            "....... ---> x\n",
            "......x ---> a\n",
            ".....xa ---> v\n",
            "....xav ---> i\n",
            "...xavi ---> e\n"
          ]
        }
      ],
      "source": [
        "for i in range(20):\n",
        "  print(f\"{''.join(itos[ix.item()] for ix in Xtr[i])} ---> {itos[Ytr[i].item()]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aEpC4irkYEJ"
      },
      "source": [
        "# MLP Revisited"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAvWn29RkZtt"
      },
      "source": [
        "## Model params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p-7-hDykbji",
        "outputId": "742b51d6-11b9-43de-a219-82448102dfd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape = torch.Size([27, 10])\n",
            "shape = torch.Size([70, 400])\n",
            "shape = torch.Size([400])\n",
            "shape = torch.Size([400, 400])\n",
            "shape = torch.Size([400])\n",
            "shape = torch.Size([400, 400])\n",
            "shape = torch.Size([400])\n",
            "shape = torch.Size([400, 400])\n",
            "shape = torch.Size([400])\n",
            "shape = torch.Size([400, 27])\n",
            "shape = torch.Size([27])\n",
            "\n",
            "total parameters = 520697\n"
          ]
        }
      ],
      "source": [
        "n_emb = 10\n",
        "n_hidden = 400\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C = torch.randn((vocab_size, n_emb), generator=g).to(device)\n",
        "W1 = torch.randn((block_size * n_emb, n_hidden), generator=g).to(device)\n",
        "b1 = torch.randn(n_hidden, generator=g).to(device)\n",
        "W2 = torch.randn((n_hidden, n_hidden), generator=g).to(device)\n",
        "b2 = torch.randn(n_hidden, generator=g).to(device)\n",
        "W3 = torch.randn((n_hidden, n_hidden), generator=g).to(device)\n",
        "b3 = torch.randn(n_hidden, generator=g).to(device)\n",
        "W4 = torch.randn((n_hidden, n_hidden), generator=g).to(device)\n",
        "b4 = torch.randn(n_hidden, generator=g).to(device)\n",
        "Wl = torch.randn((n_hidden, vocab_size), generator=g).to(device)\n",
        "bl = torch.randn(vocab_size, generator=g).to(device)\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, W3, b3, W4, b4, Wl, bl]\n",
        "for p in parameters:\n",
        "  print(f'shape = {p.shape}')\n",
        "\n",
        "n_parameters = sum(p.nelement() for p in parameters)\n",
        "print()\n",
        "print(f'total parameters = {n_parameters}')\n",
        "\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BuE1PxomWDj"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVbEWsD0mguM"
      },
      "outputs": [],
      "source": [
        "max_steps = 900_000\n",
        "batch_size = 8224\n",
        "total_loss_print_steps = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsiaVLXbml4F",
        "outputId": "27ba991b-a2c9-4813-afec-ae3a92c4e304"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0/900000: 36.7427\n",
            "18000/900000: 3.4929\n"
          ]
        }
      ],
      "source": [
        "lossi = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g).to(device)\n",
        "  Xb, Yb = Xtr[ix], Ytr[ix]\n",
        "\n",
        "  # forward pass\n",
        "  xemb = C[Xb]\n",
        "  h = xemb.view(xemb.shape[0], -1) @ W1 + b1\n",
        "  h = torch.tanh(h)\n",
        "  h = torch.tanh(h @ W2 + b2)\n",
        "  h = torch.tanh(h @ W3 + b3)\n",
        "  h = torch.tanh(h @ W4 + b4)\n",
        "\n",
        "  logits = h @ Wl + bl\n",
        "  loss = F.cross_entropy(logits, Yb, )\n",
        "  lossi.append(loss.log10().item())\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  if i < 20_000:\n",
        "    lr = 0.1\n",
        "  else:\n",
        "    lr = 0.01\n",
        "\n",
        "  for p in parameters:\n",
        "    p.data -= lr * p.grad\n",
        "\n",
        "  if i % (max_steps / total_loss_print_steps) == 0:\n",
        "    print(f'{i}/{max_steps}: {loss.item():.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCvHZ1kIrMUb"
      },
      "outputs": [],
      "source": [
        "plt.plot(lossi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7IRfWMbuVv_"
      },
      "source": [
        "## Sample the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ctaswnoucvt"
      },
      "outputs": [],
      "source": [
        "g_gpu = torch.Generator(device=device).manual_seed(2147483647) # for reproducibility\n",
        "\n",
        "out = []\n",
        "for _ in range(30):\n",
        "  context = [0] * block_size\n",
        "\n",
        "  str = ''\n",
        "  while True:\n",
        "\n",
        "    xemb = C[torch.tensor([context])]\n",
        "    h = xemb.view(xemb.shape[0], -1) @ W1 + b1\n",
        "    h = torch.tanh(h)\n",
        "    h = torch.tanh(h @ W2 + b2)\n",
        "    logits = h @ Wl + bl\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    iy = torch.multinomial(probs, num_samples=1, replacement=True, generator=g_gpu)\n",
        "\n",
        "    if iy == 0:\n",
        "      str\n",
        "      out.append(str)\n",
        "      break\n",
        "    else:\n",
        "      context = context[1:] + [iy]\n",
        "      str += itos[iy.item()]\n",
        "\n",
        "for w in out:\n",
        "  print(w)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXpGb6MzvYx9"
      },
      "outputs": [],
      "source": [
        "xemb.shape"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMneJxtzKp/CFcNHEMwfjR2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}