{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHIL1PQP9YPppLQYOFmg6y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizhieffe/language_model/blob/main/Shakespeare_LM_v5_Causal_Mask_%2B_Customized_Transformer_%2B_subword_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "fRMxsDXmpbMf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "USE_GPU = True\n",
        "\n",
        "BLOCK_SIZE = 96 # Context length: how many chars do we take to predict the next one?"
      ],
      "metadata": {
        "id": "vDHUDjtsph4G"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup GPU"
      ],
      "metadata": {
        "id": "NZIvnPdcps3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_GPU:\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  assert device.type != 'cpu', \"GPU is not available\"\n",
        "else:\n",
        "  device = 'cpu'\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sd95wthQptwk",
        "outputId": "6f5634e9-4c3a-4e57-f132-3f5f2cf3dfe4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g_cpu = torch.Generator(device='cpu').manual_seed(2147483647) # for reproducibility\n",
        "g_device = torch.Generator(device=device).manual_seed(2147483647) # for reproducibility"
      ],
      "metadata": {
        "id": "FDyu_mmdxQNC"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Util Functions"
      ],
      "metadata": {
        "id": "hHD6RNbQpv4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "CXSgUDhjdCxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the names.txt file from github\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "# read in all the words\n",
        "text = open('input.txt', 'r').read()\n",
        "\n",
        "text[:800]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "Uw1N2UjndEDQ",
        "outputId": "21d7c13d-2826-4658-ee70-9d4517c3afd5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-03 05:58:05--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-10-03 05:58:05 (19.3 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to p\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'total char # = {len(text)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_Lr-G6NdF62",
        "outputId": "cba97637-8670-4d82-9852-2b41bc218c64"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total char # = 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup tokenizer"
      ],
      "metadata": {
        "id": "ZFA5J8NjdHj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup tokenizer\n",
        "\n",
        "!pip install tokenizers # Since we are running in colab docker image, install it here.\n",
        "# See https://anaconda.org/conda-forge/tokenizers\n",
        "from tokenizers import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JStJ6DOrCIIn",
        "outputId": "118b5de2-cfd4-4744-b10f-e31b7b544588"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
            "Requirement already satisfied: huggingface_hub<0.17,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.16.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (23.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = tokenizer.encode(\"I can feel the magic, can you, what's that bro?\")\n",
        "\n",
        "test.ids, test.type_ids, test.tokens, test.offsets\n",
        "\n",
        "# testto"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLfEt32uC6Qn",
        "outputId": "6b9d3303-6c98-4962-8d95-c886fee3791c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([101,\n",
              "  146,\n",
              "  1169,\n",
              "  1631,\n",
              "  1103,\n",
              "  3974,\n",
              "  117,\n",
              "  1169,\n",
              "  1128,\n",
              "  117,\n",
              "  1184,\n",
              "  112,\n",
              "  188,\n",
              "  1115,\n",
              "  9304,\n",
              "  1186,\n",
              "  136,\n",
              "  102],\n",
              " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " ['[CLS]',\n",
              "  'I',\n",
              "  'can',\n",
              "  'feel',\n",
              "  'the',\n",
              "  'magic',\n",
              "  ',',\n",
              "  'can',\n",
              "  'you',\n",
              "  ',',\n",
              "  'what',\n",
              "  \"'\",\n",
              "  's',\n",
              "  'that',\n",
              "  'br',\n",
              "  '##o',\n",
              "  '?',\n",
              "  '[SEP]'],\n",
              " [(0, 0),\n",
              "  (0, 1),\n",
              "  (2, 5),\n",
              "  (6, 10),\n",
              "  (11, 14),\n",
              "  (15, 20),\n",
              "  (20, 21),\n",
              "  (22, 25),\n",
              "  (26, 29),\n",
              "  (29, 30),\n",
              "  (31, 35),\n",
              "  (35, 36),\n",
              "  (36, 37),\n",
              "  (38, 42),\n",
              "  (43, 45),\n",
              "  (45, 46),\n",
              "  (46, 47),\n",
              "  (0, 0)])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = tokenizer.get_vocab_size()\n",
        "print(f'{vocab_size=}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OkzEoS1DKxN",
        "outputId": "1e25bf53-d037-45df-ffb2-97e4219b5b0b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size=28996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# str ---> list of integer\n",
        "# encode = tokenizer.encode\n",
        "encode = lambda str: tokenizer.encode(str).ids\n",
        "\n",
        "# list of integer ---> str\n",
        "decode = tokenizer.decode\n",
        "\n",
        "_test_str = \"adb dfd \\nplace\"\n",
        "\n",
        "# TODO: this fails  because the tokenizer's encoding result doesn't include \"\\n\"\n",
        "# assert _test_str == decode(encode(_test_str).ids)"
      ],
      "metadata": {
        "id": "_wjY-ZmAK_Mt"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create DS"
      ],
      "metadata": {
        "id": "tnC-XqfXdKoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n1 = int(len(text) * 0.9)\n",
        "train_data = encode(text[:n1])\n",
        "dev_data = encode(text[n1:])\n",
        "\n",
        "print(f'{len(train_data)=}, {len(dev_data)=}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vU05Ihvwl_b",
        "outputId": "083b575e-9604-44c9-e289-ae5a3f6f15c2"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(train_data)=283186, len(dev_data)=33357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(data, batch_size: int, block_size: int):\n",
        "  \"\"\" Sample a batch using Causal style. \"\"\"\n",
        "  x_list = []\n",
        "  y_list = []\n",
        "  ix = torch.randint(0, len(data)-block_size, (batch_size,), generator=g_cpu)\n",
        "  X = torch.stack([torch.tensor(data[i:i+block_size]) for i in ix]).to(device)\n",
        "  Y = torch.stack([torch.tensor(data[i+1:i+1+block_size]) for i in ix]).to(device)\n",
        "  return X, Y"
      ],
      "metadata": {
        "id": "eWvuZyaOwXai"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = get_batch(train_data, 16, BLOCK_SIZE)\n",
        "\n",
        "for b in range(3):\n",
        "  it = 0\n",
        "  for t in range(X.shape[1]):\n",
        "    print(f'{decode(X[b, :t+1].tolist())} ---> {decode([Y[b, t].item()])}')\n",
        "    it += 1\n",
        "    if it > 7:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2EL4GRCyDcr",
        "outputId": "5e22359c-ab96-42bf-d348-a4013bce87f8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "King ---> .\n",
            "King. ---> '\n",
            "King. ' ---> My\n",
            "King. ' My ---> dangerous\n",
            "King. ' My dangerous ---> cousin\n",
            "King. ' My dangerous cousin ---> ,\n",
            "King. ' My dangerous cousin, ---> let\n",
            "King. ' My dangerous cousin, let ---> your\n",
            "dear ---> dear\n",
            "dear dear ---> lord\n",
            "dear dear lord ---> ,\n",
            "dear dear lord, ---> The\n",
            "dear dear lord, The ---> pure\n",
            "dear dear lord, The pure ---> ##st\n",
            "dear dear lord, The purest ---> treasure\n",
            "dear dear lord, The purest treasure ---> mortal\n",
            "as ---> I\n",
            "as I ---> guess\n",
            "as I guess ---> ,\n",
            "as I guess, ---> To\n",
            "as I guess, To ---> make\n",
            "as I guess, To make ---> a\n",
            "as I guess, To make a ---> bloody\n",
            "as I guess, To make a bloody ---> supper\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "bm2SpJ6CfOHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_embd = 768 # the dimensionality of the character embedding vectors\n",
        "d_head = 2 * n_embd # the dim of the transformer's head\n",
        "N_HIDDEN = n_embd * 4 # the number of neurons in the hidden layer of the MLP\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility"
      ],
      "metadata": {
        "id": "lAE6DUi2qcNO"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model class"
      ],
      "metadata": {
        "id": "o2LWaIYbri0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(torch.nn.Module):\n",
        "  \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
        "\n",
        "  def __init__(self, ndim: int, bias: bool):\n",
        "    super().__init__()\n",
        "    self.weight = torch.nn.Parameter(torch.ones(ndim))\n",
        "    self.bias = torch.nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "  def forward(self, x):\n",
        "    return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)"
      ],
      "metadata": {
        "id": "R-mApi-YzvvD"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, d_in, d_hidden, d_head):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      vocab_size: volabulary size\n",
        "      d_in: dim of input. If this is the immediate next layer of the token\n",
        "        embedding layer, this is the dim of the embedding for a token.\n",
        "      d_hidden: dim of hidden FFN layers\n",
        "      d_head: dim of the transformer head\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_head = d_head\n",
        "    self.key1 = torch.nn.Linear(d_in, d_head, bias=False)\n",
        "    self.query1 = torch.nn.Linear(d_in, d_head, bias=False)\n",
        "    self.value1 = torch.nn.Linear(d_in, d_head, bias=False)\n",
        "\n",
        "    self.linear1 = torch.nn.Linear(d_head, d_hidden, bias=True)\n",
        "    self.tanh1 = torch.nn.Tanh()\n",
        "    # Project d_hidden back to d_head as output\n",
        "    self.proj = torch.nn.Linear(d_hidden, d_head, bias=True)\n",
        "\n",
        "    self.ln1 = LayerNorm(d_in, bias=True)\n",
        "    self.ln2 = LayerNorm(d_head, bias=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      x: (B, T, C). The input to the model.\n",
        "    Returns:\n",
        "      loss: int\n",
        "      logits: (B, T, C)\n",
        "    \"\"\"\n",
        "\n",
        "    T = x.shape[-2]\n",
        "    tril = torch.tril(torch.ones(T, T)).to(device)\n",
        "\n",
        "    x = self.ln1(x)\n",
        "\n",
        "    k = self.key1(x)  # (B, T, d_head)\n",
        "    q = self.query1(x) # (B, T, d_head)\n",
        "    wei = k @ q.transpose(-2, -1) # (B, T, d_head) @ (B, d_head, T) = (B, T, T)\n",
        "    wei = wei.masked_fill(tril==0, -float('inf')) # (B, T, T)\n",
        "    wei = wei * self.d_head**-0.5\n",
        "    wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "\n",
        "    v = self.value1(x) # (B, T, d_head)\n",
        "    # This makes the y[:,t,:], to have the information of the embedding (v)\n",
        "    # v[:, u (u<=t), :], but not have the infomration of v[:, w (w>t), :]\n",
        "    #\n",
        "    # 1. Spread v's information at all t to y[:, any t, :]\n",
        "    #\n",
        "    # Because v is on rhs of @, its information at different T are spread out to\n",
        "    # the different T in y\n",
        "    #\n",
        "    # Think about (T, T) @ (T, d_head) = (T, d_head)\n",
        "    #\n",
        "    # a11, a12     b1   a11*b1+a12*b2\n",
        "    # a21, a22  @  b2 = a21*b1+a22*b2\n",
        "    #\n",
        "    # In the result, at T=1, it has b1 and b2, which are the rhs of @'s info at\n",
        "    # different T\n",
        "    #\n",
        "    # 2. Limit y[:, t, :] to not access v[:, w (w>t), :].\n",
        "    #\n",
        "    # This is done by `tril`\n",
        "    y = wei @ v # (B, T, d_head)\n",
        "\n",
        "    y = self.ln2(y)\n",
        "\n",
        "    # It doesn't need tril here, because the lhs and rhs doesn't exchange\n",
        "    # information at different T.\n",
        "    #\n",
        "    # Let's say:\n",
        "    # - input is y (B, T, d_head)\n",
        "    # - Linear(d_in, d_out) is a matrix l(d_in, d_out), here dim is l(d_head, d_hidden)\n",
        "    # - result is z (B, T, d_hidden)\n",
        "    #\n",
        "    # linear(y) = y @ l = z\n",
        "    #\n",
        "    # To simplify, ignore B, T=3, d_head=2, d_hidden=1\n",
        "    #\n",
        "    #         y11 y12         y11*l1+y12*l2\n",
        "    # y @ l = y21 y22 @ l1  = y21*l1+y22*l2\n",
        "    #         y31 y32   l2    y31*l1+y32*l2\n",
        "    #\n",
        "    # We can see z[:, T, :] only contains y[:, T, :]'s info\n",
        "    #\n",
        "    # To summarize this and the previous section\n",
        "    #\n",
        "    # Z = X @ Y\n",
        "    #\n",
        "    # Z[T, :] only contains X[T, :]'s info, doesn't contain X[S != T, :]'s infor\n",
        "    # Z[T, :] contains Y[at any index, :]'s info\n",
        "    y = self.linear1(y) # (B, T, d_hidden)\n",
        "    y = self.tanh1(y) # (B, T, d_hidden)\n",
        "    y = self.proj(y) # (B, T, d_head)\n",
        "\n",
        "    return y\n",
        "\n",
        "  def generate(self, idx, max_new_tokens: int):\n",
        "    \"\"\"Generate new tokens given idx as context.\n",
        "\n",
        "    Args:\n",
        "      idx: (B, T)\n",
        "      max_new_tokens: number of new tokens to generate\n",
        "\n",
        "    Returns:\n",
        "      (B,T+max_new_tokens)\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, loss = self(idx, targets=None)  # logits: (B, T, vocab_size), loss: None\n",
        "      logits = logits[:, -1, :] # (B, vocab_size)\n",
        "      prob = F.softmax(logits, dim=1) # (B, vocab_size)\n",
        "      idx_next = torch.multinomial(prob, num_samples=1, replacement=False)  # (B, 1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "LT6iVQP14kXa"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, d_embd, d_hidden, d_head):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      vocab_size: volabulary size\n",
        "      d_embd: dim of embedding for the token\n",
        "      d_hidden: dim of hidden FFN layers\n",
        "      d_head: dim of the transformer head\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_head = d_head\n",
        "\n",
        "    self.embd = torch.nn.Embedding(\n",
        "        num_embeddings=vocab_size,\n",
        "        embedding_dim=d_embd\n",
        "    )\n",
        "    self.attn1 = AttentionBlock(vocab_size, d_embd, d_hidden, d_head)\n",
        "    self.attn2 = AttentionBlock(vocab_size, d_head, d_hidden, d_head)\n",
        "    self.linear_logit = torch.nn.Linear(d_head, vocab_size, bias=True)\n",
        "\n",
        "\n",
        "  def forward(self, x, targets=None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      x: (B, T). The input to the model.\n",
        "      targets: (B, T). When it is not None, the func calculates and return the\n",
        "        loss in additional to other returned item(s)\n",
        "    Returns:\n",
        "      loss: int\n",
        "      logits: (B, T, C)\n",
        "    \"\"\"\n",
        "\n",
        "    T = x.shape[-1]\n",
        "    tril = torch.tril(torch.ones(T, T)).to(device)\n",
        "\n",
        "    xemb = self.embd(x) # (B, T, C)\n",
        "\n",
        "    y = self.attn1(xemb)\n",
        "    y = self.attn2(y)\n",
        "\n",
        "    logits = self.linear_logit(y) # (B, T, vocab_size)\n",
        "    logits = logits.view(-1, logits.shape[-1]) # (B, T, vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      loss = F.cross_entropy(logits, targets.view(-1))\n",
        "\n",
        "    return logits.view(-1, T, logits.shape[1]), loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens: int):\n",
        "    \"\"\"Generate new tokens given idx as context.\n",
        "\n",
        "    Args:\n",
        "      idx: (B, T)\n",
        "      max_new_tokens: number of new tokens to generate\n",
        "\n",
        "    Returns:\n",
        "      (B, T+max_new_tokens)\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, loss = self(idx, targets=None)  # logits: (B, T, vocab_size), loss: None\n",
        "      logits = logits[:, -1, :] # (B, vocab_size)\n",
        "      prob = F.softmax(logits, dim=1) # (B, vocab_size)\n",
        "      idx_next = torch.multinomial(prob, num_samples=1, replacement=False)  # (B, 1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "net = Net(vocab_size, d_embd=n_embd, d_hidden=N_HIDDEN, d_head=N_HIDDEN).to(device)"
      ],
      "metadata": {
        "id": "6QusOiY5HEBH"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_total_params = 0\n",
        "\n",
        "for p in net.parameters():\n",
        "  _total_params += p.nelement()\n",
        "\n",
        "print(f'Total params = {_total_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eMi8TezrkY4",
        "outputId": "0c440e8c-b471-4896-c962-7bb430d6ae9a"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total params = 184544068\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define optimizer"
      ],
      "metadata": {
        "id": "m04BZ-9f26w7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "# optimizer = optim.Adam(net.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "5xJsIU2z28us"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "5ObPwkVj3MFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_steps = 700000\n",
        "batch_size = 16\n",
        "lossi = []\n",
        "lossi_dev = []\n",
        "ud = []\n",
        "log_interval = 50\n",
        "\n",
        "running_loss = 0.0\n",
        "running_loss_dev = 0.0\n",
        "running_loss_steps = 0\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # Forward\n",
        "  Xb, Yb = get_batch(train_data, batch_size, BLOCK_SIZE)\n",
        "  optimizer.zero_grad()\n",
        "  logits, loss = net(Xb, targets=Yb)\n",
        "\n",
        "  # Loss\n",
        "  # print(f'{outputs.shape=}, {Yb.shape=}')\n",
        "  running_loss += loss.item()\n",
        "  running_loss_steps += 1\n",
        "\n",
        "  # Eval dev DS\n",
        "  Xb_dev, Yb_dev = get_batch(dev_data, batch_size, BLOCK_SIZE)\n",
        "  logits_dev, loss_dev = net(Xb_dev, targets=Yb_dev)\n",
        "  running_loss_dev += loss_dev.item()\n",
        "\n",
        "  # Update\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # Track status\n",
        "  if i % log_interval == 0:\n",
        "    print(f'{i}/{max_steps}: training loss={running_loss/running_loss_steps:.4f}, dev loss={running_loss_dev/running_loss_steps:.4f}')\n",
        "    running_loss = 0.0\n",
        "    running_loss_dev = 0.0\n",
        "    running_loss_steps = 0\n",
        "\n",
        "  lossi.append(loss.log10().item())\n",
        "  lossi_dev.append(loss_dev.log10().item())"
      ],
      "metadata": {
        "id": "3BtGf_U93M9V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4b517bd6-2ace-4b43-f79c-39e105c2d981"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0/700000: training loss=10.2902, dev loss=10.2917\n",
            "50/700000: training loss=9.7995, dev loss=9.7726\n",
            "100/700000: training loss=8.3527, dev loss=8.3291\n",
            "150/700000: training loss=7.6600, dev loss=7.6613\n",
            "200/700000: training loss=7.3409, dev loss=7.3684\n",
            "250/700000: training loss=7.1840, dev loss=7.2077\n",
            "300/700000: training loss=7.0567, dev loss=7.1158\n",
            "350/700000: training loss=6.9703, dev loss=7.0544\n",
            "400/700000: training loss=6.9225, dev loss=6.9978\n",
            "450/700000: training loss=6.8799, dev loss=6.9878\n",
            "500/700000: training loss=6.8334, dev loss=6.9570\n",
            "550/700000: training loss=6.8332, dev loss=6.9249\n",
            "600/700000: training loss=6.7843, dev loss=6.9130\n",
            "650/700000: training loss=6.7752, dev loss=6.9110\n",
            "700/700000: training loss=6.7817, dev loss=6.9145\n",
            "750/700000: training loss=6.7390, dev loss=6.8784\n",
            "800/700000: training loss=6.7367, dev loss=6.8474\n",
            "850/700000: training loss=6.7394, dev loss=6.8709\n",
            "900/700000: training loss=6.7267, dev loss=6.8415\n",
            "950/700000: training loss=6.7043, dev loss=6.8407\n",
            "1000/700000: training loss=6.6875, dev loss=6.8021\n",
            "1050/700000: training loss=6.6897, dev loss=6.8146\n",
            "1100/700000: training loss=6.6781, dev loss=6.7791\n",
            "1150/700000: training loss=6.6558, dev loss=6.7648\n",
            "1200/700000: training loss=6.6319, dev loss=6.7528\n",
            "1250/700000: training loss=6.6221, dev loss=6.7126\n",
            "1300/700000: training loss=6.6046, dev loss=6.7050\n",
            "1350/700000: training loss=6.5810, dev loss=6.7162\n",
            "1400/700000: training loss=6.5759, dev loss=6.6935\n",
            "1450/700000: training loss=6.5240, dev loss=6.6588\n",
            "1500/700000: training loss=6.5464, dev loss=6.6407\n",
            "1550/700000: training loss=6.5028, dev loss=6.6345\n",
            "1600/700000: training loss=6.4795, dev loss=6.6155\n",
            "1650/700000: training loss=6.4692, dev loss=6.5959\n",
            "1700/700000: training loss=6.4383, dev loss=6.5720\n",
            "1750/700000: training loss=6.4556, dev loss=6.5706\n",
            "1800/700000: training loss=6.4121, dev loss=6.5592\n",
            "1850/700000: training loss=6.3828, dev loss=6.5457\n",
            "1900/700000: training loss=6.3835, dev loss=6.5445\n",
            "1950/700000: training loss=6.3686, dev loss=6.5368\n",
            "2000/700000: training loss=6.3634, dev loss=6.5271\n",
            "2050/700000: training loss=6.3295, dev loss=6.4927\n",
            "2100/700000: training loss=6.3345, dev loss=6.5175\n",
            "2150/700000: training loss=6.3610, dev loss=6.4949\n",
            "2200/700000: training loss=6.3164, dev loss=6.4633\n",
            "2250/700000: training loss=6.3027, dev loss=6.4754\n",
            "2300/700000: training loss=6.3076, dev loss=6.4679\n",
            "2350/700000: training loss=6.3421, dev loss=6.4775\n",
            "2400/700000: training loss=6.2934, dev loss=6.4548\n",
            "2450/700000: training loss=6.2879, dev loss=6.4265\n",
            "2500/700000: training loss=6.3002, dev loss=6.4373\n",
            "2550/700000: training loss=6.2648, dev loss=6.4178\n",
            "2600/700000: training loss=6.2837, dev loss=6.4438\n",
            "2650/700000: training loss=6.2109, dev loss=6.3867\n",
            "2700/700000: training loss=6.2697, dev loss=6.3889\n",
            "2750/700000: training loss=6.2202, dev loss=6.3821\n",
            "2800/700000: training loss=6.2133, dev loss=6.3819\n",
            "2850/700000: training loss=6.2251, dev loss=6.3750\n",
            "2900/700000: training loss=6.2080, dev loss=6.3453\n",
            "2950/700000: training loss=6.2082, dev loss=6.3643\n",
            "3000/700000: training loss=6.1866, dev loss=6.3600\n",
            "3050/700000: training loss=6.1408, dev loss=6.3316\n",
            "3100/700000: training loss=6.1973, dev loss=6.3320\n",
            "3150/700000: training loss=6.1947, dev loss=6.3852\n",
            "3200/700000: training loss=6.1781, dev loss=6.3374\n",
            "3250/700000: training loss=6.1489, dev loss=6.3355\n",
            "3300/700000: training loss=6.1662, dev loss=6.3094\n",
            "3350/700000: training loss=6.1076, dev loss=6.3163\n",
            "3400/700000: training loss=6.0959, dev loss=6.2937\n",
            "3450/700000: training loss=6.1618, dev loss=6.2995\n",
            "3500/700000: training loss=6.1255, dev loss=6.3009\n",
            "3550/700000: training loss=6.1368, dev loss=6.3009\n",
            "3600/700000: training loss=6.1345, dev loss=6.3021\n",
            "3650/700000: training loss=6.1267, dev loss=6.3062\n",
            "3700/700000: training loss=6.0988, dev loss=6.3126\n",
            "3750/700000: training loss=6.1186, dev loss=6.2934\n",
            "3800/700000: training loss=6.1015, dev loss=6.2842\n",
            "3850/700000: training loss=6.0863, dev loss=6.2874\n",
            "3900/700000: training loss=6.0902, dev loss=6.2832\n",
            "3950/700000: training loss=6.0979, dev loss=6.2821\n",
            "4000/700000: training loss=6.0812, dev loss=6.2895\n",
            "4050/700000: training loss=6.1125, dev loss=6.2669\n",
            "4100/700000: training loss=6.0815, dev loss=6.2704\n",
            "4150/700000: training loss=6.0785, dev loss=6.2463\n",
            "4200/700000: training loss=6.0813, dev loss=6.2380\n",
            "4250/700000: training loss=6.0316, dev loss=6.2824\n",
            "4300/700000: training loss=6.0191, dev loss=6.2290\n",
            "4350/700000: training loss=6.0513, dev loss=6.2403\n",
            "4400/700000: training loss=6.0604, dev loss=6.2228\n",
            "4450/700000: training loss=6.0528, dev loss=6.2417\n",
            "4500/700000: training loss=6.0314, dev loss=6.2523\n",
            "4550/700000: training loss=6.0049, dev loss=6.2284\n",
            "4600/700000: training loss=6.0009, dev loss=6.2282\n",
            "4650/700000: training loss=6.0196, dev loss=6.2253\n",
            "4700/700000: training loss=5.9958, dev loss=6.2280\n",
            "4750/700000: training loss=6.0006, dev loss=6.2036\n",
            "4800/700000: training loss=6.0047, dev loss=6.2504\n",
            "4850/700000: training loss=5.9753, dev loss=6.1900\n",
            "4900/700000: training loss=5.9934, dev loss=6.1960\n",
            "4950/700000: training loss=5.9982, dev loss=6.1830\n",
            "5000/700000: training loss=5.9653, dev loss=6.1761\n",
            "5050/700000: training loss=5.9428, dev loss=6.1938\n",
            "5100/700000: training loss=5.9258, dev loss=6.2215\n",
            "5150/700000: training loss=5.9766, dev loss=6.1740\n",
            "5200/700000: training loss=5.9186, dev loss=6.1891\n",
            "5250/700000: training loss=5.9037, dev loss=6.1766\n",
            "5300/700000: training loss=5.9149, dev loss=6.1617\n",
            "5350/700000: training loss=5.9421, dev loss=6.1640\n",
            "5400/700000: training loss=5.9231, dev loss=6.1599\n",
            "5450/700000: training loss=5.9339, dev loss=6.1807\n",
            "5500/700000: training loss=5.9058, dev loss=6.1613\n",
            "5550/700000: training loss=5.8893, dev loss=6.1720\n",
            "5600/700000: training loss=5.9159, dev loss=6.1314\n",
            "5650/700000: training loss=5.8948, dev loss=6.1507\n",
            "5700/700000: training loss=5.8910, dev loss=6.1357\n",
            "5750/700000: training loss=5.8240, dev loss=6.1666\n",
            "5800/700000: training loss=5.8211, dev loss=6.1569\n",
            "5850/700000: training loss=5.8218, dev loss=6.1385\n",
            "5900/700000: training loss=5.8382, dev loss=6.1237\n",
            "5950/700000: training loss=5.8434, dev loss=6.1021\n",
            "6000/700000: training loss=5.8311, dev loss=6.1234\n",
            "6050/700000: training loss=5.8035, dev loss=6.0818\n",
            "6100/700000: training loss=5.8193, dev loss=6.1166\n",
            "6150/700000: training loss=5.8113, dev loss=6.1015\n",
            "6200/700000: training loss=5.7733, dev loss=6.0945\n",
            "6250/700000: training loss=5.7667, dev loss=6.1133\n",
            "6300/700000: training loss=5.7773, dev loss=6.0817\n",
            "6350/700000: training loss=5.7878, dev loss=6.0864\n",
            "6400/700000: training loss=5.7759, dev loss=6.1276\n",
            "6450/700000: training loss=5.8110, dev loss=6.1105\n",
            "6500/700000: training loss=5.7352, dev loss=6.0998\n",
            "6550/700000: training loss=5.7372, dev loss=6.0687\n",
            "6600/700000: training loss=5.7373, dev loss=6.0678\n",
            "6650/700000: training loss=5.7430, dev loss=6.0607\n",
            "6700/700000: training loss=5.7161, dev loss=6.0695\n",
            "6750/700000: training loss=5.7195, dev loss=6.0375\n",
            "6800/700000: training loss=5.7197, dev loss=6.0663\n",
            "6850/700000: training loss=5.6794, dev loss=6.0513\n",
            "6900/700000: training loss=5.6942, dev loss=6.0608\n",
            "6950/700000: training loss=5.6393, dev loss=6.0604\n",
            "7000/700000: training loss=5.6929, dev loss=6.0335\n",
            "7050/700000: training loss=5.6803, dev loss=6.0082\n",
            "7100/700000: training loss=5.6725, dev loss=6.0546\n",
            "7150/700000: training loss=5.6588, dev loss=6.0495\n",
            "7200/700000: training loss=5.6598, dev loss=6.0344\n",
            "7250/700000: training loss=5.6454, dev loss=6.0101\n",
            "7300/700000: training loss=5.6250, dev loss=6.0191\n",
            "7350/700000: training loss=5.6269, dev loss=6.0109\n",
            "7400/700000: training loss=5.5916, dev loss=6.0190\n",
            "7450/700000: training loss=5.5832, dev loss=6.0079\n",
            "7500/700000: training loss=5.6253, dev loss=6.0111\n",
            "7550/700000: training loss=5.5795, dev loss=6.0382\n",
            "7600/700000: training loss=5.5848, dev loss=5.9771\n",
            "7650/700000: training loss=5.5877, dev loss=5.9377\n",
            "7700/700000: training loss=5.5957, dev loss=5.9797\n",
            "7750/700000: training loss=5.5782, dev loss=5.9916\n",
            "7800/700000: training loss=5.5478, dev loss=5.9386\n",
            "7850/700000: training loss=5.5634, dev loss=5.9526\n",
            "7900/700000: training loss=5.5331, dev loss=5.9436\n",
            "7950/700000: training loss=5.5519, dev loss=5.9127\n",
            "8000/700000: training loss=5.5237, dev loss=5.9383\n",
            "8050/700000: training loss=5.5388, dev loss=5.9441\n",
            "8100/700000: training loss=5.5315, dev loss=5.9134\n",
            "8150/700000: training loss=5.4778, dev loss=5.9301\n",
            "8200/700000: training loss=5.4738, dev loss=5.9371\n",
            "8250/700000: training loss=5.4859, dev loss=5.9540\n",
            "8300/700000: training loss=5.4636, dev loss=5.9132\n",
            "8350/700000: training loss=5.4583, dev loss=5.9136\n",
            "8400/700000: training loss=5.4691, dev loss=5.9479\n",
            "8450/700000: training loss=5.4846, dev loss=5.8906\n",
            "8500/700000: training loss=5.4814, dev loss=5.8865\n",
            "8550/700000: training loss=5.4469, dev loss=5.8964\n",
            "8600/700000: training loss=5.3987, dev loss=5.9005\n",
            "8650/700000: training loss=5.4758, dev loss=5.8699\n",
            "8700/700000: training loss=5.4478, dev loss=5.8724\n",
            "8750/700000: training loss=5.4080, dev loss=5.8560\n",
            "8800/700000: training loss=5.4227, dev loss=5.8811\n",
            "8850/700000: training loss=5.3932, dev loss=5.8783\n",
            "8900/700000: training loss=5.3803, dev loss=5.8541\n",
            "8950/700000: training loss=5.3691, dev loss=5.8456\n",
            "9000/700000: training loss=5.3679, dev loss=5.8411\n",
            "9050/700000: training loss=5.3576, dev loss=5.8445\n",
            "9100/700000: training loss=5.3791, dev loss=5.8283\n",
            "9150/700000: training loss=5.3723, dev loss=5.8214\n",
            "9200/700000: training loss=5.3450, dev loss=5.8497\n",
            "9250/700000: training loss=5.3585, dev loss=5.8404\n",
            "9300/700000: training loss=5.3443, dev loss=5.8381\n",
            "9350/700000: training loss=5.3063, dev loss=5.8178\n",
            "9400/700000: training loss=5.3488, dev loss=5.8163\n",
            "9450/700000: training loss=5.3371, dev loss=5.8468\n",
            "9500/700000: training loss=5.3130, dev loss=5.8185\n",
            "9550/700000: training loss=5.3345, dev loss=5.8264\n",
            "9600/700000: training loss=5.3140, dev loss=5.7899\n",
            "9650/700000: training loss=5.3094, dev loss=5.8114\n",
            "9700/700000: training loss=5.2917, dev loss=5.8060\n",
            "9750/700000: training loss=5.2718, dev loss=5.7695\n",
            "9800/700000: training loss=5.2588, dev loss=5.8076\n",
            "9850/700000: training loss=5.2995, dev loss=5.7863\n",
            "9900/700000: training loss=5.2399, dev loss=5.7854\n",
            "9950/700000: training loss=5.2798, dev loss=5.7635\n",
            "10000/700000: training loss=5.2727, dev loss=5.7826\n",
            "10050/700000: training loss=5.2285, dev loss=5.7734\n",
            "10100/700000: training loss=5.2588, dev loss=5.7782\n",
            "10150/700000: training loss=5.2319, dev loss=5.7849\n",
            "10200/700000: training loss=5.2426, dev loss=5.7848\n",
            "10250/700000: training loss=5.1741, dev loss=5.7636\n",
            "10300/700000: training loss=5.2326, dev loss=5.7834\n",
            "10350/700000: training loss=5.2217, dev loss=5.7424\n",
            "10400/700000: training loss=5.2143, dev loss=5.7310\n",
            "10450/700000: training loss=5.2237, dev loss=5.7212\n",
            "10500/700000: training loss=5.2328, dev loss=5.7495\n",
            "10550/700000: training loss=5.1447, dev loss=5.7506\n",
            "10600/700000: training loss=5.1881, dev loss=5.7544\n",
            "10650/700000: training loss=5.2045, dev loss=5.7280\n",
            "10700/700000: training loss=5.1368, dev loss=5.7341\n",
            "10750/700000: training loss=5.1502, dev loss=5.7098\n",
            "10800/700000: training loss=5.1312, dev loss=5.6797\n",
            "10850/700000: training loss=5.1840, dev loss=5.6840\n",
            "10900/700000: training loss=5.1411, dev loss=5.6562\n",
            "10950/700000: training loss=5.0750, dev loss=5.7180\n",
            "11000/700000: training loss=5.1660, dev loss=5.7075\n",
            "11050/700000: training loss=5.1346, dev loss=5.7034\n",
            "11100/700000: training loss=5.1385, dev loss=5.6694\n",
            "11150/700000: training loss=5.1441, dev loss=5.7002\n",
            "11200/700000: training loss=5.1048, dev loss=5.7080\n",
            "11250/700000: training loss=5.1096, dev loss=5.6614\n",
            "11300/700000: training loss=5.1284, dev loss=5.7069\n",
            "11350/700000: training loss=5.1553, dev loss=5.6688\n",
            "11400/700000: training loss=5.1133, dev loss=5.6724\n",
            "11450/700000: training loss=5.0855, dev loss=5.6289\n",
            "11500/700000: training loss=5.0757, dev loss=5.6604\n",
            "11550/700000: training loss=5.0929, dev loss=5.6765\n",
            "11600/700000: training loss=5.0384, dev loss=5.6450\n",
            "11650/700000: training loss=5.1099, dev loss=5.6272\n",
            "11700/700000: training loss=5.0643, dev loss=5.6323\n",
            "11750/700000: training loss=5.0610, dev loss=5.6345\n",
            "11800/700000: training loss=5.0813, dev loss=5.6553\n",
            "11850/700000: training loss=5.0217, dev loss=5.6338\n",
            "11900/700000: training loss=5.0563, dev loss=5.6549\n",
            "11950/700000: training loss=5.0621, dev loss=5.6087\n",
            "12000/700000: training loss=5.0427, dev loss=5.6493\n",
            "12050/700000: training loss=5.0568, dev loss=5.6294\n",
            "12100/700000: training loss=5.0262, dev loss=5.6216\n",
            "12150/700000: training loss=4.9957, dev loss=5.5836\n",
            "12200/700000: training loss=5.0033, dev loss=5.6082\n",
            "12250/700000: training loss=5.0108, dev loss=5.5947\n",
            "12300/700000: training loss=5.0171, dev loss=5.5841\n",
            "12350/700000: training loss=5.0367, dev loss=5.5870\n",
            "12400/700000: training loss=4.9939, dev loss=5.6110\n",
            "12450/700000: training loss=5.0061, dev loss=5.5533\n",
            "12500/700000: training loss=4.9816, dev loss=5.5842\n",
            "12550/700000: training loss=4.9737, dev loss=5.5869\n",
            "12600/700000: training loss=5.0047, dev loss=5.6194\n",
            "12650/700000: training loss=4.9548, dev loss=5.5796\n",
            "12700/700000: training loss=4.9556, dev loss=5.5369\n",
            "12750/700000: training loss=4.9605, dev loss=5.5677\n",
            "12800/700000: training loss=4.9096, dev loss=5.5976\n",
            "12850/700000: training loss=4.9705, dev loss=5.4982\n",
            "12900/700000: training loss=4.9628, dev loss=5.5774\n",
            "12950/700000: training loss=4.9839, dev loss=5.5528\n",
            "13000/700000: training loss=4.9092, dev loss=5.6077\n",
            "13050/700000: training loss=5.0030, dev loss=5.5920\n",
            "13100/700000: training loss=4.9375, dev loss=5.5777\n",
            "13150/700000: training loss=4.9463, dev loss=5.5576\n",
            "13200/700000: training loss=4.9364, dev loss=5.5269\n",
            "13250/700000: training loss=4.9575, dev loss=5.5377\n",
            "13300/700000: training loss=4.9359, dev loss=5.6074\n",
            "13350/700000: training loss=4.9585, dev loss=5.5260\n",
            "13400/700000: training loss=4.9128, dev loss=5.5640\n",
            "13450/700000: training loss=4.9029, dev loss=5.5415\n",
            "13500/700000: training loss=4.8594, dev loss=5.5360\n",
            "13550/700000: training loss=4.8991, dev loss=5.5445\n",
            "13600/700000: training loss=4.8924, dev loss=5.4902\n",
            "13650/700000: training loss=4.9191, dev loss=5.5513\n",
            "13700/700000: training loss=4.8649, dev loss=5.4948\n",
            "13750/700000: training loss=4.8927, dev loss=5.4661\n",
            "13800/700000: training loss=4.8996, dev loss=5.5563\n",
            "13850/700000: training loss=4.9141, dev loss=5.4706\n",
            "13900/700000: training loss=4.8452, dev loss=5.5491\n",
            "13950/700000: training loss=4.8630, dev loss=5.4671\n",
            "14000/700000: training loss=4.8047, dev loss=5.5116\n",
            "14050/700000: training loss=4.8346, dev loss=5.4755\n",
            "14100/700000: training loss=4.8332, dev loss=5.4848\n",
            "14150/700000: training loss=4.8455, dev loss=5.4976\n",
            "14200/700000: training loss=4.8446, dev loss=5.4609\n",
            "14250/700000: training loss=4.8555, dev loss=5.4430\n",
            "14300/700000: training loss=4.8544, dev loss=5.4871\n",
            "14350/700000: training loss=4.8292, dev loss=5.4592\n",
            "14400/700000: training loss=4.8446, dev loss=5.5032\n",
            "14450/700000: training loss=4.8622, dev loss=5.5000\n",
            "14500/700000: training loss=4.8350, dev loss=5.4623\n",
            "14550/700000: training loss=4.8288, dev loss=5.4972\n",
            "14600/700000: training loss=4.8291, dev loss=5.4453\n",
            "14650/700000: training loss=4.7850, dev loss=5.4466\n",
            "14700/700000: training loss=4.8025, dev loss=5.4094\n",
            "14750/700000: training loss=4.7922, dev loss=5.4596\n",
            "14800/700000: training loss=4.7882, dev loss=5.4631\n",
            "14850/700000: training loss=4.8101, dev loss=5.4656\n",
            "14900/700000: training loss=4.8003, dev loss=5.4288\n",
            "14950/700000: training loss=4.7443, dev loss=5.4863\n",
            "15000/700000: training loss=4.7855, dev loss=5.4218\n",
            "15050/700000: training loss=4.7544, dev loss=5.4378\n",
            "15100/700000: training loss=4.7557, dev loss=5.4183\n",
            "15150/700000: training loss=4.7786, dev loss=5.4111\n",
            "15200/700000: training loss=4.7818, dev loss=5.4196\n",
            "15250/700000: training loss=4.7609, dev loss=5.4171\n",
            "15300/700000: training loss=4.7634, dev loss=5.4372\n",
            "15350/700000: training loss=4.7604, dev loss=5.4582\n",
            "15400/700000: training loss=4.7754, dev loss=5.4522\n",
            "15450/700000: training loss=4.7223, dev loss=5.4142\n",
            "15500/700000: training loss=4.7882, dev loss=5.4545\n",
            "15550/700000: training loss=4.7062, dev loss=5.4052\n",
            "15600/700000: training loss=4.7682, dev loss=5.4088\n",
            "15650/700000: training loss=4.7196, dev loss=5.3996\n",
            "15700/700000: training loss=4.7105, dev loss=5.4333\n",
            "15750/700000: training loss=4.7219, dev loss=5.4200\n",
            "15800/700000: training loss=4.6816, dev loss=5.4325\n",
            "15850/700000: training loss=4.7238, dev loss=5.4078\n",
            "15900/700000: training loss=4.7435, dev loss=5.3438\n",
            "15950/700000: training loss=4.6858, dev loss=5.3628\n",
            "16000/700000: training loss=4.6702, dev loss=5.4352\n",
            "16050/700000: training loss=4.7036, dev loss=5.4275\n",
            "16100/700000: training loss=4.7091, dev loss=5.4456\n",
            "16150/700000: training loss=4.6949, dev loss=5.3792\n",
            "16200/700000: training loss=4.7057, dev loss=5.4166\n",
            "16250/700000: training loss=4.6925, dev loss=5.4139\n",
            "16300/700000: training loss=4.7086, dev loss=5.4203\n",
            "16350/700000: training loss=4.6862, dev loss=5.3908\n",
            "16400/700000: training loss=4.6847, dev loss=5.3956\n",
            "16450/700000: training loss=4.6598, dev loss=5.3713\n",
            "16500/700000: training loss=4.6843, dev loss=5.3888\n",
            "16550/700000: training loss=4.6787, dev loss=5.3540\n",
            "16600/700000: training loss=4.6581, dev loss=5.3975\n",
            "16650/700000: training loss=4.7023, dev loss=5.4186\n",
            "16700/700000: training loss=4.6879, dev loss=5.3236\n",
            "16750/700000: training loss=4.6840, dev loss=5.4078\n",
            "16800/700000: training loss=4.6099, dev loss=5.3683\n",
            "16850/700000: training loss=4.6460, dev loss=5.3869\n",
            "16900/700000: training loss=4.6478, dev loss=5.3388\n",
            "16950/700000: training loss=4.6834, dev loss=5.3795\n",
            "17000/700000: training loss=4.6906, dev loss=5.4268\n",
            "17050/700000: training loss=4.6244, dev loss=5.3135\n",
            "17100/700000: training loss=4.6962, dev loss=5.3267\n",
            "17150/700000: training loss=4.6197, dev loss=5.3366\n",
            "17200/700000: training loss=4.6683, dev loss=5.3718\n",
            "17250/700000: training loss=4.6551, dev loss=5.3781\n",
            "17300/700000: training loss=4.6380, dev loss=5.3541\n",
            "17350/700000: training loss=4.6375, dev loss=5.3921\n",
            "17400/700000: training loss=4.6005, dev loss=5.3112\n",
            "17450/700000: training loss=4.6346, dev loss=5.3675\n",
            "17500/700000: training loss=4.6002, dev loss=5.3398\n",
            "17550/700000: training loss=4.6106, dev loss=5.3510\n",
            "17600/700000: training loss=4.6330, dev loss=5.2944\n",
            "17650/700000: training loss=4.6385, dev loss=5.3187\n",
            "17700/700000: training loss=4.5762, dev loss=5.3498\n",
            "17750/700000: training loss=4.6141, dev loss=5.3234\n",
            "17800/700000: training loss=4.6329, dev loss=5.3605\n",
            "17850/700000: training loss=4.5742, dev loss=5.3395\n",
            "17900/700000: training loss=4.5963, dev loss=5.3209\n",
            "17950/700000: training loss=4.5454, dev loss=5.3606\n",
            "18000/700000: training loss=4.5788, dev loss=5.3180\n",
            "18050/700000: training loss=4.5246, dev loss=5.3340\n",
            "18100/700000: training loss=4.5722, dev loss=5.3023\n",
            "18150/700000: training loss=4.5409, dev loss=5.2914\n",
            "18200/700000: training loss=4.6233, dev loss=5.3755\n",
            "18250/700000: training loss=4.5276, dev loss=5.3537\n",
            "18300/700000: training loss=4.5823, dev loss=5.3448\n",
            "18350/700000: training loss=4.5652, dev loss=5.2757\n",
            "18400/700000: training loss=4.5463, dev loss=5.3131\n",
            "18450/700000: training loss=4.5474, dev loss=5.3748\n",
            "18500/700000: training loss=4.5640, dev loss=5.2869\n",
            "18550/700000: training loss=4.5428, dev loss=5.2711\n",
            "18600/700000: training loss=4.5644, dev loss=5.2829\n",
            "18650/700000: training loss=4.5273, dev loss=5.2615\n",
            "18700/700000: training loss=4.5509, dev loss=5.3612\n",
            "18750/700000: training loss=4.5167, dev loss=5.2897\n",
            "18800/700000: training loss=4.5538, dev loss=5.3596\n",
            "18850/700000: training loss=4.5585, dev loss=5.2621\n",
            "18900/700000: training loss=4.5116, dev loss=5.2891\n",
            "18950/700000: training loss=4.5391, dev loss=5.2871\n",
            "19000/700000: training loss=4.5183, dev loss=5.3182\n",
            "19050/700000: training loss=4.5221, dev loss=5.2875\n",
            "19100/700000: training loss=4.4950, dev loss=5.2978\n",
            "19150/700000: training loss=4.5192, dev loss=5.2773\n",
            "19200/700000: training loss=4.5199, dev loss=5.3635\n",
            "19250/700000: training loss=4.5569, dev loss=5.2992\n",
            "19300/700000: training loss=4.5060, dev loss=5.3260\n",
            "19350/700000: training loss=4.5011, dev loss=5.3641\n",
            "19400/700000: training loss=4.5022, dev loss=5.3225\n",
            "19450/700000: training loss=4.4797, dev loss=5.2968\n",
            "19500/700000: training loss=4.5105, dev loss=5.3000\n",
            "19550/700000: training loss=4.5150, dev loss=5.3243\n",
            "19600/700000: training loss=4.4796, dev loss=5.3015\n",
            "19650/700000: training loss=4.5076, dev loss=5.3456\n",
            "19700/700000: training loss=4.5106, dev loss=5.3040\n",
            "19750/700000: training loss=4.4532, dev loss=5.3133\n",
            "19800/700000: training loss=4.4836, dev loss=5.2587\n",
            "19850/700000: training loss=4.5206, dev loss=5.3190\n",
            "19900/700000: training loss=4.4478, dev loss=5.3413\n",
            "19950/700000: training loss=4.4791, dev loss=5.3128\n",
            "20000/700000: training loss=4.4990, dev loss=5.3067\n",
            "20050/700000: training loss=4.4923, dev loss=5.2495\n",
            "20100/700000: training loss=4.4841, dev loss=5.3364\n",
            "20150/700000: training loss=4.4758, dev loss=5.2910\n",
            "20200/700000: training loss=4.4721, dev loss=5.2662\n",
            "20250/700000: training loss=4.4692, dev loss=5.2827\n",
            "20300/700000: training loss=4.4559, dev loss=5.3210\n",
            "20350/700000: training loss=4.4699, dev loss=5.3400\n",
            "20400/700000: training loss=4.4819, dev loss=5.2380\n",
            "20450/700000: training loss=4.4838, dev loss=5.2884\n",
            "20500/700000: training loss=4.4534, dev loss=5.2658\n",
            "20550/700000: training loss=4.4831, dev loss=5.2412\n",
            "20600/700000: training loss=4.4419, dev loss=5.2739\n",
            "20650/700000: training loss=4.3983, dev loss=5.2634\n",
            "20700/700000: training loss=4.4274, dev loss=5.3192\n",
            "20750/700000: training loss=4.4433, dev loss=5.3103\n",
            "20800/700000: training loss=4.4113, dev loss=5.2757\n",
            "20850/700000: training loss=4.4490, dev loss=5.2668\n",
            "20900/700000: training loss=4.4152, dev loss=5.2560\n",
            "20950/700000: training loss=4.3948, dev loss=5.3030\n",
            "21000/700000: training loss=4.4247, dev loss=5.2776\n",
            "21050/700000: training loss=4.4275, dev loss=5.2789\n",
            "21100/700000: training loss=4.4159, dev loss=5.2862\n",
            "21150/700000: training loss=4.4165, dev loss=5.2618\n",
            "21200/700000: training loss=4.3863, dev loss=5.2490\n",
            "21250/700000: training loss=4.4357, dev loss=5.2514\n",
            "21300/700000: training loss=4.4439, dev loss=5.2541\n",
            "21350/700000: training loss=4.4255, dev loss=5.2294\n",
            "21400/700000: training loss=4.4732, dev loss=5.3399\n",
            "21450/700000: training loss=4.3767, dev loss=5.2500\n",
            "21500/700000: training loss=4.3740, dev loss=5.2651\n",
            "21550/700000: training loss=4.3693, dev loss=5.2810\n",
            "21600/700000: training loss=4.4098, dev loss=5.3030\n",
            "21650/700000: training loss=4.3901, dev loss=5.2467\n",
            "21700/700000: training loss=4.4269, dev loss=5.2560\n",
            "21750/700000: training loss=4.4028, dev loss=5.2851\n",
            "21800/700000: training loss=4.4024, dev loss=5.2456\n",
            "21850/700000: training loss=4.3988, dev loss=5.2360\n",
            "21900/700000: training loss=4.3867, dev loss=5.2854\n",
            "21950/700000: training loss=4.3673, dev loss=5.2664\n",
            "22000/700000: training loss=4.3756, dev loss=5.2078\n",
            "22050/700000: training loss=4.3910, dev loss=5.2710\n",
            "22100/700000: training loss=4.4178, dev loss=5.2324\n",
            "22150/700000: training loss=4.3571, dev loss=5.3148\n",
            "22200/700000: training loss=4.3520, dev loss=5.2799\n",
            "22250/700000: training loss=4.3587, dev loss=5.2224\n",
            "22300/700000: training loss=4.3550, dev loss=5.2427\n",
            "22350/700000: training loss=4.3508, dev loss=5.2199\n",
            "22400/700000: training loss=4.3387, dev loss=5.2616\n",
            "22450/700000: training loss=4.3447, dev loss=5.2277\n",
            "22500/700000: training loss=4.3452, dev loss=5.2972\n",
            "22550/700000: training loss=4.3487, dev loss=5.2732\n",
            "22600/700000: training loss=4.3818, dev loss=5.2854\n",
            "22650/700000: training loss=4.3438, dev loss=5.2631\n",
            "22700/700000: training loss=4.3742, dev loss=5.2746\n",
            "22750/700000: training loss=4.3808, dev loss=5.2172\n",
            "22800/700000: training loss=4.3676, dev loss=5.2743\n",
            "22850/700000: training loss=4.3828, dev loss=5.2808\n",
            "22900/700000: training loss=4.3075, dev loss=5.3031\n",
            "22950/700000: training loss=4.3404, dev loss=5.2377\n",
            "23000/700000: training loss=4.3602, dev loss=5.2012\n",
            "23050/700000: training loss=4.3268, dev loss=5.3146\n",
            "23100/700000: training loss=4.3133, dev loss=5.2767\n",
            "23150/700000: training loss=4.3297, dev loss=5.2623\n",
            "23200/700000: training loss=4.3322, dev loss=5.2939\n",
            "23250/700000: training loss=4.3136, dev loss=5.2387\n",
            "23300/700000: training loss=4.3049, dev loss=5.2271\n",
            "23350/700000: training loss=4.2902, dev loss=5.2704\n",
            "23400/700000: training loss=4.3079, dev loss=5.2788\n",
            "23450/700000: training loss=4.3003, dev loss=5.2138\n",
            "23500/700000: training loss=4.3065, dev loss=5.3056\n",
            "23550/700000: training loss=4.2884, dev loss=5.2549\n",
            "23600/700000: training loss=4.2826, dev loss=5.2207\n",
            "23650/700000: training loss=4.2996, dev loss=5.2193\n",
            "23700/700000: training loss=4.2785, dev loss=5.3193\n",
            "23750/700000: training loss=4.3105, dev loss=5.1776\n",
            "23800/700000: training loss=4.3379, dev loss=5.3378\n",
            "23850/700000: training loss=4.2985, dev loss=5.2254\n",
            "23900/700000: training loss=4.2581, dev loss=5.2495\n",
            "23950/700000: training loss=4.2549, dev loss=5.2222\n",
            "24000/700000: training loss=4.2795, dev loss=5.3003\n",
            "24050/700000: training loss=4.2886, dev loss=5.2412\n",
            "24100/700000: training loss=4.3005, dev loss=5.2648\n",
            "24150/700000: training loss=4.3126, dev loss=5.2169\n",
            "24200/700000: training loss=4.2958, dev loss=5.1714\n",
            "24250/700000: training loss=4.2668, dev loss=5.3219\n",
            "24300/700000: training loss=4.3035, dev loss=5.2959\n",
            "24350/700000: training loss=4.2697, dev loss=5.2510\n",
            "24400/700000: training loss=4.2553, dev loss=5.2259\n",
            "24450/700000: training loss=4.2596, dev loss=5.2199\n",
            "24500/700000: training loss=4.2843, dev loss=5.2244\n",
            "24550/700000: training loss=4.2639, dev loss=5.2623\n",
            "24600/700000: training loss=4.2532, dev loss=5.2704\n",
            "24650/700000: training loss=4.2636, dev loss=5.2920\n",
            "24700/700000: training loss=4.2169, dev loss=5.2337\n",
            "24750/700000: training loss=4.2705, dev loss=5.2422\n",
            "24800/700000: training loss=4.2749, dev loss=5.2553\n",
            "24850/700000: training loss=4.2818, dev loss=5.2442\n",
            "24900/700000: training loss=4.2264, dev loss=5.2844\n",
            "24950/700000: training loss=4.2331, dev loss=5.3051\n",
            "25000/700000: training loss=4.2725, dev loss=5.2601\n",
            "25050/700000: training loss=4.2627, dev loss=5.2504\n",
            "25100/700000: training loss=4.2567, dev loss=5.3038\n",
            "25150/700000: training loss=4.2331, dev loss=5.2392\n",
            "25200/700000: training loss=4.2163, dev loss=5.1828\n",
            "25250/700000: training loss=4.2099, dev loss=5.2450\n",
            "25300/700000: training loss=4.2495, dev loss=5.2252\n",
            "25350/700000: training loss=4.2327, dev loss=5.2553\n",
            "25400/700000: training loss=4.2172, dev loss=5.2623\n",
            "25450/700000: training loss=4.2402, dev loss=5.2216\n",
            "25500/700000: training loss=4.1907, dev loss=5.3270\n",
            "25550/700000: training loss=4.1977, dev loss=5.2043\n",
            "25600/700000: training loss=4.1988, dev loss=5.2777\n",
            "25650/700000: training loss=4.2374, dev loss=5.3049\n",
            "25700/700000: training loss=4.2074, dev loss=5.3063\n",
            "25750/700000: training loss=4.1998, dev loss=5.2882\n",
            "25800/700000: training loss=4.2226, dev loss=5.2890\n",
            "25850/700000: training loss=4.2230, dev loss=5.2126\n",
            "25900/700000: training loss=4.2063, dev loss=5.3145\n",
            "25950/700000: training loss=4.1962, dev loss=5.2583\n",
            "26000/700000: training loss=4.1894, dev loss=5.2441\n",
            "26050/700000: training loss=4.1917, dev loss=5.2429\n",
            "26100/700000: training loss=4.1915, dev loss=5.2986\n",
            "26150/700000: training loss=4.2231, dev loss=5.2917\n",
            "26200/700000: training loss=4.2028, dev loss=5.3158\n",
            "26250/700000: training loss=4.1779, dev loss=5.2425\n",
            "26300/700000: training loss=4.1863, dev loss=5.2632\n",
            "26350/700000: training loss=4.2186, dev loss=5.2827\n",
            "26400/700000: training loss=4.1743, dev loss=5.2077\n",
            "26450/700000: training loss=4.1976, dev loss=5.2957\n",
            "26500/700000: training loss=4.1559, dev loss=5.2463\n",
            "26550/700000: training loss=4.2029, dev loss=5.2741\n",
            "26600/700000: training loss=4.1569, dev loss=5.2776\n",
            "26650/700000: training loss=4.2064, dev loss=5.2191\n",
            "26700/700000: training loss=4.1519, dev loss=5.3004\n",
            "26750/700000: training loss=4.1594, dev loss=5.2577\n",
            "26800/700000: training loss=4.1344, dev loss=5.2557\n",
            "26850/700000: training loss=4.1868, dev loss=5.2667\n",
            "26900/700000: training loss=4.1760, dev loss=5.2412\n",
            "26950/700000: training loss=4.1961, dev loss=5.2339\n",
            "27000/700000: training loss=4.1885, dev loss=5.1781\n",
            "27050/700000: training loss=4.1351, dev loss=5.2354\n",
            "27100/700000: training loss=4.1725, dev loss=5.2668\n",
            "27150/700000: training loss=4.1065, dev loss=5.3297\n",
            "27200/700000: training loss=4.1381, dev loss=5.2838\n",
            "27250/700000: training loss=4.1679, dev loss=5.3060\n",
            "27300/700000: training loss=4.1298, dev loss=5.3324\n",
            "27350/700000: training loss=4.1256, dev loss=5.2353\n",
            "27400/700000: training loss=4.1331, dev loss=5.3209\n",
            "27450/700000: training loss=4.1859, dev loss=5.2832\n",
            "27500/700000: training loss=4.1064, dev loss=5.2975\n",
            "27550/700000: training loss=4.1263, dev loss=5.2556\n",
            "27600/700000: training loss=4.1344, dev loss=5.3000\n",
            "27650/700000: training loss=4.1745, dev loss=5.2375\n",
            "27700/700000: training loss=4.1209, dev loss=5.2139\n",
            "27750/700000: training loss=4.1099, dev loss=5.2289\n",
            "27800/700000: training loss=4.0948, dev loss=5.2642\n",
            "27850/700000: training loss=4.1061, dev loss=5.2376\n",
            "27900/700000: training loss=4.1134, dev loss=5.2512\n",
            "27950/700000: training loss=4.1554, dev loss=5.2981\n",
            "28000/700000: training loss=4.1018, dev loss=5.2545\n",
            "28050/700000: training loss=4.1142, dev loss=5.3428\n",
            "28100/700000: training loss=4.1097, dev loss=5.3057\n",
            "28150/700000: training loss=4.1256, dev loss=5.2577\n",
            "28200/700000: training loss=4.0921, dev loss=5.3335\n",
            "28250/700000: training loss=4.0775, dev loss=5.2439\n",
            "28300/700000: training loss=4.0726, dev loss=5.2610\n",
            "28350/700000: training loss=4.1275, dev loss=5.2987\n",
            "28400/700000: training loss=4.0486, dev loss=5.2517\n",
            "28450/700000: training loss=4.1000, dev loss=5.2951\n",
            "28500/700000: training loss=4.0771, dev loss=5.2133\n",
            "28550/700000: training loss=4.1068, dev loss=5.2940\n",
            "28600/700000: training loss=4.0764, dev loss=5.2852\n",
            "28650/700000: training loss=4.0568, dev loss=5.3134\n",
            "28700/700000: training loss=4.0559, dev loss=5.2608\n",
            "28750/700000: training loss=4.0530, dev loss=5.2815\n",
            "28800/700000: training loss=4.1170, dev loss=5.2855\n",
            "28850/700000: training loss=4.1014, dev loss=5.2849\n",
            "28900/700000: training loss=4.0760, dev loss=5.2674\n",
            "28950/700000: training loss=4.0989, dev loss=5.2578\n",
            "29000/700000: training loss=4.0710, dev loss=5.3000\n",
            "29050/700000: training loss=4.0741, dev loss=5.2886\n",
            "29100/700000: training loss=4.0708, dev loss=5.2366\n",
            "29150/700000: training loss=4.0753, dev loss=5.2600\n",
            "29200/700000: training loss=4.0778, dev loss=5.3058\n",
            "29250/700000: training loss=4.0527, dev loss=5.2907\n",
            "29300/700000: training loss=4.0765, dev loss=5.2647\n",
            "29350/700000: training loss=4.0652, dev loss=5.2446\n",
            "29400/700000: training loss=4.0683, dev loss=5.2154\n",
            "29450/700000: training loss=4.0562, dev loss=5.2757\n",
            "29500/700000: training loss=4.0286, dev loss=5.2858\n",
            "29550/700000: training loss=4.0745, dev loss=5.2865\n",
            "29600/700000: training loss=4.0344, dev loss=5.3188\n",
            "29650/700000: training loss=4.0280, dev loss=5.3061\n",
            "29700/700000: training loss=4.0371, dev loss=5.2511\n",
            "29750/700000: training loss=4.0137, dev loss=5.2834\n",
            "29800/700000: training loss=4.0336, dev loss=5.2797\n",
            "29850/700000: training loss=4.0221, dev loss=5.2868\n",
            "29900/700000: training loss=4.0144, dev loss=5.2075\n",
            "29950/700000: training loss=4.0477, dev loss=5.2966\n",
            "30000/700000: training loss=4.0447, dev loss=5.3219\n",
            "30050/700000: training loss=4.0284, dev loss=5.2409\n",
            "30100/700000: training loss=4.0371, dev loss=5.2764\n",
            "30150/700000: training loss=4.0065, dev loss=5.3088\n",
            "30200/700000: training loss=4.0302, dev loss=5.3025\n",
            "30250/700000: training loss=4.0574, dev loss=5.3435\n",
            "30300/700000: training loss=4.0181, dev loss=5.2551\n",
            "30350/700000: training loss=3.9897, dev loss=5.2292\n",
            "30400/700000: training loss=4.0018, dev loss=5.3304\n",
            "30450/700000: training loss=4.0258, dev loss=5.3025\n",
            "30500/700000: training loss=4.0114, dev loss=5.2892\n",
            "30550/700000: training loss=4.0057, dev loss=5.3043\n",
            "30600/700000: training loss=4.0056, dev loss=5.3024\n",
            "30650/700000: training loss=4.0125, dev loss=5.2924\n",
            "30700/700000: training loss=4.0012, dev loss=5.2953\n",
            "30750/700000: training loss=3.9882, dev loss=5.2438\n",
            "30800/700000: training loss=4.0136, dev loss=5.2984\n",
            "30850/700000: training loss=4.0007, dev loss=5.2155\n",
            "30900/700000: training loss=3.9839, dev loss=5.2267\n",
            "30950/700000: training loss=3.9675, dev loss=5.3174\n",
            "31000/700000: training loss=4.0038, dev loss=5.3437\n",
            "31050/700000: training loss=4.0180, dev loss=5.3155\n",
            "31100/700000: training loss=3.9632, dev loss=5.2979\n",
            "31150/700000: training loss=3.9549, dev loss=5.2722\n",
            "31200/700000: training loss=3.9898, dev loss=5.2805\n",
            "31250/700000: training loss=3.9688, dev loss=5.2924\n",
            "31300/700000: training loss=4.0154, dev loss=5.2895\n",
            "31350/700000: training loss=3.9705, dev loss=5.3321\n",
            "31400/700000: training loss=3.9715, dev loss=5.3279\n",
            "31450/700000: training loss=3.9430, dev loss=5.3453\n",
            "31500/700000: training loss=3.9339, dev loss=5.3101\n",
            "31550/700000: training loss=3.9523, dev loss=5.3779\n",
            "31600/700000: training loss=3.9929, dev loss=5.2962\n",
            "31650/700000: training loss=3.9412, dev loss=5.2485\n",
            "31700/700000: training loss=3.9395, dev loss=5.3107\n",
            "31750/700000: training loss=3.9633, dev loss=5.2723\n",
            "31800/700000: training loss=3.9606, dev loss=5.2817\n",
            "31850/700000: training loss=3.9584, dev loss=5.3404\n",
            "31900/700000: training loss=3.9501, dev loss=5.2408\n",
            "31950/700000: training loss=3.9720, dev loss=5.2818\n",
            "32000/700000: training loss=3.9408, dev loss=5.2996\n",
            "32050/700000: training loss=3.9349, dev loss=5.3514\n",
            "32100/700000: training loss=3.9496, dev loss=5.2851\n",
            "32150/700000: training loss=3.9191, dev loss=5.3435\n",
            "32200/700000: training loss=3.9159, dev loss=5.2613\n",
            "32250/700000: training loss=3.9362, dev loss=5.2977\n",
            "32300/700000: training loss=3.9189, dev loss=5.2710\n",
            "32350/700000: training loss=3.9432, dev loss=5.2945\n",
            "32400/700000: training loss=3.9410, dev loss=5.3070\n",
            "32450/700000: training loss=3.9258, dev loss=5.3059\n",
            "32500/700000: training loss=3.9238, dev loss=5.2928\n",
            "32550/700000: training loss=3.9070, dev loss=5.2721\n",
            "32600/700000: training loss=3.8980, dev loss=5.3875\n",
            "32650/700000: training loss=3.9249, dev loss=5.3175\n",
            "32700/700000: training loss=3.9166, dev loss=5.3346\n",
            "32750/700000: training loss=3.9085, dev loss=5.2540\n",
            "32800/700000: training loss=3.9574, dev loss=5.2841\n",
            "32850/700000: training loss=3.8910, dev loss=5.3116\n",
            "32900/700000: training loss=3.8728, dev loss=5.3773\n",
            "32950/700000: training loss=3.9110, dev loss=5.3154\n",
            "33000/700000: training loss=3.9070, dev loss=5.2890\n",
            "33050/700000: training loss=3.8872, dev loss=5.2412\n",
            "33100/700000: training loss=3.9075, dev loss=5.2820\n",
            "33150/700000: training loss=3.9251, dev loss=5.2689\n",
            "33200/700000: training loss=3.8894, dev loss=5.3109\n",
            "33250/700000: training loss=3.8748, dev loss=5.3140\n",
            "33300/700000: training loss=3.8717, dev loss=5.3511\n",
            "33350/700000: training loss=3.9105, dev loss=5.3229\n",
            "33400/700000: training loss=3.9061, dev loss=5.3417\n",
            "33450/700000: training loss=3.8679, dev loss=5.3181\n",
            "33500/700000: training loss=3.8837, dev loss=5.3467\n",
            "33550/700000: training loss=3.8368, dev loss=5.2870\n",
            "33600/700000: training loss=3.8850, dev loss=5.3719\n",
            "33650/700000: training loss=3.8484, dev loss=5.3232\n",
            "33700/700000: training loss=3.8950, dev loss=5.3898\n",
            "33750/700000: training loss=3.8370, dev loss=5.3263\n",
            "33800/700000: training loss=3.8924, dev loss=5.3567\n",
            "33850/700000: training loss=3.8565, dev loss=5.3416\n",
            "33900/700000: training loss=3.8358, dev loss=5.3627\n",
            "33950/700000: training loss=3.8694, dev loss=5.3522\n",
            "34000/700000: training loss=3.8549, dev loss=5.4097\n",
            "34050/700000: training loss=3.8469, dev loss=5.3638\n",
            "34100/700000: training loss=3.8485, dev loss=5.3230\n",
            "34150/700000: training loss=3.8421, dev loss=5.3439\n",
            "34200/700000: training loss=3.8521, dev loss=5.4328\n",
            "34250/700000: training loss=3.8475, dev loss=5.3702\n",
            "34300/700000: training loss=3.8189, dev loss=5.3207\n",
            "34350/700000: training loss=3.8681, dev loss=5.2836\n",
            "34400/700000: training loss=3.8301, dev loss=5.3693\n",
            "34450/700000: training loss=3.8096, dev loss=5.3801\n",
            "34500/700000: training loss=3.8578, dev loss=5.3465\n",
            "34550/700000: training loss=3.7996, dev loss=5.3393\n",
            "34600/700000: training loss=3.8309, dev loss=5.3572\n",
            "34650/700000: training loss=3.8330, dev loss=5.3615\n",
            "34700/700000: training loss=3.8067, dev loss=5.3191\n",
            "34750/700000: training loss=3.8258, dev loss=5.3550\n",
            "34800/700000: training loss=3.8071, dev loss=5.3473\n",
            "34850/700000: training loss=3.7892, dev loss=5.3869\n",
            "34900/700000: training loss=3.7921, dev loss=5.3506\n",
            "34950/700000: training loss=3.8261, dev loss=5.3513\n",
            "35000/700000: training loss=3.8353, dev loss=5.3679\n",
            "35050/700000: training loss=3.8203, dev loss=5.4264\n",
            "35100/700000: training loss=3.8049, dev loss=5.3798\n",
            "35150/700000: training loss=3.7746, dev loss=5.3716\n",
            "35200/700000: training loss=3.8212, dev loss=5.3951\n",
            "35250/700000: training loss=3.7646, dev loss=5.4043\n",
            "35300/700000: training loss=3.8257, dev loss=5.3450\n",
            "35350/700000: training loss=3.8359, dev loss=5.3663\n",
            "35400/700000: training loss=3.7957, dev loss=5.3423\n",
            "35450/700000: training loss=3.7930, dev loss=5.3504\n",
            "35500/700000: training loss=3.7881, dev loss=5.4167\n",
            "35550/700000: training loss=3.8135, dev loss=5.3626\n",
            "35600/700000: training loss=3.7862, dev loss=5.3553\n",
            "35650/700000: training loss=3.7784, dev loss=5.4090\n",
            "35700/700000: training loss=3.8008, dev loss=5.4031\n",
            "35750/700000: training loss=3.7644, dev loss=5.3826\n",
            "35800/700000: training loss=3.7847, dev loss=5.3843\n",
            "35850/700000: training loss=3.7705, dev loss=5.4229\n",
            "35900/700000: training loss=3.7807, dev loss=5.3969\n",
            "35950/700000: training loss=3.7612, dev loss=5.3961\n",
            "36000/700000: training loss=3.7806, dev loss=5.3992\n",
            "36050/700000: training loss=3.7577, dev loss=5.4410\n",
            "36100/700000: training loss=3.7964, dev loss=5.3544\n",
            "36150/700000: training loss=3.7516, dev loss=5.4569\n",
            "36200/700000: training loss=3.7850, dev loss=5.4266\n",
            "36250/700000: training loss=3.7489, dev loss=5.3497\n",
            "36300/700000: training loss=3.7823, dev loss=5.4023\n",
            "36350/700000: training loss=3.7815, dev loss=5.4405\n",
            "36400/700000: training loss=3.7703, dev loss=5.3267\n",
            "36450/700000: training loss=3.7697, dev loss=5.4042\n",
            "36500/700000: training loss=3.7235, dev loss=5.4050\n",
            "36550/700000: training loss=3.7226, dev loss=5.3286\n",
            "36600/700000: training loss=3.7094, dev loss=5.4582\n",
            "36650/700000: training loss=3.7172, dev loss=5.3221\n",
            "36700/700000: training loss=3.7607, dev loss=5.4536\n",
            "36750/700000: training loss=3.6788, dev loss=5.3414\n",
            "36800/700000: training loss=3.7347, dev loss=5.4372\n",
            "36850/700000: training loss=3.7578, dev loss=5.4148\n",
            "36900/700000: training loss=3.7341, dev loss=5.3535\n",
            "36950/700000: training loss=3.7180, dev loss=5.4225\n",
            "37000/700000: training loss=3.7331, dev loss=5.3636\n",
            "37050/700000: training loss=3.7267, dev loss=5.3728\n",
            "37100/700000: training loss=3.6947, dev loss=5.4231\n",
            "37150/700000: training loss=3.7011, dev loss=5.4676\n",
            "37200/700000: training loss=3.6839, dev loss=5.4039\n",
            "37250/700000: training loss=3.7042, dev loss=5.4129\n",
            "37300/700000: training loss=3.7064, dev loss=5.4681\n",
            "37350/700000: training loss=3.6527, dev loss=5.3730\n",
            "37400/700000: training loss=3.7020, dev loss=5.4076\n",
            "37450/700000: training loss=3.6661, dev loss=5.3564\n",
            "37500/700000: training loss=3.7042, dev loss=5.4599\n",
            "37550/700000: training loss=3.6799, dev loss=5.4052\n",
            "37600/700000: training loss=3.7129, dev loss=5.4209\n",
            "37650/700000: training loss=3.6694, dev loss=5.3978\n",
            "37700/700000: training loss=3.6698, dev loss=5.3420\n",
            "37750/700000: training loss=3.6668, dev loss=5.4507\n",
            "37800/700000: training loss=3.7229, dev loss=5.4346\n",
            "37850/700000: training loss=3.6829, dev loss=5.4056\n",
            "37900/700000: training loss=3.6649, dev loss=5.3913\n",
            "37950/700000: training loss=3.6459, dev loss=5.4804\n",
            "38000/700000: training loss=3.6393, dev loss=5.3762\n",
            "38050/700000: training loss=3.7023, dev loss=5.4554\n",
            "38100/700000: training loss=3.6608, dev loss=5.4288\n",
            "38150/700000: training loss=3.6470, dev loss=5.4555\n",
            "38200/700000: training loss=3.6433, dev loss=5.4099\n",
            "38250/700000: training loss=3.6574, dev loss=5.3347\n",
            "38300/700000: training loss=3.6539, dev loss=5.4237\n",
            "38350/700000: training loss=3.6739, dev loss=5.3841\n",
            "38400/700000: training loss=3.6647, dev loss=5.4427\n",
            "38450/700000: training loss=3.6268, dev loss=5.4225\n",
            "38500/700000: training loss=3.6457, dev loss=5.4407\n",
            "38550/700000: training loss=3.6520, dev loss=5.3874\n",
            "38600/700000: training loss=3.6380, dev loss=5.5454\n",
            "38650/700000: training loss=3.6381, dev loss=5.4265\n",
            "38700/700000: training loss=3.6393, dev loss=5.4470\n",
            "38750/700000: training loss=3.6135, dev loss=5.4495\n",
            "38800/700000: training loss=3.6678, dev loss=5.4249\n",
            "38850/700000: training loss=3.6263, dev loss=5.4355\n",
            "38900/700000: training loss=3.6454, dev loss=5.5273\n",
            "38950/700000: training loss=3.6144, dev loss=5.4666\n",
            "39000/700000: training loss=3.5948, dev loss=5.5047\n",
            "39050/700000: training loss=3.6673, dev loss=5.5125\n",
            "39100/700000: training loss=3.6167, dev loss=5.4942\n",
            "39150/700000: training loss=3.6107, dev loss=5.5210\n",
            "39200/700000: training loss=3.6175, dev loss=5.4201\n",
            "39250/700000: training loss=3.6227, dev loss=5.4784\n",
            "39300/700000: training loss=3.6227, dev loss=5.5394\n",
            "39350/700000: training loss=3.6128, dev loss=5.4536\n",
            "39400/700000: training loss=3.5869, dev loss=5.5004\n",
            "39450/700000: training loss=3.6154, dev loss=5.4780\n",
            "39500/700000: training loss=3.5886, dev loss=5.5082\n",
            "39550/700000: training loss=3.6209, dev loss=5.4345\n",
            "39600/700000: training loss=3.5883, dev loss=5.5360\n",
            "39650/700000: training loss=3.6142, dev loss=5.4443\n",
            "39700/700000: training loss=3.5773, dev loss=5.5361\n",
            "39750/700000: training loss=3.5894, dev loss=5.4305\n",
            "39800/700000: training loss=3.6099, dev loss=5.4561\n",
            "39850/700000: training loss=3.5684, dev loss=5.5007\n",
            "39900/700000: training loss=3.5682, dev loss=5.4468\n",
            "39950/700000: training loss=3.5701, dev loss=5.4660\n",
            "40000/700000: training loss=3.5626, dev loss=5.4480\n",
            "40050/700000: training loss=3.5703, dev loss=5.4015\n",
            "40100/700000: training loss=3.5600, dev loss=5.4791\n",
            "40150/700000: training loss=3.5753, dev loss=5.4702\n",
            "40200/700000: training loss=3.5401, dev loss=5.5348\n",
            "40250/700000: training loss=3.5883, dev loss=5.4275\n",
            "40300/700000: training loss=3.5680, dev loss=5.5062\n",
            "40350/700000: training loss=3.5788, dev loss=5.3940\n",
            "40400/700000: training loss=3.5563, dev loss=5.5209\n",
            "40450/700000: training loss=3.5461, dev loss=5.4588\n",
            "40500/700000: training loss=3.5262, dev loss=5.4740\n",
            "40550/700000: training loss=3.5351, dev loss=5.4975\n",
            "40600/700000: training loss=3.5903, dev loss=5.5154\n",
            "40650/700000: training loss=3.5486, dev loss=5.5247\n",
            "40700/700000: training loss=3.4853, dev loss=5.4911\n",
            "40750/700000: training loss=3.5778, dev loss=5.5051\n",
            "40800/700000: training loss=3.5405, dev loss=5.4832\n",
            "40850/700000: training loss=3.5618, dev loss=5.4877\n",
            "40900/700000: training loss=3.5319, dev loss=5.4438\n",
            "40950/700000: training loss=3.5464, dev loss=5.5279\n",
            "41000/700000: training loss=3.5221, dev loss=5.5588\n",
            "41050/700000: training loss=3.5255, dev loss=5.5041\n",
            "41100/700000: training loss=3.5232, dev loss=5.5313\n",
            "41150/700000: training loss=3.5166, dev loss=5.5027\n",
            "41200/700000: training loss=3.5125, dev loss=5.5233\n",
            "41250/700000: training loss=3.5421, dev loss=5.5055\n",
            "41300/700000: training loss=3.5393, dev loss=5.5330\n",
            "41350/700000: training loss=3.5149, dev loss=5.5144\n",
            "41400/700000: training loss=3.5105, dev loss=5.5308\n",
            "41450/700000: training loss=3.5062, dev loss=5.5243\n",
            "41500/700000: training loss=3.4760, dev loss=5.4647\n",
            "41550/700000: training loss=3.5041, dev loss=5.5061\n",
            "41600/700000: training loss=3.4777, dev loss=5.6027\n",
            "41650/700000: training loss=3.5074, dev loss=5.5717\n",
            "41700/700000: training loss=3.4866, dev loss=5.5819\n",
            "41750/700000: training loss=3.4840, dev loss=5.4806\n",
            "41800/700000: training loss=3.5009, dev loss=5.5416\n",
            "41850/700000: training loss=3.4694, dev loss=5.5819\n",
            "41900/700000: training loss=3.4925, dev loss=5.5178\n",
            "41950/700000: training loss=3.4640, dev loss=5.5375\n",
            "42000/700000: training loss=3.4802, dev loss=5.5322\n",
            "42050/700000: training loss=3.4812, dev loss=5.5141\n",
            "42100/700000: training loss=3.4689, dev loss=5.5680\n",
            "42150/700000: training loss=3.4442, dev loss=5.5810\n",
            "42200/700000: training loss=3.4716, dev loss=5.5691\n",
            "42250/700000: training loss=3.4728, dev loss=5.4819\n",
            "42300/700000: training loss=3.4785, dev loss=5.5538\n",
            "42350/700000: training loss=3.4317, dev loss=5.5282\n",
            "42400/700000: training loss=3.4480, dev loss=5.4608\n",
            "42450/700000: training loss=3.3979, dev loss=5.5539\n",
            "42500/700000: training loss=3.4408, dev loss=5.5255\n",
            "42550/700000: training loss=3.4250, dev loss=5.5319\n",
            "42600/700000: training loss=3.4596, dev loss=5.5445\n",
            "42650/700000: training loss=3.4487, dev loss=5.5620\n",
            "42700/700000: training loss=3.4250, dev loss=5.5831\n",
            "42750/700000: training loss=3.4201, dev loss=5.5148\n",
            "42800/700000: training loss=3.4476, dev loss=5.5129\n",
            "42850/700000: training loss=3.4082, dev loss=5.5408\n",
            "42900/700000: training loss=3.4450, dev loss=5.5514\n",
            "42950/700000: training loss=3.4360, dev loss=5.5968\n",
            "43000/700000: training loss=3.4259, dev loss=5.5898\n",
            "43050/700000: training loss=3.4006, dev loss=5.5925\n",
            "43100/700000: training loss=3.4261, dev loss=5.4799\n",
            "43150/700000: training loss=3.4225, dev loss=5.5290\n",
            "43200/700000: training loss=3.4075, dev loss=5.5131\n",
            "43250/700000: training loss=3.4195, dev loss=5.6313\n",
            "43300/700000: training loss=3.4065, dev loss=5.5632\n",
            "43350/700000: training loss=3.4503, dev loss=5.5850\n",
            "43400/700000: training loss=3.3755, dev loss=5.5968\n",
            "43450/700000: training loss=3.4353, dev loss=5.6916\n",
            "43500/700000: training loss=3.4139, dev loss=5.5782\n",
            "43550/700000: training loss=3.3926, dev loss=5.5564\n",
            "43600/700000: training loss=3.3898, dev loss=5.6488\n",
            "43650/700000: training loss=3.4049, dev loss=5.6382\n",
            "43700/700000: training loss=3.3952, dev loss=5.6614\n",
            "43750/700000: training loss=3.3559, dev loss=5.5527\n",
            "43800/700000: training loss=3.3964, dev loss=5.6219\n",
            "43850/700000: training loss=3.3731, dev loss=5.5939\n",
            "43900/700000: training loss=3.3968, dev loss=5.5037\n",
            "43950/700000: training loss=3.3225, dev loss=5.6191\n",
            "44000/700000: training loss=3.3983, dev loss=5.6109\n",
            "44050/700000: training loss=3.3722, dev loss=5.5616\n",
            "44100/700000: training loss=3.3688, dev loss=5.6754\n",
            "44150/700000: training loss=3.3480, dev loss=5.6649\n",
            "44200/700000: training loss=3.3599, dev loss=5.5997\n",
            "44250/700000: training loss=3.3722, dev loss=5.5709\n",
            "44300/700000: training loss=3.3402, dev loss=5.6291\n",
            "44350/700000: training loss=3.3355, dev loss=5.6017\n",
            "44400/700000: training loss=3.3315, dev loss=5.6651\n",
            "44450/700000: training loss=3.3348, dev loss=5.6481\n",
            "44500/700000: training loss=3.3560, dev loss=5.5309\n",
            "44550/700000: training loss=3.3387, dev loss=5.5938\n",
            "44600/700000: training loss=3.3325, dev loss=5.6929\n",
            "44650/700000: training loss=3.3744, dev loss=5.6082\n",
            "44700/700000: training loss=3.3184, dev loss=5.7138\n",
            "44750/700000: training loss=3.3692, dev loss=5.7064\n",
            "44800/700000: training loss=3.3304, dev loss=5.6935\n",
            "44850/700000: training loss=3.3110, dev loss=5.5748\n",
            "44900/700000: training loss=3.3147, dev loss=5.6141\n",
            "44950/700000: training loss=3.3483, dev loss=5.7096\n",
            "45000/700000: training loss=3.3386, dev loss=5.5841\n",
            "45050/700000: training loss=3.3190, dev loss=5.6313\n",
            "45100/700000: training loss=3.3375, dev loss=5.7010\n",
            "45150/700000: training loss=3.3349, dev loss=5.6164\n",
            "45200/700000: training loss=3.3140, dev loss=5.6929\n",
            "45250/700000: training loss=3.2981, dev loss=5.6540\n",
            "45300/700000: training loss=3.2898, dev loss=5.6345\n",
            "45350/700000: training loss=3.2871, dev loss=5.6372\n",
            "45400/700000: training loss=3.3161, dev loss=5.6986\n",
            "45450/700000: training loss=3.3323, dev loss=5.6260\n",
            "45500/700000: training loss=3.3060, dev loss=5.7070\n",
            "45550/700000: training loss=3.2748, dev loss=5.5944\n",
            "45600/700000: training loss=3.2972, dev loss=5.7367\n",
            "45650/700000: training loss=3.2866, dev loss=5.6737\n",
            "45700/700000: training loss=3.3028, dev loss=5.6411\n",
            "45750/700000: training loss=3.2928, dev loss=5.6091\n",
            "45800/700000: training loss=3.2619, dev loss=5.6679\n",
            "45850/700000: training loss=3.2744, dev loss=5.6693\n",
            "45900/700000: training loss=3.2729, dev loss=5.6614\n",
            "45950/700000: training loss=3.2629, dev loss=5.6981\n",
            "46000/700000: training loss=3.2764, dev loss=5.6985\n",
            "46050/700000: training loss=3.2740, dev loss=5.7587\n",
            "46100/700000: training loss=3.2356, dev loss=5.7116\n",
            "46150/700000: training loss=3.2552, dev loss=5.7081\n",
            "46200/700000: training loss=3.2534, dev loss=5.7049\n",
            "46250/700000: training loss=3.2420, dev loss=5.7424\n",
            "46300/700000: training loss=3.2373, dev loss=5.6725\n",
            "46350/700000: training loss=3.2512, dev loss=5.7328\n",
            "46400/700000: training loss=3.2513, dev loss=5.7429\n",
            "46450/700000: training loss=3.2488, dev loss=5.6598\n",
            "46500/700000: training loss=3.2595, dev loss=5.8236\n",
            "46550/700000: training loss=3.2528, dev loss=5.6542\n",
            "46600/700000: training loss=3.2166, dev loss=5.6195\n",
            "46650/700000: training loss=3.2529, dev loss=5.7137\n",
            "46700/700000: training loss=3.2297, dev loss=5.7493\n",
            "46750/700000: training loss=3.2440, dev loss=5.7064\n",
            "46800/700000: training loss=3.2373, dev loss=5.7184\n",
            "46850/700000: training loss=3.2394, dev loss=5.7674\n",
            "46900/700000: training loss=3.2118, dev loss=5.7203\n",
            "46950/700000: training loss=3.1960, dev loss=5.6977\n",
            "47000/700000: training loss=3.2264, dev loss=5.6690\n",
            "47050/700000: training loss=3.2252, dev loss=5.7056\n",
            "47100/700000: training loss=3.2239, dev loss=5.7090\n",
            "47150/700000: training loss=3.1859, dev loss=5.7219\n",
            "47200/700000: training loss=3.2087, dev loss=5.7708\n",
            "47250/700000: training loss=3.2030, dev loss=5.7781\n",
            "47300/700000: training loss=3.2002, dev loss=5.7663\n",
            "47350/700000: training loss=3.2241, dev loss=5.7508\n",
            "47400/700000: training loss=3.1645, dev loss=5.7203\n",
            "47450/700000: training loss=3.2097, dev loss=5.7184\n",
            "47500/700000: training loss=3.1925, dev loss=5.7320\n",
            "47550/700000: training loss=3.1902, dev loss=5.7929\n",
            "47600/700000: training loss=3.2043, dev loss=5.7194\n",
            "47650/700000: training loss=3.1486, dev loss=5.8103\n",
            "47700/700000: training loss=3.1662, dev loss=5.8081\n",
            "47750/700000: training loss=3.1687, dev loss=5.7908\n",
            "47800/700000: training loss=3.1988, dev loss=5.7356\n",
            "47850/700000: training loss=3.1673, dev loss=5.8322\n",
            "47900/700000: training loss=3.1650, dev loss=5.7875\n",
            "47950/700000: training loss=3.1612, dev loss=5.7466\n",
            "48000/700000: training loss=3.1294, dev loss=5.7724\n",
            "48050/700000: training loss=3.1345, dev loss=5.7694\n",
            "48100/700000: training loss=3.1511, dev loss=5.7877\n",
            "48150/700000: training loss=3.1214, dev loss=5.8299\n",
            "48200/700000: training loss=3.1504, dev loss=5.7496\n",
            "48250/700000: training loss=3.1447, dev loss=5.7891\n",
            "48300/700000: training loss=3.1449, dev loss=5.8038\n",
            "48350/700000: training loss=3.1361, dev loss=5.8265\n",
            "48400/700000: training loss=3.1481, dev loss=5.7739\n",
            "48450/700000: training loss=3.1200, dev loss=5.8095\n",
            "48500/700000: training loss=3.1197, dev loss=5.8256\n",
            "48550/700000: training loss=3.1052, dev loss=5.8139\n",
            "48600/700000: training loss=3.1084, dev loss=5.7911\n",
            "48650/700000: training loss=3.1510, dev loss=5.8975\n",
            "48700/700000: training loss=3.1396, dev loss=5.8584\n",
            "48750/700000: training loss=3.0902, dev loss=5.7955\n",
            "48800/700000: training loss=3.1443, dev loss=5.7540\n",
            "48850/700000: training loss=3.1231, dev loss=5.7962\n",
            "48900/700000: training loss=3.1046, dev loss=5.8151\n",
            "48950/700000: training loss=3.0931, dev loss=5.7789\n",
            "49000/700000: training loss=3.1061, dev loss=5.7919\n",
            "49050/700000: training loss=3.0878, dev loss=5.8138\n",
            "49100/700000: training loss=3.0700, dev loss=5.7683\n",
            "49150/700000: training loss=3.1013, dev loss=5.8128\n",
            "49200/700000: training loss=3.0861, dev loss=5.8329\n",
            "49250/700000: training loss=3.1050, dev loss=5.8793\n",
            "49300/700000: training loss=3.0843, dev loss=5.8331\n",
            "49350/700000: training loss=3.0948, dev loss=5.9086\n",
            "49400/700000: training loss=3.0688, dev loss=5.8172\n",
            "49450/700000: training loss=3.0727, dev loss=5.8566\n",
            "49500/700000: training loss=3.0728, dev loss=5.8467\n",
            "49550/700000: training loss=3.0638, dev loss=5.8279\n",
            "49600/700000: training loss=3.0950, dev loss=5.8814\n",
            "49650/700000: training loss=3.0529, dev loss=5.8315\n",
            "49700/700000: training loss=3.0482, dev loss=5.8603\n",
            "49750/700000: training loss=3.0711, dev loss=5.8337\n",
            "49800/700000: training loss=3.0606, dev loss=5.8685\n",
            "49850/700000: training loss=3.0237, dev loss=6.0012\n",
            "49900/700000: training loss=3.0314, dev loss=5.8705\n",
            "49950/700000: training loss=3.0760, dev loss=5.9107\n",
            "50000/700000: training loss=3.0388, dev loss=5.8987\n",
            "50050/700000: training loss=3.0228, dev loss=5.8766\n",
            "50100/700000: training loss=3.0473, dev loss=5.9690\n",
            "50150/700000: training loss=3.0520, dev loss=5.8372\n",
            "50200/700000: training loss=3.0393, dev loss=5.8684\n",
            "50250/700000: training loss=3.0333, dev loss=5.9049\n",
            "50300/700000: training loss=3.0085, dev loss=5.9024\n",
            "50350/700000: training loss=3.0378, dev loss=5.8661\n",
            "50400/700000: training loss=3.0314, dev loss=5.9271\n",
            "50450/700000: training loss=3.0013, dev loss=5.9447\n",
            "50500/700000: training loss=3.0149, dev loss=5.8369\n",
            "50550/700000: training loss=3.0210, dev loss=5.9122\n",
            "50600/700000: training loss=2.9950, dev loss=5.8352\n",
            "50650/700000: training loss=3.0088, dev loss=5.9463\n",
            "50700/700000: training loss=2.9939, dev loss=5.8855\n",
            "50750/700000: training loss=2.9962, dev loss=6.0237\n",
            "50800/700000: training loss=3.0126, dev loss=5.9072\n",
            "50850/700000: training loss=2.9950, dev loss=5.9182\n",
            "50900/700000: training loss=2.9801, dev loss=5.9062\n",
            "50950/700000: training loss=2.9759, dev loss=5.9193\n",
            "51000/700000: training loss=2.9791, dev loss=5.9701\n",
            "51050/700000: training loss=3.0014, dev loss=5.9422\n",
            "51100/700000: training loss=2.9654, dev loss=5.9602\n",
            "51150/700000: training loss=2.9658, dev loss=5.9871\n",
            "51200/700000: training loss=2.9935, dev loss=5.9574\n",
            "51250/700000: training loss=2.9859, dev loss=6.0052\n",
            "51300/700000: training loss=2.9511, dev loss=5.8694\n",
            "51350/700000: training loss=2.9612, dev loss=5.9234\n",
            "51400/700000: training loss=2.9942, dev loss=5.9918\n",
            "51450/700000: training loss=2.9335, dev loss=5.9376\n",
            "51500/700000: training loss=2.9502, dev loss=5.9850\n",
            "51550/700000: training loss=2.9750, dev loss=6.0431\n",
            "51600/700000: training loss=2.9700, dev loss=5.9374\n",
            "51650/700000: training loss=2.9776, dev loss=5.9826\n",
            "51700/700000: training loss=2.9416, dev loss=5.9635\n",
            "51750/700000: training loss=2.9557, dev loss=5.9286\n",
            "51800/700000: training loss=2.9347, dev loss=5.9702\n",
            "51850/700000: training loss=2.9383, dev loss=5.9267\n",
            "51900/700000: training loss=2.9029, dev loss=5.9487\n",
            "51950/700000: training loss=2.9611, dev loss=5.9252\n",
            "52000/700000: training loss=2.9193, dev loss=5.9393\n",
            "52050/700000: training loss=2.9307, dev loss=6.0506\n",
            "52100/700000: training loss=2.8786, dev loss=6.0343\n",
            "52150/700000: training loss=2.9532, dev loss=5.9606\n",
            "52200/700000: training loss=2.9104, dev loss=6.0197\n",
            "52250/700000: training loss=2.9312, dev loss=6.0071\n",
            "52300/700000: training loss=2.9312, dev loss=6.0042\n",
            "52350/700000: training loss=2.9058, dev loss=5.9732\n",
            "52400/700000: training loss=2.9035, dev loss=6.0553\n",
            "52450/700000: training loss=2.9346, dev loss=6.0060\n",
            "52500/700000: training loss=2.9012, dev loss=6.0005\n",
            "52550/700000: training loss=2.9111, dev loss=5.9944\n",
            "52600/700000: training loss=2.9047, dev loss=5.9898\n",
            "52650/700000: training loss=2.8934, dev loss=6.0731\n",
            "52700/700000: training loss=2.9148, dev loss=5.9861\n",
            "52750/700000: training loss=2.9244, dev loss=5.9574\n",
            "52800/700000: training loss=2.8870, dev loss=6.0533\n",
            "52850/700000: training loss=2.9143, dev loss=6.0545\n",
            "52900/700000: training loss=2.8820, dev loss=6.0062\n",
            "52950/700000: training loss=2.8881, dev loss=6.0618\n",
            "53000/700000: training loss=2.8849, dev loss=5.9524\n",
            "53050/700000: training loss=2.8941, dev loss=6.0305\n",
            "53100/700000: training loss=2.8654, dev loss=6.0324\n",
            "53150/700000: training loss=2.8619, dev loss=6.0320\n",
            "53200/700000: training loss=2.8439, dev loss=6.0445\n",
            "53250/700000: training loss=2.8411, dev loss=6.0488\n",
            "53300/700000: training loss=2.8937, dev loss=6.0931\n",
            "53350/700000: training loss=2.8808, dev loss=5.9699\n",
            "53400/700000: training loss=2.8476, dev loss=6.1070\n",
            "53450/700000: training loss=2.8829, dev loss=6.0555\n",
            "53500/700000: training loss=2.8609, dev loss=6.0858\n",
            "53550/700000: training loss=2.8312, dev loss=6.0614\n",
            "53600/700000: training loss=2.8524, dev loss=6.1122\n",
            "53650/700000: training loss=2.8487, dev loss=6.0669\n",
            "53700/700000: training loss=2.8432, dev loss=6.1304\n",
            "53750/700000: training loss=2.8254, dev loss=6.0955\n",
            "53800/700000: training loss=2.8696, dev loss=6.1311\n",
            "53850/700000: training loss=2.8621, dev loss=6.0872\n",
            "53900/700000: training loss=2.8027, dev loss=6.0301\n",
            "53950/700000: training loss=2.8037, dev loss=6.1231\n",
            "54000/700000: training loss=2.8180, dev loss=6.1002\n",
            "54050/700000: training loss=2.8410, dev loss=6.0463\n",
            "54100/700000: training loss=2.8298, dev loss=6.0918\n",
            "54150/700000: training loss=2.7975, dev loss=6.1127\n",
            "54200/700000: training loss=2.7786, dev loss=6.0679\n",
            "54250/700000: training loss=2.8284, dev loss=6.0811\n",
            "54300/700000: training loss=2.8054, dev loss=6.0755\n",
            "54350/700000: training loss=2.8103, dev loss=6.1522\n",
            "54400/700000: training loss=2.7969, dev loss=6.1463\n",
            "54450/700000: training loss=2.7847, dev loss=6.0961\n",
            "54500/700000: training loss=2.7951, dev loss=6.1557\n",
            "54550/700000: training loss=2.7752, dev loss=6.1506\n",
            "54600/700000: training loss=2.8014, dev loss=6.1922\n",
            "54650/700000: training loss=2.8010, dev loss=6.0667\n",
            "54700/700000: training loss=2.7744, dev loss=6.1386\n",
            "54750/700000: training loss=2.7747, dev loss=6.1382\n",
            "54800/700000: training loss=2.7571, dev loss=6.1621\n",
            "54850/700000: training loss=2.7764, dev loss=6.1381\n",
            "54900/700000: training loss=2.7597, dev loss=6.1518\n",
            "54950/700000: training loss=2.7428, dev loss=6.1901\n",
            "55000/700000: training loss=2.7800, dev loss=6.1133\n",
            "55050/700000: training loss=2.7768, dev loss=6.0930\n",
            "55100/700000: training loss=2.7467, dev loss=6.2268\n",
            "55150/700000: training loss=2.7689, dev loss=6.1098\n",
            "55200/700000: training loss=2.7670, dev loss=6.1870\n",
            "55250/700000: training loss=2.7432, dev loss=6.1290\n",
            "55300/700000: training loss=2.7340, dev loss=6.1538\n",
            "55350/700000: training loss=2.7273, dev loss=6.1324\n",
            "55400/700000: training loss=2.7600, dev loss=6.1359\n",
            "55450/700000: training loss=2.7110, dev loss=6.1867\n",
            "55500/700000: training loss=2.7574, dev loss=6.1979\n",
            "55550/700000: training loss=2.7050, dev loss=6.1970\n",
            "55600/700000: training loss=2.7424, dev loss=6.1891\n",
            "55650/700000: training loss=2.7293, dev loss=6.1392\n",
            "55700/700000: training loss=2.7384, dev loss=6.1224\n",
            "55750/700000: training loss=2.7145, dev loss=6.2296\n",
            "55800/700000: training loss=2.7126, dev loss=6.2632\n",
            "55850/700000: training loss=2.7183, dev loss=6.2568\n",
            "55900/700000: training loss=2.7127, dev loss=6.2404\n",
            "55950/700000: training loss=2.7228, dev loss=6.2176\n",
            "56000/700000: training loss=2.7321, dev loss=6.1794\n",
            "56050/700000: training loss=2.6907, dev loss=6.2299\n",
            "56100/700000: training loss=2.7164, dev loss=6.1767\n",
            "56150/700000: training loss=2.6792, dev loss=6.2138\n",
            "56200/700000: training loss=2.7036, dev loss=6.1806\n",
            "56250/700000: training loss=2.6934, dev loss=6.2198\n",
            "56300/700000: training loss=2.6994, dev loss=6.2363\n",
            "56350/700000: training loss=2.6589, dev loss=6.2166\n",
            "56400/700000: training loss=2.6853, dev loss=6.1756\n",
            "56450/700000: training loss=2.6810, dev loss=6.2737\n",
            "56500/700000: training loss=2.6815, dev loss=6.2652\n",
            "56550/700000: training loss=2.6755, dev loss=6.2644\n",
            "56600/700000: training loss=2.6711, dev loss=6.2239\n",
            "56650/700000: training loss=2.6718, dev loss=6.2571\n",
            "56700/700000: training loss=2.6684, dev loss=6.2280\n",
            "56750/700000: training loss=2.6673, dev loss=6.2008\n",
            "56800/700000: training loss=2.6757, dev loss=6.2975\n",
            "56850/700000: training loss=2.6695, dev loss=6.2283\n",
            "56900/700000: training loss=2.6636, dev loss=6.2938\n",
            "56950/700000: training loss=2.6610, dev loss=6.2436\n",
            "57000/700000: training loss=2.6754, dev loss=6.1901\n",
            "57050/700000: training loss=2.6605, dev loss=6.1707\n",
            "57100/700000: training loss=2.6400, dev loss=6.2811\n",
            "57150/700000: training loss=2.6623, dev loss=6.3044\n",
            "57200/700000: training loss=2.6420, dev loss=6.2292\n",
            "57250/700000: training loss=2.6383, dev loss=6.2591\n",
            "57300/700000: training loss=2.6407, dev loss=6.2912\n",
            "57350/700000: training loss=2.6179, dev loss=6.2915\n",
            "57400/700000: training loss=2.6244, dev loss=6.3115\n",
            "57450/700000: training loss=2.6369, dev loss=6.3646\n",
            "57500/700000: training loss=2.6388, dev loss=6.3226\n",
            "57550/700000: training loss=2.6267, dev loss=6.3005\n",
            "57600/700000: training loss=2.6271, dev loss=6.4084\n",
            "57650/700000: training loss=2.6030, dev loss=6.3151\n",
            "57700/700000: training loss=2.6268, dev loss=6.3756\n",
            "57750/700000: training loss=2.5826, dev loss=6.3898\n",
            "57800/700000: training loss=2.5851, dev loss=6.2807\n",
            "57850/700000: training loss=2.6237, dev loss=6.3985\n",
            "57900/700000: training loss=2.6218, dev loss=6.2990\n",
            "57950/700000: training loss=2.6087, dev loss=6.1841\n",
            "58000/700000: training loss=2.5955, dev loss=6.3659\n",
            "58050/700000: training loss=2.5920, dev loss=6.2800\n",
            "58100/700000: training loss=2.5921, dev loss=6.3705\n",
            "58150/700000: training loss=2.5964, dev loss=6.3955\n",
            "58200/700000: training loss=2.5945, dev loss=6.2759\n",
            "58250/700000: training loss=2.6179, dev loss=6.3127\n",
            "58300/700000: training loss=2.5754, dev loss=6.3434\n",
            "58350/700000: training loss=2.5928, dev loss=6.3305\n",
            "58400/700000: training loss=2.5564, dev loss=6.2871\n",
            "58450/700000: training loss=2.5884, dev loss=6.3487\n",
            "58500/700000: training loss=2.5603, dev loss=6.3395\n",
            "58550/700000: training loss=2.5475, dev loss=6.3770\n",
            "58600/700000: training loss=2.5485, dev loss=6.3654\n",
            "58650/700000: training loss=2.5704, dev loss=6.3087\n",
            "58700/700000: training loss=2.5368, dev loss=6.3682\n",
            "58750/700000: training loss=2.5502, dev loss=6.3921\n",
            "58800/700000: training loss=2.5914, dev loss=6.4210\n",
            "58850/700000: training loss=2.5435, dev loss=6.4170\n",
            "58900/700000: training loss=2.5360, dev loss=6.3980\n",
            "58950/700000: training loss=2.5455, dev loss=6.3774\n",
            "59000/700000: training loss=2.5496, dev loss=6.3321\n",
            "59050/700000: training loss=2.5157, dev loss=6.4264\n",
            "59100/700000: training loss=2.5578, dev loss=6.3379\n",
            "59150/700000: training loss=2.5405, dev loss=6.3975\n",
            "59200/700000: training loss=2.5196, dev loss=6.4410\n",
            "59250/700000: training loss=2.5371, dev loss=6.4636\n",
            "59300/700000: training loss=2.5392, dev loss=6.4026\n",
            "59350/700000: training loss=2.5200, dev loss=6.3590\n",
            "59400/700000: training loss=2.4946, dev loss=6.4023\n",
            "59450/700000: training loss=2.5107, dev loss=6.5003\n",
            "59500/700000: training loss=2.5142, dev loss=6.4595\n",
            "59550/700000: training loss=2.5033, dev loss=6.4399\n",
            "59600/700000: training loss=2.4932, dev loss=6.5550\n",
            "59650/700000: training loss=2.4841, dev loss=6.3714\n",
            "59700/700000: training loss=2.4965, dev loss=6.4247\n",
            "59750/700000: training loss=2.5144, dev loss=6.4468\n",
            "59800/700000: training loss=2.5250, dev loss=6.5038\n",
            "59850/700000: training loss=2.5115, dev loss=6.3913\n",
            "59900/700000: training loss=2.5352, dev loss=6.4495\n",
            "59950/700000: training loss=2.5119, dev loss=6.4866\n",
            "60000/700000: training loss=2.4995, dev loss=6.4994\n",
            "60050/700000: training loss=2.4854, dev loss=6.4238\n",
            "60100/700000: training loss=2.5104, dev loss=6.4561\n",
            "60150/700000: training loss=2.4896, dev loss=6.5023\n",
            "60200/700000: training loss=2.4774, dev loss=6.5053\n",
            "60250/700000: training loss=2.4669, dev loss=6.4314\n",
            "60300/700000: training loss=2.4731, dev loss=6.5182\n",
            "60350/700000: training loss=2.4901, dev loss=6.4406\n",
            "60400/700000: training loss=2.4708, dev loss=6.5178\n",
            "60450/700000: training loss=2.4696, dev loss=6.6004\n",
            "60500/700000: training loss=2.4482, dev loss=6.5112\n",
            "60550/700000: training loss=2.4607, dev loss=6.4330\n",
            "60600/700000: training loss=2.4367, dev loss=6.5237\n",
            "60650/700000: training loss=2.4444, dev loss=6.5021\n",
            "60700/700000: training loss=2.4550, dev loss=6.5311\n",
            "60750/700000: training loss=2.4383, dev loss=6.4663\n",
            "60800/700000: training loss=2.4621, dev loss=6.5484\n",
            "60850/700000: training loss=2.4425, dev loss=6.5120\n",
            "60900/700000: training loss=2.4256, dev loss=6.4706\n",
            "60950/700000: training loss=2.4362, dev loss=6.5401\n",
            "61000/700000: training loss=2.4374, dev loss=6.5081\n",
            "61050/700000: training loss=2.3929, dev loss=6.5255\n",
            "61100/700000: training loss=2.4096, dev loss=6.5560\n",
            "61150/700000: training loss=2.4230, dev loss=6.4852\n",
            "61200/700000: training loss=2.4187, dev loss=6.5445\n",
            "61250/700000: training loss=2.4199, dev loss=6.5078\n",
            "61300/700000: training loss=2.4093, dev loss=6.6366\n",
            "61350/700000: training loss=2.4158, dev loss=6.5865\n",
            "61400/700000: training loss=2.4272, dev loss=6.5519\n",
            "61450/700000: training loss=2.4299, dev loss=6.5877\n",
            "61500/700000: training loss=2.4261, dev loss=6.5641\n",
            "61550/700000: training loss=2.3997, dev loss=6.6339\n",
            "61600/700000: training loss=2.3867, dev loss=6.5946\n",
            "61650/700000: training loss=2.4122, dev loss=6.6207\n",
            "61700/700000: training loss=2.4030, dev loss=6.4814\n",
            "61750/700000: training loss=2.3767, dev loss=6.6220\n",
            "61800/700000: training loss=2.3784, dev loss=6.5963\n",
            "61850/700000: training loss=2.4014, dev loss=6.5990\n",
            "61900/700000: training loss=2.3872, dev loss=6.5536\n",
            "61950/700000: training loss=2.3775, dev loss=6.5878\n",
            "62000/700000: training loss=2.3894, dev loss=6.5408\n",
            "62050/700000: training loss=2.3652, dev loss=6.5852\n",
            "62100/700000: training loss=2.3620, dev loss=6.6598\n",
            "62150/700000: training loss=2.3703, dev loss=6.5731\n",
            "62200/700000: training loss=2.3731, dev loss=6.5385\n",
            "62250/700000: training loss=2.3555, dev loss=6.6344\n",
            "62300/700000: training loss=2.3787, dev loss=6.5611\n",
            "62350/700000: training loss=2.3483, dev loss=6.5549\n",
            "62400/700000: training loss=2.3814, dev loss=6.6413\n",
            "62450/700000: training loss=2.3649, dev loss=6.6417\n",
            "62500/700000: training loss=2.3722, dev loss=6.6936\n",
            "62550/700000: training loss=2.3301, dev loss=6.5784\n",
            "62600/700000: training loss=2.3623, dev loss=6.5702\n",
            "62650/700000: training loss=2.3323, dev loss=6.6666\n",
            "62700/700000: training loss=2.3396, dev loss=6.6329\n",
            "62750/700000: training loss=2.3598, dev loss=6.6876\n",
            "62800/700000: training loss=2.3395, dev loss=6.6787\n",
            "62850/700000: training loss=2.3144, dev loss=6.5838\n",
            "62900/700000: training loss=2.3209, dev loss=6.6777\n",
            "62950/700000: training loss=2.3301, dev loss=6.6080\n",
            "63000/700000: training loss=2.3109, dev loss=6.6833\n",
            "63050/700000: training loss=2.3528, dev loss=6.6918\n",
            "63100/700000: training loss=2.2860, dev loss=6.6752\n",
            "63150/700000: training loss=2.3179, dev loss=6.6725\n",
            "63200/700000: training loss=2.3129, dev loss=6.6741\n",
            "63250/700000: training loss=2.3097, dev loss=6.6082\n",
            "63300/700000: training loss=2.3112, dev loss=6.6342\n",
            "63350/700000: training loss=2.3343, dev loss=6.5570\n",
            "63400/700000: training loss=2.3012, dev loss=6.6154\n",
            "63450/700000: training loss=2.3011, dev loss=6.6652\n",
            "63500/700000: training loss=2.3002, dev loss=6.6848\n",
            "63550/700000: training loss=2.3132, dev loss=6.7032\n",
            "63600/700000: training loss=2.3166, dev loss=6.5974\n",
            "63650/700000: training loss=2.2809, dev loss=6.7517\n",
            "63700/700000: training loss=2.2974, dev loss=6.6296\n",
            "63750/700000: training loss=2.3052, dev loss=6.6002\n",
            "63800/700000: training loss=2.2882, dev loss=6.6671\n",
            "63850/700000: training loss=2.3058, dev loss=6.7403\n",
            "63900/700000: training loss=2.2677, dev loss=6.6540\n",
            "63950/700000: training loss=2.2560, dev loss=6.7353\n",
            "64000/700000: training loss=2.2731, dev loss=6.6195\n",
            "64050/700000: training loss=2.2643, dev loss=6.6783\n",
            "64100/700000: training loss=2.2743, dev loss=6.6951\n",
            "64150/700000: training loss=2.2665, dev loss=6.6764\n",
            "64200/700000: training loss=2.2390, dev loss=6.6942\n",
            "64250/700000: training loss=2.2449, dev loss=6.7416\n",
            "64300/700000: training loss=2.2585, dev loss=6.8264\n",
            "64350/700000: training loss=2.2652, dev loss=6.7231\n",
            "64400/700000: training loss=2.2422, dev loss=6.6970\n",
            "64450/700000: training loss=2.2519, dev loss=6.7098\n",
            "64500/700000: training loss=2.2458, dev loss=6.7753\n",
            "64550/700000: training loss=2.2446, dev loss=6.7450\n",
            "64600/700000: training loss=2.2621, dev loss=6.6942\n",
            "64650/700000: training loss=2.2467, dev loss=6.8258\n",
            "64700/700000: training loss=2.2512, dev loss=6.7186\n",
            "64750/700000: training loss=2.2381, dev loss=6.7275\n",
            "64800/700000: training loss=2.2255, dev loss=6.7275\n",
            "64850/700000: training loss=2.2106, dev loss=6.7006\n",
            "64900/700000: training loss=2.2444, dev loss=6.7365\n",
            "64950/700000: training loss=2.2098, dev loss=6.7984\n",
            "65000/700000: training loss=2.2154, dev loss=6.8169\n",
            "65050/700000: training loss=2.2450, dev loss=6.7725\n",
            "65100/700000: training loss=2.2134, dev loss=6.7642\n",
            "65150/700000: training loss=2.2392, dev loss=6.7654\n",
            "65200/700000: training loss=2.2021, dev loss=6.7778\n",
            "65250/700000: training loss=2.2019, dev loss=6.7789\n",
            "65300/700000: training loss=2.2035, dev loss=6.8741\n",
            "65350/700000: training loss=2.2159, dev loss=6.7556\n",
            "65400/700000: training loss=2.2088, dev loss=6.8103\n",
            "65450/700000: training loss=2.2192, dev loss=6.8037\n",
            "65500/700000: training loss=2.2064, dev loss=6.7550\n",
            "65550/700000: training loss=2.1918, dev loss=6.8030\n",
            "65600/700000: training loss=2.2146, dev loss=6.8155\n",
            "65650/700000: training loss=2.1766, dev loss=6.8197\n",
            "65700/700000: training loss=2.1776, dev loss=6.7883\n",
            "65750/700000: training loss=2.1749, dev loss=6.7983\n",
            "65800/700000: training loss=2.1857, dev loss=6.7583\n",
            "65850/700000: training loss=2.1891, dev loss=6.7785\n",
            "65900/700000: training loss=2.1532, dev loss=6.8481\n",
            "65950/700000: training loss=2.1798, dev loss=6.8409\n",
            "66000/700000: training loss=2.1670, dev loss=6.7346\n",
            "66050/700000: training loss=2.1479, dev loss=6.8425\n",
            "66100/700000: training loss=2.1734, dev loss=6.8304\n",
            "66150/700000: training loss=2.1361, dev loss=6.8247\n",
            "66200/700000: training loss=2.1710, dev loss=6.9025\n",
            "66250/700000: training loss=2.1637, dev loss=6.8001\n",
            "66300/700000: training loss=2.1517, dev loss=6.8139\n",
            "66350/700000: training loss=2.1515, dev loss=6.7432\n",
            "66400/700000: training loss=2.1511, dev loss=6.8234\n",
            "66450/700000: training loss=2.1445, dev loss=6.8138\n",
            "66500/700000: training loss=2.1509, dev loss=6.8679\n",
            "66550/700000: training loss=2.1389, dev loss=6.8913\n",
            "66600/700000: training loss=2.1316, dev loss=6.8752\n",
            "66650/700000: training loss=2.1540, dev loss=6.7545\n",
            "66700/700000: training loss=2.1514, dev loss=6.8070\n",
            "66750/700000: training loss=2.1323, dev loss=6.9918\n",
            "66800/700000: training loss=2.1309, dev loss=6.8920\n",
            "66850/700000: training loss=2.1153, dev loss=6.9071\n",
            "66900/700000: training loss=2.1148, dev loss=6.8402\n",
            "66950/700000: training loss=2.1222, dev loss=6.7776\n",
            "67000/700000: training loss=2.0985, dev loss=6.8991\n",
            "67050/700000: training loss=2.0994, dev loss=6.8979\n",
            "67100/700000: training loss=2.1062, dev loss=6.9582\n",
            "67150/700000: training loss=2.1166, dev loss=6.9407\n",
            "67200/700000: training loss=2.1445, dev loss=6.8516\n",
            "67250/700000: training loss=2.1212, dev loss=6.8668\n",
            "67300/700000: training loss=2.1297, dev loss=6.9398\n",
            "67350/700000: training loss=2.0975, dev loss=6.9133\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-0ba235989465>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;31m# Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;31m# print(f'{outputs.shape=}, {Yb.shape=}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m   \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m   \u001b[0mrunning_loss_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(lossi)"
      ],
      "metadata": {
        "id": "jjTgq78BCCRd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "23db3163-6363-4376-dd13-374238db29ff"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fe548183fd0>]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(lossi_dev)"
      ],
      "metadata": {
        "id": "c6QgPaXDCHha",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "1f880cba-5801-43e0-a089-a7dfdf77760b"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fe554224130>]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "fY3YqGX2TMmv"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample the model"
      ],
      "metadata": {
        "id": "7kTWwn5Q8x4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g_gpu = torch.Generator(device=device).manual_seed(2147483647) # for reproducibility\n",
        "\n",
        "# 0, which is new line char, is a reasonable start (seed) char\n",
        "idx = torch.tensor([[0]]).to(device)\n",
        "new_idx = net.generate(idx, BLOCK_SIZE * 2)\n",
        "\n",
        "print(decode(new_idx.view(-1).tolist()))"
      ],
      "metadata": {
        "id": "CGiFf5b280CQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3364ba25-6849-46ae-ebfe-a66e9b70a74c"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out of a restraint, EXT : Which triumph roared and not delivers thee : thou lies didst in a life, If I swear for I stood for thee will use to light, I beggarly proportion in a thing I looked fast my soul Be prey. HORTENSIO : And fear, And ne ' er - As pretty degree, Petruchio, I seize vengeance for me, As well - Both is my fault son Who brings makes This and by heaven, And she serve I was a woman Will he is banish ' Twas made the supreme me, As I would Be executed ' s weapon, I am the crown About a time in great does or say well my lips in a bless lamenta of my life be j adjoining is my brother eye - Both ' such vanity, As I heard it is the bless tears little divine, or two And blow, That know the poor execution Of said. As little\n"
          ]
        }
      ]
    }
  ]
}