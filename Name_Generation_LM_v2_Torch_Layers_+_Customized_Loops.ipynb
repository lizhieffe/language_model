{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZN1uqItoHNWMo+P/PyM3G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizhieffe/language_model/blob/main/Name_Generation_LM_v2_Torch_Layers_%2B_Customized_Loops.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NctKHdOgc2_D"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "USE_GPU = False\n",
        "BLOCK_SIZE = 7 # Context length: how many chars do we take to predict the next one?"
      ],
      "metadata": {
        "id": "QRj4U-ZPc9YS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup GPU"
      ],
      "metadata": {
        "id": "s-1y_yqOc9q8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_GPU:\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  assert device != 'cpu', \"GPU is not available\"\n",
        "else:\n",
        "  device = 'cpu'\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqoaRfk7dBRP",
        "outputId": "19308ae6-0560-41f1-bee8-85279faa0de2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Util Functions"
      ],
      "metadata": {
        "id": "W4sWuWcfZm9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _total_params(layers):\n",
        "  \"\"\" Get the total parameter number.\n",
        "\n",
        "  Args:\n",
        "    layers: the list of layers of the model\n",
        "\n",
        "  Returns:\n",
        "    Number of total parameters\n",
        "  \"\"\"\n",
        "  total_params = 0\n",
        "  for l in layers:\n",
        "    for p in l.parameters():\n",
        "      total_params += p.data.nelement()\n",
        "  return total_params"
      ],
      "metadata": {
        "id": "IKIeOra1ZoMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _sample_one_batch(X, Y, batch_size, generator):\n",
        "  \"\"\"Sample from ds and generate a batch.\n",
        "\n",
        "  Args:\n",
        "    X: features of ds\n",
        "    Y: labels of ds\n",
        "    batch_size: batch size\n",
        "    generator: a pseudorandom number generator for sampling\n",
        "  Returns:\n",
        "    Xb: batched features\n",
        "    Yb: batched labels\n",
        "  \"\"\"\n",
        "  ix = torch.randint(0, X.shape[0], (batch_size, ), generator=generator).to(device)\n",
        "  # print(f'{ix.device=}')\n",
        "  Xb, Yb = X[ix], Y[ix]\n",
        "  return Xb, Yb"
      ],
      "metadata": {
        "id": "jtobwSgeQQf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _calculate_loss(Xb, Yb, layers):\n",
        "  \"\"\" Calculate loss.\n",
        "\n",
        "  Args:\n",
        "    Xb: the feature batch\n",
        "    Yb: the label batch\n",
        "    layers: the layers of the model\n",
        "\n",
        "  Returns:\n",
        "    loss: the calculated loss\n",
        "  \"\"\"\n",
        "  emb = C[Xb]\n",
        "  # print(f'{emb.device=}')\n",
        "  x = emb.view(emb.shape[0], -1)\n",
        "  # print(f'{x.device=}')\n",
        "  for l in layers:\n",
        "    x = l(x)\n",
        "    # print(f'{x.device=}')\n",
        "  loss = F.cross_entropy(x, Yb)\n",
        "  # print(f'{loss.device=}')\n",
        "  return loss"
      ],
      "metadata": {
        "id": "ML2jiL3zR0jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "CXSgUDhjdCxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the names.txt file from github\n",
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
        "\n",
        "# read in all the words\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "words[:8]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uw1N2UjndEDQ",
        "outputId": "8eaf1c73-9cfc-4071-c230-cfc54abb8a7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-30 16:20:39--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt.3’\n",
            "\n",
            "names.txt.3         100%[===================>] 222.80K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-09-30 16:20:39 (6.37 MB/s) - ‘names.txt.3’ saved [228145/228145]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_Lr-G6NdF62",
        "outputId": "a28b6dfe-1df7-45a9-d5df-2558d817ffa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32033"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build vocabulary"
      ],
      "metadata": {
        "id": "ZFA5J8NjdHj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(''.join(words))))\n",
        "\n",
        "stoi = {c:i+1 for i,c in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "\n",
        "itos = {i:c for c,i in stoi.items()}\n",
        "\n",
        "assert len(stoi) == len(itos)\n",
        "assert len(stoi) == 27\n",
        "\n",
        "vocab_size = len(stoi)"
      ],
      "metadata": {
        "id": "K31lXht9dIww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create DS"
      ],
      "metadata": {
        "id": "tnC-XqfXdKoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataset(words):\n",
        "  X = []\n",
        "  Y = []\n",
        "  for w in words:\n",
        "    context = [0] * BLOCK_SIZE\n",
        "    for c in w + '.':\n",
        "      iy = stoi[c]\n",
        "      X.append(context)\n",
        "      Y.append(iy)\n",
        "      context = context[1:] + [iy]\n",
        "\n",
        "  X = torch.tensor(X).to(device)\n",
        "  Y = torch.tensor(Y).to(device)\n",
        "  return X, Y"
      ],
      "metadata": {
        "id": "it6_-hY4dRxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "\n",
        "n1 = int(0.6 * len(words))\n",
        "n2 = int(0.8 * len(words))\n",
        "\n",
        "print(f'total size = {len(words)}, n1 = {n1}, n2 = {n2}')\n",
        "\n",
        "Xtr, Ytr = build_dataset(words[:n1])\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])\n",
        "Xte, Yte = build_dataset(words[n2:])\n",
        "\n",
        "assert Xtr.shape[0] == Ytr.shape[0]\n",
        "assert Xdev.shape[0] == Ydev.shape[0]\n",
        "assert Xte.shape[0] == Yte.shape[0]\n",
        "\n",
        "print(f'{Xtr.shape=}, {Ytr.shape=}')\n",
        "print(f'{Xdev.shape=}, {Ydev.shape=}')\n",
        "print(f'{Xte.shape=}, {Yte.shape=}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYwNe-L5dStW",
        "outputId": "a6c71348-066f-4524-cd8d-3e02a5cf2e89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total size = 32033, n1 = 19219, n2 = 25626\n",
            "Xtr.shape=torch.Size([137024, 7]), Ytr.shape=torch.Size([137024])\n",
            "Xdev.shape=torch.Size([45601, 7]), Ydev.shape=torch.Size([45601])\n",
            "Xte.shape=torch.Size([45521, 7]), Yte.shape=torch.Size([45521])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(20):\n",
        "  print(f\"{''.join(itos[ix.item()] for ix in Xtr[i])} ---> {itos[Ytr[i].item()]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vB04TMXwe8Rk",
        "outputId": "a39123cf-4908-46f6-b944-22fdae9a5dd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "....... ---> y\n",
            "......y ---> u\n",
            ".....yu ---> h\n",
            "....yuh ---> e\n",
            "...yuhe ---> n\n",
            "..yuhen ---> g\n",
            ".yuheng ---> .\n",
            "....... ---> d\n",
            "......d ---> i\n",
            ".....di ---> o\n",
            "....dio ---> n\n",
            "...dion ---> d\n",
            "..diond ---> r\n",
            ".diondr ---> e\n",
            "diondre ---> .\n",
            "....... ---> x\n",
            "......x ---> a\n",
            ".....xa ---> v\n",
            "....xav ---> i\n",
            "...xavi ---> e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "bm2SpJ6CfOHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 100 # the number of neurons in the hidden layer of the MLP\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility"
      ],
      "metadata": {
        "id": "LbOD4GXUfPHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "C = torch.randn((vocab_size, n_embd), generator=g).to(device)\n",
        "\n",
        "layers = [\n",
        "    torch.nn.Linear(n_embd * BLOCK_SIZE, n_hidden, bias=True).to(device),\n",
        "    torch.nn.Tanh().to(device),\n",
        "    torch.nn.Linear(n_hidden, n_hidden, bias=True).to(device),\n",
        "    torch.nn.Tanh().to(device),\n",
        "    torch.nn.Linear(n_hidden, n_hidden, bias=True).to(device),\n",
        "    torch.nn.Tanh().to(device),\n",
        "    torch.nn.Linear(n_hidden, vocab_size, bias=True).to(device)\n",
        "]\n",
        "\n",
        "print(f'total params = {_total_params(layers)}')"
      ],
      "metadata": {
        "id": "f-XWUSwCfZR3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5ec0ba1-a735-458e-aa31-1ff55f1f61c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total params = 30027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_steps = 200000\n",
        "batch_size = 32000\n",
        "lossi = []\n",
        "lossi_dev = []\n",
        "ud = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "  Xb, Yb = _sample_one_batch(Xtr, Ytr, batch_size, g)\n",
        "  loss = _calculate_loss(Xb, Yb, layers)\n",
        "\n",
        "  # emb = C[Xb]\n",
        "\n",
        "  # x = emb.view(emb.shape[0], -1)\n",
        "  # for l in layers:\n",
        "  #   x = l(x)\n",
        "\n",
        "  # loss = F.cross_entropy(x, Yb)\n",
        "\n",
        "  for l in layers:\n",
        "    l.zero_grad()\n",
        "  loss.backward()\n",
        "\n",
        "  Xb_dev, Yb_dev = _sample_one_batch(Xdev, Ydev, batch_size, g)\n",
        "  loss_dev = _calculate_loss(Xb_dev, Yb_dev, layers)\n",
        "\n",
        "  # update\n",
        "  lr = 0.1 if i < 15000 else 0.01 # step learning rate decay\n",
        "  # lr = 0.01\n",
        "  for l in layers:\n",
        "    for p in l.parameters():\n",
        "      # print(f'{p.data.shape=}, {p.grad.shape=}')\n",
        "      p.data -= lr * p.grad\n",
        "\n",
        "  # Track status\n",
        "  if i % 1000 == 0:\n",
        "    print(f'{i}/{max_steps}: training loss={loss.item():.4f}, dev loss={loss_dev.item():.4f}')\n",
        "  lossi.append(loss.log10().item())\n",
        "  lossi_dev.append(loss_dev.log10().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "ZUPJnuT_g1bA",
        "outputId": "0964cdc6-2c70-451c-bd5f-35ebcbe08223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0/200000: training loss=3.2814, dev loss=3.2837\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-1ed2b334063d>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mXb_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYb_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sample_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "C.device\n",
        "Xb.device\n",
        "# for l in layers:\n",
        "#   print(f'{l.device=}')"
      ],
      "metadata": {
        "id": "eSDgHWTNb1VD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(lossi)"
      ],
      "metadata": {
        "id": "r-tpE7UonNrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(lossi_dev)"
      ],
      "metadata": {
        "id": "VO9zKx7ZUeSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g_gpu = torch.Generator(device=device).manual_seed(2147483647) # for reproducibility\n",
        "\n",
        "out = []\n",
        "for _ in range(30):\n",
        "  context = [0] * BLOCK_SIZE\n",
        "\n",
        "  str = ''\n",
        "  while True:\n",
        "\n",
        "    xemb = C[torch.tensor([context])]\n",
        "    x = xemb.view(xemb.shape[0], -1)\n",
        "    for l in layers:\n",
        "      x = l(x)\n",
        "    logits = x\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    iy = torch.multinomial(probs, num_samples=1, replacement=True, generator=g_gpu)\n",
        "\n",
        "    if iy == 0:\n",
        "      str\n",
        "      out.append(str)\n",
        "      break\n",
        "    else:\n",
        "      context = context[1:] + [iy]\n",
        "      str += itos[iy.item()]\n",
        "\n",
        "for w in out:\n",
        "  print(w)\n"
      ],
      "metadata": {
        "id": "Bph2RnQSnUas"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}