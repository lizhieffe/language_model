{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMoeXgtAQ3p5aTQikjb++wa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizhieffe/language_model/blob/main/Name_Generation_LM_v3_Torch_Library.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRMxsDXmpbMf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "USE_GPU = True\n",
        "\n",
        "BLOCK_SIZE = 7 # Context length: how many chars do we take to predict the next one?"
      ],
      "metadata": {
        "id": "vDHUDjtsph4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup GPU"
      ],
      "metadata": {
        "id": "NZIvnPdcps3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_GPU:\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  assert device != 'cpu', \"GPU is not available\"\n",
        "else:\n",
        "  device = 'cpu'\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sd95wthQptwk",
        "outputId": "edb9ebad-ae64-47dd-e072-6d23456d392c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Util Functions"
      ],
      "metadata": {
        "id": "hHD6RNbQpv4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _total_params(layers):\n",
        "  \"\"\" Get the total parameter number.\n",
        "\n",
        "  Args:\n",
        "    layers: the list of layers of the model\n",
        "\n",
        "  Returns:\n",
        "    Number of total parameters\n",
        "  \"\"\"\n",
        "  total_params = 0\n",
        "  for l in layers:\n",
        "    for p in l.parameters():\n",
        "      total_params += p.data.nelement()\n",
        "  return total_params"
      ],
      "metadata": {
        "id": "IKIeOra1ZoMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _sample_one_batch(X, Y, batch_size, generator):\n",
        "  \"\"\"Sample from ds and generate a batch.\n",
        "\n",
        "  Args:\n",
        "    X: features of ds\n",
        "    Y: labels of ds\n",
        "    batch_size: batch size\n",
        "    generator: a pseudorandom number generator for sampling\n",
        "  Returns:\n",
        "    Xb: batched features\n",
        "    Yb: batched labels\n",
        "  \"\"\"\n",
        "  ix = torch.randint(0, X.shape[0], (batch_size, ), generator=generator).to(device)\n",
        "  # print(f'{ix.device=}')\n",
        "  Xb, Yb = X[ix], Y[ix]\n",
        "  return Xb, Yb"
      ],
      "metadata": {
        "id": "jtobwSgeQQf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _calculate_loss(Xb, Yb, layers):\n",
        "  \"\"\" Calculate loss.\n",
        "\n",
        "  Args:\n",
        "    Xb: the feature batch\n",
        "    Yb: the label batch\n",
        "    layers: the layers of the model\n",
        "\n",
        "  Returns:\n",
        "    loss: the calculated loss\n",
        "  \"\"\"\n",
        "  emb = C[Xb]\n",
        "  # print(f'{emb.device=}')\n",
        "  x = emb.view(emb.shape[0], -1)\n",
        "  # print(f'{x.device=}')\n",
        "  for l in layers:\n",
        "    x = l(x)\n",
        "    # print(f'{x.device=}')\n",
        "  loss = F.cross_entropy(x, Yb)\n",
        "  # print(f'{loss.device=}')\n",
        "  return loss"
      ],
      "metadata": {
        "id": "ML2jiL3zR0jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "CXSgUDhjdCxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the names.txt file from github\n",
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
        "\n",
        "# read in all the words\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "words[:8]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uw1N2UjndEDQ",
        "outputId": "ce17b3e4-f751-4f16-de5b-b88b3441fe05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-30 16:21:44--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt.5’\n",
            "\n",
            "names.txt.5         100%[===================>] 222.80K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-09-30 16:21:44 (7.85 MB/s) - ‘names.txt.5’ saved [228145/228145]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
            ]
          },
          "metadata": {},
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_Lr-G6NdF62",
        "outputId": "10567f5b-ee0e-43d6-a7b2-6526a797f828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32033"
            ]
          },
          "metadata": {},
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build vocabulary"
      ],
      "metadata": {
        "id": "ZFA5J8NjdHj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(''.join(words))))\n",
        "\n",
        "stoi = {c:i+1 for i,c in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "\n",
        "itos = {i:c for c,i in stoi.items()}\n",
        "\n",
        "assert len(stoi) == len(itos)\n",
        "assert len(stoi) == 27\n",
        "\n",
        "vocab_size = len(stoi)"
      ],
      "metadata": {
        "id": "K31lXht9dIww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create DS"
      ],
      "metadata": {
        "id": "tnC-XqfXdKoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataset(words):\n",
        "  X = []\n",
        "  Y = []\n",
        "  for w in words:\n",
        "    context = [0] * BLOCK_SIZE\n",
        "    for c in w + '.':\n",
        "      iy = stoi[c]\n",
        "      X.append(context)\n",
        "      Y.append(iy)\n",
        "      context = context[1:] + [iy]\n",
        "\n",
        "  X = torch.tensor(X).to(device)\n",
        "  Y = torch.tensor(Y).to(device)\n",
        "  return X, Y"
      ],
      "metadata": {
        "id": "it6_-hY4dRxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "\n",
        "n1 = int(0.6 * len(words))\n",
        "n2 = int(0.8 * len(words))\n",
        "\n",
        "print(f'total size = {len(words)}, n1 = {n1}, n2 = {n2}')\n",
        "\n",
        "Xtr, Ytr = build_dataset(words[:n1])\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])\n",
        "Xte, Yte = build_dataset(words[n2:])\n",
        "\n",
        "assert Xtr.shape[0] == Ytr.shape[0]\n",
        "assert Xdev.shape[0] == Ydev.shape[0]\n",
        "assert Xte.shape[0] == Yte.shape[0]\n",
        "\n",
        "print(f'{Xtr.shape=}, {Ytr.shape=}')\n",
        "print(f'{Xdev.shape=}, {Ydev.shape=}')\n",
        "print(f'{Xte.shape=}, {Yte.shape=}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYwNe-L5dStW",
        "outputId": "159c7db7-538f-4439-d26b-18ac7c077db1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total size = 32033, n1 = 19219, n2 = 25626\n",
            "Xtr.shape=torch.Size([137024, 7]), Ytr.shape=torch.Size([137024])\n",
            "Xdev.shape=torch.Size([45601, 7]), Ydev.shape=torch.Size([45601])\n",
            "Xte.shape=torch.Size([45521, 7]), Yte.shape=torch.Size([45521])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(20):\n",
        "  print(f\"{''.join(itos[ix.item()] for ix in Xtr[i])} ---> {itos[Ytr[i].item()]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vB04TMXwe8Rk",
        "outputId": "f040fa56-411e-4022-addd-9f1648154c45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "....... ---> y\n",
            "......y ---> u\n",
            ".....yu ---> h\n",
            "....yuh ---> e\n",
            "...yuhe ---> n\n",
            "..yuhen ---> g\n",
            ".yuheng ---> .\n",
            "....... ---> d\n",
            "......d ---> i\n",
            ".....di ---> o\n",
            "....dio ---> n\n",
            "...dion ---> d\n",
            "..diond ---> r\n",
            ".diondr ---> e\n",
            "diondre ---> .\n",
            "....... ---> x\n",
            "......x ---> a\n",
            ".....xa ---> v\n",
            "....xav ---> i\n",
            "...xavi ---> e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "bm2SpJ6CfOHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "N_HIDDEN = 100 # the number of neurons in the hidden layer of the MLP\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility"
      ],
      "metadata": {
        "id": "lAE6DUi2qcNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model class"
      ],
      "metadata": {
        "id": "o2LWaIYbri0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, n_embd, block_size, vocab_size, generator=None):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embd = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=n_embd).to(device)\n",
        "    self.linear1 = torch.nn.Linear(block_size * n_embd, N_HIDDEN, bias=True).to(device)\n",
        "    self.tanh1 = torch.nn.Tanh().to(device)\n",
        "    self.linear2 = torch.nn.Linear(N_HIDDEN, N_HIDDEN, bias=True).to(device)\n",
        "    self.tanh2 = torch.nn.Tanh().to(device)\n",
        "    self.linear3 = torch.nn.Linear(N_HIDDEN, N_HIDDEN, bias=True).to(device)\n",
        "    self.tanh3 = torch.nn.Tanh().to(device)\n",
        "    self.linear_logits = torch.nn.Linear(N_HIDDEN, vocab_size, bias=True).to(device)\n",
        "\n",
        "    self.ffn_layers = [\n",
        "        self.linear1,\n",
        "        self.tanh1,\n",
        "        self.linear2,\n",
        "        self.tanh2,\n",
        "        self.linear3,\n",
        "        self.tanh3,\n",
        "        self.linear_logits,\n",
        "    ]\n",
        "\n",
        "    self.layers = self.ffn_layers + [self.embd]\n",
        "\n",
        "  def forward(self, x):\n",
        "    xemb = self.embd(x)\n",
        "    y = xemb.view(xemb.shape[0], -1)\n",
        "    for l in self.ffn_layers:\n",
        "      y = l(y)\n",
        "    return y\n",
        "\n",
        "net = Net(n_embd=n_embd, block_size=BLOCK_SIZE, vocab_size=vocab_size)"
      ],
      "metadata": {
        "id": "3PcKxL7zqDhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_total_params = 0\n",
        "\n",
        "for p in net.parameters():\n",
        "  _total_params += p.nelement()\n",
        "  # print(f'{p.data.shape=}')\n",
        "\n",
        "print(f'Total params = {_total_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eMi8TezrkY4",
        "outputId": "ca2e7642-0bd4-4fca-e6d0-fd6735d80060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total params = 30297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define loss fn and optimizer"
      ],
      "metadata": {
        "id": "m04BZ-9f26w7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "5xJsIU2z28us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "5ObPwkVj3MFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_steps = 200000\n",
        "batch_size = 32000\n",
        "lossi = []\n",
        "lossi_dev = []\n",
        "ud = []\n",
        "\n",
        "running_loss = 0.0\n",
        "running_loss_dev = 0.0\n",
        "running_loss_steps = 0\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # Forward\n",
        "  Xb, Yb = _sample_one_batch(Xtr, Ytr, batch_size, generator=g)\n",
        "  optimizer.zero_grad()\n",
        "  outputs = net(Xb)\n",
        "\n",
        "  # Loss\n",
        "  loss = loss_fn(outputs, Yb)\n",
        "  running_loss += loss.item()\n",
        "  running_loss_steps += 1\n",
        "\n",
        "  # Eval dev DS\n",
        "  Xb_dev, Yb_dev = _sample_one_batch(Xdev, Ydev, batch_size, generator=g)\n",
        "  outputs_dev = net(Xb_dev)\n",
        "  loss_dev = loss_fn(outputs_dev, Yb_dev)\n",
        "  running_loss_dev += loss_dev.item()\n",
        "\n",
        "  # Update\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # Track status\n",
        "  if i % 1000 == 0:\n",
        "    print(f'{i}/{max_steps}: training loss={running_loss/running_loss_steps:.4f}, dev loss={running_loss_dev/running_loss_steps:.4f}')\n",
        "    running_loss = 0.0\n",
        "    running_loss_dev = 0.0\n",
        "    running_loss_steps = 0\n",
        "\n",
        "  lossi.append(loss.log10().item())\n",
        "  lossi_dev.append(loss_dev.log10().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3BtGf_U93M9V",
        "outputId": "5de50a53-64f7-44a7-d832-e9fde4bfaefc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0/200000: training loss=3.3001, dev loss=3.2996\n",
            "1000/200000: training loss=2.8431, dev loss=2.8443\n",
            "2000/200000: training loss=2.5777, dev loss=2.5776\n",
            "3000/200000: training loss=2.5144, dev loss=2.5131\n",
            "4000/200000: training loss=2.4738, dev loss=2.4711\n",
            "5000/200000: training loss=2.4415, dev loss=2.4387\n",
            "6000/200000: training loss=2.4119, dev loss=2.4097\n",
            "7000/200000: training loss=2.3866, dev loss=2.3843\n",
            "8000/200000: training loss=2.3657, dev loss=2.3637\n",
            "9000/200000: training loss=2.3487, dev loss=2.3472\n",
            "10000/200000: training loss=2.3338, dev loss=2.3330\n",
            "11000/200000: training loss=2.3202, dev loss=2.3201\n",
            "12000/200000: training loss=2.3079, dev loss=2.3081\n",
            "13000/200000: training loss=2.2972, dev loss=2.2975\n",
            "14000/200000: training loss=2.2871, dev loss=2.2884\n",
            "15000/200000: training loss=2.2779, dev loss=2.2796\n",
            "16000/200000: training loss=2.2699, dev loss=2.2720\n",
            "17000/200000: training loss=2.2617, dev loss=2.2647\n",
            "18000/200000: training loss=2.2548, dev loss=2.2580\n",
            "19000/200000: training loss=2.2476, dev loss=2.2516\n",
            "20000/200000: training loss=2.2416, dev loss=2.2461\n",
            "21000/200000: training loss=2.2359, dev loss=2.2402\n",
            "22000/200000: training loss=2.2302, dev loss=2.2356\n",
            "23000/200000: training loss=2.2244, dev loss=2.2306\n",
            "24000/200000: training loss=2.2190, dev loss=2.2255\n",
            "25000/200000: training loss=2.2139, dev loss=2.2216\n",
            "26000/200000: training loss=2.2092, dev loss=2.2172\n",
            "27000/200000: training loss=2.2046, dev loss=2.2132\n",
            "28000/200000: training loss=2.2001, dev loss=2.2095\n",
            "29000/200000: training loss=2.1957, dev loss=2.2058\n",
            "30000/200000: training loss=2.1918, dev loss=2.2022\n",
            "31000/200000: training loss=2.1872, dev loss=2.1987\n",
            "32000/200000: training loss=2.1833, dev loss=2.1953\n",
            "33000/200000: training loss=2.1791, dev loss=2.1921\n",
            "34000/200000: training loss=2.1756, dev loss=2.1895\n",
            "35000/200000: training loss=2.1717, dev loss=2.1864\n",
            "36000/200000: training loss=2.1678, dev loss=2.1829\n",
            "37000/200000: training loss=2.1644, dev loss=2.1811\n",
            "38000/200000: training loss=2.1612, dev loss=2.1781\n",
            "39000/200000: training loss=2.1573, dev loss=2.1756\n",
            "40000/200000: training loss=2.1539, dev loss=2.1733\n",
            "41000/200000: training loss=2.1506, dev loss=2.1709\n",
            "42000/200000: training loss=2.1475, dev loss=2.1685\n",
            "43000/200000: training loss=2.1449, dev loss=2.1657\n",
            "44000/200000: training loss=2.1409, dev loss=2.1633\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-248-967e8cf362b6>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0;31m# Update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(lossi)"
      ],
      "metadata": {
        "id": "jjTgq78BCCRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(lossi_dev)"
      ],
      "metadata": {
        "id": "c6QgPaXDCHha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample the model"
      ],
      "metadata": {
        "id": "7kTWwn5Q8x4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g_gpu = torch.Generator(device=device).manual_seed(2147483647) # for reproducibility\n",
        "\n",
        "words = []\n",
        "for _ in range(30):\n",
        "\n",
        "  iys = []\n",
        "  context = [0] * BLOCK_SIZE\n",
        "  while True:\n",
        "    x = torch.tensor([context]).to(device)\n",
        "    logits = net(x)\n",
        "    prob = F.softmax(logits, dim=1)\n",
        "    iy = torch.multinomial(prob, num_samples=1, replacement=True, generator=g_gpu)\n",
        "\n",
        "    if iy == 0:\n",
        "      words.append(''.join(itos[iy.item()] for iy in iys))\n",
        "      break\n",
        "    else:\n",
        "      iys.append(iy)\n",
        "      context = context[1:] + [iy]\n",
        "\n",
        "for w in words:\n",
        "  print(w)"
      ],
      "metadata": {
        "id": "CGiFf5b280CQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}