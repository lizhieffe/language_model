{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4BFeNMGwi1kr9qMS1Jo8S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b539d7b6f69046bab5b05755b7bdc2de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f3909079cd604c4592108a76c2be852d",
              "IPY_MODEL_744b460f18ed4851a5f7b15b1f3acd02",
              "IPY_MODEL_b1d98568b18a4469b47c53aef01c311c"
            ],
            "layout": "IPY_MODEL_e5edb534c4094ba5b037a2788c7a31a6"
          }
        },
        "f3909079cd604c4592108a76c2be852d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3a0246472d046979e2ffb591eea1475",
            "placeholder": "​",
            "style": "IPY_MODEL_1ac59d6fa8714ec194a204d37e93312a",
            "value": "100%"
          }
        },
        "744b460f18ed4851a5f7b15b1f3acd02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08705a50bfd8493d9a918f4134e2dfdd",
            "max": 247731,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_31295d469ba648e7a20a565f95f742ee",
            "value": 247731
          }
        },
        "b1d98568b18a4469b47c53aef01c311c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56e9cbe5190b482aafd3cafc8f32b26f",
            "placeholder": "​",
            "style": "IPY_MODEL_394fba2b69904e06bd8c6526e90ad576",
            "value": " 247731/247731 [00:07&lt;00:00, 36124.51it/s]"
          }
        },
        "e5edb534c4094ba5b037a2788c7a31a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3a0246472d046979e2ffb591eea1475": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ac59d6fa8714ec194a204d37e93312a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08705a50bfd8493d9a918f4134e2dfdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31295d469ba648e7a20a565f95f742ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "56e9cbe5190b482aafd3cafc8f32b26f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "394fba2b69904e06bd8c6526e90ad576": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f43920f035944328f8db936f5d8eb15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_35536d72c9b440d0831a68b4ed879056",
              "IPY_MODEL_3001ad4253df42d49d68145b6a19bd12",
              "IPY_MODEL_34d41b64f2f140cdafb926cbe82cdb9b"
            ],
            "layout": "IPY_MODEL_16260d0621d743f888c5acbe849c5e20"
          }
        },
        "35536d72c9b440d0831a68b4ed879056": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b08a09bb95474bcb9121940f9dee6176",
            "placeholder": "​",
            "style": "IPY_MODEL_457ab62e3cfe42bf981cb3ebddbe45b6",
            "value": "100%"
          }
        },
        "3001ad4253df42d49d68145b6a19bd12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b2d63e250b7407ea681e1c3e40db744",
            "max": 100682,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ccaf0db493704cab84947414298007e7",
            "value": 100682
          }
        },
        "34d41b64f2f140cdafb926cbe82cdb9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecef6df9b8f4448c924b65cf3f486f9c",
            "placeholder": "​",
            "style": "IPY_MODEL_51721b0693f04a5daa304d43cacc7e06",
            "value": " 100682/100682 [00:00&lt;00:00, 534636.88it/s]"
          }
        },
        "16260d0621d743f888c5acbe849c5e20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b08a09bb95474bcb9121940f9dee6176": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "457ab62e3cfe42bf981cb3ebddbe45b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b2d63e250b7407ea681e1c3e40db744": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccaf0db493704cab84947414298007e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ecef6df9b8f4448c924b65cf3f486f9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51721b0693f04a5daa304d43cacc7e06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2aa78db0bf9448c96d9b64a70ae9ed5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ed92f6a575e43e2aca105e7d2b05729",
              "IPY_MODEL_11ad3f3ca0d247459a864a2426d39a27",
              "IPY_MODEL_bf4bfe3b6d41444e924a2b897628ceee"
            ],
            "layout": "IPY_MODEL_a663aaf0abe340969ca1bd5b42e23544"
          }
        },
        "4ed92f6a575e43e2aca105e7d2b05729": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_295fb589493c4540811bdbcc9d47c659",
            "placeholder": "​",
            "style": "IPY_MODEL_0d3293631b454cc0906b539e68b2ba20",
            "value": "100%"
          }
        },
        "11ad3f3ca0d247459a864a2426d39a27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab1a1bc5dc314a179090a4b869c016ba",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_53374450f9214885a28a4465a8995d22",
            "value": 25000
          }
        },
        "bf4bfe3b6d41444e924a2b897628ceee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65752360fdc84a9c9289187a7866d075",
            "placeholder": "​",
            "style": "IPY_MODEL_2ef798642f784d838c7c6ece00b92bda",
            "value": " 25000/25000 [00:58&lt;00:00, 511.15it/s]"
          }
        },
        "a663aaf0abe340969ca1bd5b42e23544": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "295fb589493c4540811bdbcc9d47c659": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d3293631b454cc0906b539e68b2ba20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab1a1bc5dc314a179090a4b869c016ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53374450f9214885a28a4465a8995d22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "65752360fdc84a9c9289187a7866d075": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ef798642f784d838c7c6ece00b92bda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizhieffe/language_model/blob/main/Bert_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Good tutorial: https://coaxsoft.com/blog/building-bert-with-pytorch-from-scratch\n",
        "\n",
        "Another tutorial: https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial\n"
      ],
      "metadata": {
        "id": "vgugZfdOJzaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import re\n",
        "\n",
        "from typing import Dict, List\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "!pip install tqdm\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wX-MYEZUKnwa",
        "outputId": "77f0cf20-f07c-4bec-bf73-50ec1d03ac67"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "USE_GPU = True\n",
        "\n",
        "BLOCK_SIZE = 96 # Context length: how many chars do we take to predict the next one?\n",
        "\n",
        "# number of workers in .map() call\n",
        "# good number to use is ~order number of cpu cores // 2\n",
        "NUM_PROC = 24"
      ],
      "metadata": {
        "id": "vDHUDjtsph4G"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup GPU"
      ],
      "metadata": {
        "id": "NZIvnPdcps3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_GPU:\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  assert device.type != 'cpu', \"GPU is not available\"\n",
        "else:\n",
        "  device = 'cpu'\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sd95wthQptwk",
        "outputId": "f00a038c-fc92-46d3-8209-92eb78cc7de5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g_cpu = torch.Generator(device='cpu').manual_seed(2147483647) # for reproducibility\n",
        "g_device = torch.Generator(device=device).manual_seed(2147483647) # for reproducibility"
      ],
      "metadata": {
        "id": "FDyu_mmdxQNC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer\n",
        "\n",
        "- **TODO**: the tokenizer in IMDBBertDataset doesn't convert the word to id. It similar to splitting the sentence to words. Integrate with a more advanced one."
      ],
      "metadata": {
        "id": "eIpZ2CQ52aC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Dataset"
      ],
      "metadata": {
        "id": "P3kYTRN8yAk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download data - openwebtext\n",
        "\n",
        "!pip install datasets # Since we are running in colab docker image, install it here.\n",
        "\n",
        "from datasets import load_dataset # huggingface datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLi0aaU9yB4G",
        "outputId": "2dcddaee-a543-4dc1-ab58-6000ae5c433f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 50K imdb reviews.\n",
        "# https://huggingface.co/datasets/imdb\n",
        "dataset = load_dataset(\"imdb\", num_proc=NUM_PROC)"
      ],
      "metadata": {
        "id": "23ytzd1UyGgC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = dataset['train']"
      ],
      "metadata": {
        "id": "P7JmJd0WybG5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for it  in train_ds:\n",
        "  print(it)\n",
        "  i += 1\n",
        "  if i > 4:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWdkk8TbyhyC",
        "outputId": "98a5a9f9-35b6-4922-b70d-50c958a5b0e2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n",
            "{'text': '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.', 'label': 0}\n",
            "{'text': \"If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />One might better spend one's time staring out a window at a tree growing.<br /><br />\", 'label': 0}\n",
            "{'text': \"This film was probably inspired by Godard's Masculin, féminin and I urge you to see that film instead.<br /><br />The film has two strong elements and those are, (1) the realistic acting (2) the impressive, undeservedly good, photo. Apart from that, what strikes me most is the endless stream of silliness. Lena Nyman has to be most annoying actress in the world. She acts so stupid and with all the nudity in this film,...it's unattractive. Comparing to Godard's film, intellectuality has been replaced with stupidity. Without going too far on this subject, I would say that follows from the difference in ideals between the French and the Swedish society.<br /><br />A movie of its time, and place. 2/10.\", 'label': 0}\n",
            "{'text': 'Oh, brother...after hearing about this ridiculous film for umpteen years all I can think of is that old Peggy Lee song..<br /><br />\"Is that all there is??\" ...I was just an early teen when this smoked fish hit the U.S. I was too young to get in the theater (although I did manage to sneak into \"Goodbye Columbus\"). Then a screening at a local film museum beckoned - Finally I could see this film, except now I was as old as my parents were when they schlepped to see it!!<br /><br />The ONLY reason this film was not condemned to the anonymous sands of time was because of the obscenity case sparked by its U.S. release. MILLIONS of people flocked to this stinker, thinking they were going to see a sex film...Instead, they got lots of closeups of gnarly, repulsive Swedes, on-street interviews in bland shopping malls, asinie political pretension...and feeble who-cares simulated sex scenes with saggy, pale actors.<br /><br />Cultural icon, holy grail, historic artifact..whatever this thing was, shred it, burn it, then stuff the ashes in a lead box!<br /><br />Elite esthetes still scrape to find value in its boring pseudo revolutionary political spewings..But if it weren\\'t for the censorship scandal, it would have been ignored, then forgotten.<br /><br />Instead, the \"I Am Blank, Blank\" rhythymed title was repeated endlessly for years as a titilation for porno films (I am Curious, Lavender - for gay films, I Am Curious, Black - for blaxploitation films, etc..) and every ten years or so the thing rises from the dead, to be viewed by a new generation of suckers who want to see that \"naughty sex film\" that \"revolutionized the film industry\"...<br /><br />Yeesh, avoid like the plague..Or if you MUST see it - rent the video and fast forward to the \"dirty\" parts, just to get it over with.<br /><br />', 'label': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drZrdM6PIJoz",
        "outputId": "4cfbb80f-8c91-4eb4-9574-df5284273707"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'label'],\n",
              "    num_rows: 25000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare DS\n",
        "\n",
        "- The original BERT uses BooksCorpus (800M words) and English Wikipedia (2,500M words) for pre-training.\n",
        "- We use IMDB reviews data with ~72k words."
      ],
      "metadata": {
        "id": "UrlatiLTy2RW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Counter():\n",
        "  \"\"\"Store the counts for individual tokens.\"\"\"\n",
        "  def __init__(self):\n",
        "    self.token_to_counts = {}\n",
        "\n",
        "  def update(self, tokens:  List[int]):\n",
        "    \"\"\"Update the counts with new tokens\"\"\"\n",
        "    for t in tokens:\n",
        "      if t in self.token_to_counts:\n",
        "        self.token_to_counts[t] += 1\n",
        "      else:\n",
        "        self.token_to_counts[t] = 1\n",
        "\n",
        "  def get(self) -> Dict[str, int]:\n",
        "    return self.token_to_counts.copy()\n",
        "\n",
        "  def __str__(self):\n",
        "    s = sorted(self.token_to_counts.items(), key=lambda x:x[1], reverse=True)\n",
        "    return s.__str__()"
      ],
      "metadata": {
        "id": "jk9z_0d_Kwvb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab:\n",
        "  def __init__(self):\n",
        "    self.ttoi = {}\n",
        "    self.itot = {}\n",
        "\n",
        "  def insert_token(self, t: str):\n",
        "    assert t not in self.ttoi\n",
        "    i = len(self.ttoi)\n",
        "    self.ttoi[t] = i\n",
        "    self.itot[i] = t\n",
        "\n",
        "  def lookup_indices(self, tokens: List[str]):\n",
        "    \"\"\"Given a list of tokens in string, return the list of indices.\n",
        "\n",
        "    Args:\n",
        "      tokens: a list of tokens\n",
        "\n",
        "    Return:\n",
        "      a list of indices.\n",
        "    \"\"\"\n",
        "    ret = []\n",
        "    for t in tokens:\n",
        "      if t in self.ttoi:\n",
        "        ret.append(self.ttoi[t])\n",
        "      else:\n",
        "        ret.append(self.ttoi['[UNK]'])\n",
        "\n",
        "    return ret\n",
        "\n",
        "  def lookup_token(self, index: int):\n",
        "    return self.itot[index]"
      ],
      "metadata": {
        "id": "dhdTZ-KmQIyA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "from torchtext.data import get_tokenizer\n",
        "import random\n",
        "\n",
        "class IMDBBertDataset(Dataset):\n",
        "  \"\"\"Dataset class.\n",
        "\n",
        "  Bert has two tasks\n",
        "  1) MLM (Mask LM)\n",
        "  2) NSP (Next sentence prediction)\n",
        "\n",
        "  Each example from this dataset contain info for both tasks.\n",
        "  \"\"\"\n",
        "\n",
        "  # Define special tokens as attributes of class\n",
        "  CLS = '[CLS]'\n",
        "  PAD = '[PAD]'\n",
        "  SEP = '[SEP]'\n",
        "  MASK = '[MASK]'\n",
        "  UNK = '[UNK]'\n",
        "\n",
        "  MASK_PERCENTAGE = 0.15\n",
        "\n",
        "  # ============================================================================\n",
        "  # These are used as column keys in the generated examples (pandas.DataFrameformat)\n",
        "\n",
        "  # The masked sentence's indices in its encoded form.\n",
        "  MASKED_INDICES_COLUMN = 'masked_indices'\n",
        "  # The unmasked sentence's indices in its encoded form.\n",
        "  TARGET_COLUMN = 'indices'\n",
        "  # The NSP task's label.\n",
        "  NSP_TARGET_COLUMN = 'is_next'\n",
        "  # The positions of the masked tokens (either [MASK], or replaced with a random\n",
        "  # token), inversed. For masked token, the corresponding position's value is\n",
        "  # False (that is why it is called inversed); otherwise True. Note that [CLS],\n",
        "  # [PAD], [SEP], [UNK] are NOT considered as mask.\n",
        "  INVERSE_TOKEN_MASK_COLUMN = 'inverse_token_mask'\n",
        "  # ============================================================================\n",
        "\n",
        "  OPTIMAL_LENGTH_PERCENTILE = 70\n",
        "\n",
        "  def __init__(self,\n",
        "               ds_from=None,\n",
        "               ds_to=None,\n",
        "               should_include_text: bool=False):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      should_include_text: if true, include the raw text in the dataset. This\n",
        "        should only be used for debugging purpose.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.ds = []\n",
        "    for it in dataset['train']:\n",
        "      self.ds.append(it['text'])\n",
        "\n",
        "    self.tokenizer = get_tokenizer('basic_english')\n",
        "    self.counter = Counter()\n",
        "    self.vocab = Vocab()\n",
        "\n",
        "    self.optimal_sentence_length = None\n",
        "    self.should_include_text = should_include_text\n",
        "\n",
        "    if self.should_include_text:\n",
        "      self.columns = [\n",
        "          'masked_sentence',\n",
        "          self.MASKED_INDICES_COLUMN,\n",
        "          'sentence',\n",
        "          self.TARGET_COLUMN,\n",
        "          self.INVERSE_TOKEN_MASK_COLUMN,\n",
        "          self.NSP_TARGET_COLUMN,\n",
        "      ]\n",
        "    else:\n",
        "      self.columns = [\n",
        "          self.MASKED_INDICES_COLUMN,\n",
        "          self.TARGET_COLUMN,\n",
        "          self.INVERSE_TOKEN_MASK_COLUMN,\n",
        "          self.NSP_TARGET_COLUMN,\n",
        "      ]\n",
        "\n",
        "    self.df = self._prepare_dataset()\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.df)\n",
        "\n",
        "  def __getitem__(self, idx: int)-> (torch.Tensor,\n",
        "                                     torch.Tensor,\n",
        "                                     torch.Tensor,\n",
        "                                     torch.Tensor,\n",
        "                                     torch.Tensor):\n",
        "    \"\"\"Implement self[idx] interface.\n",
        "\n",
        "    Args:\n",
        "      idx: the index of the example to get.\n",
        "\n",
        "    Returns:\n",
        "      inp: the encoded example. Note that for Bert the raw example is in form\n",
        "        \"[CLS] sen1 [SEP] sen2 [SEP]\", where sen1 and sen2 are already masked\n",
        "        and padded.\n",
        "      attention_mask: True when input token is [PAD], otherwise False. It is\n",
        "        used in the training process to extinguish the embeddings for [PAD]\n",
        "        tokens to exclude [PAD] when calculating transfomer prediction.\n",
        "      inverse_token_mask: see the comment for INVERSE_TOKEN_MASK_COLUMN.\n",
        "      mlm_target: the MLM target. At the masked positions, it is the index of\n",
        "        the expected token; at other positions, it is set to 0. Later in the\n",
        "        model's loss, we use NLLLoss(ignore_index=0) to ignore the target which\n",
        "        equal to 0 when calculating the loss. That's why we set the non-masked\n",
        "        position's target to 0.\n",
        "      nsp_target: the NSP target. It is a tensor of two items. It can be only in\n",
        "        two states designating whether it is next sentence or not. If yes, it is\n",
        "        [0, 1]; if not, it is [1, 0]. We use BCEWithLogitsLoss for training,\n",
        "        which expect this format.\n",
        "    \"\"\"\n",
        "    item = ds.df.iloc[idx]\n",
        "\n",
        "    inp = torch.Tensor(item[self.MASKED_INDICES_COLUMN]).long()\n",
        "\n",
        "    attention_mask = (inp == self.vocab.lookup_indices([self.PAD])[0])\n",
        "\n",
        "    inverse_token_mask = torch.Tensor(item[self.INVERSE_TOKEN_MASK_COLUMN]).bool()\n",
        "\n",
        "    # MLM target.\n",
        "    mlm_target = torch.Tensor(item[self.TARGET_COLUMN]).long()\n",
        "    # Set the non-masked positions to be 0.\n",
        "    mlm_target = mlm_target.masked_fill_(inverse_token_mask, 0)\n",
        "\n",
        "    # NSP target\n",
        "    if item[self.TARGET_COLUMN] == 0: # 2nd sen is not the next for 1st sen\n",
        "      t = [1, 0]\n",
        "    else:\n",
        "      t = [0, 1]\n",
        "    nsp_target = torch.Tensor(t)\n",
        "\n",
        "    return (\n",
        "        inp.to(device),\n",
        "        attention_mask.to(device),\n",
        "        inverse_token_mask.to(device),\n",
        "        mlm_target.to(device),\n",
        "        nsp_target.to(device)\n",
        "    )\n",
        "\n",
        "  def _update_length(self,\n",
        "                     review_sentences: List[str],\n",
        "                     sentence_lens: List[int]):\n",
        "    for s in review_sentences:\n",
        "      sentence_lens.append(len(s.split()))\n",
        "\n",
        "  def _find_optimal_sentence_length(self, sentence_lens: List[int]):\n",
        "    arr = np.array(sentence_lens)\n",
        "    ret = int(np.percentile(arr, self.OPTIMAL_LENGTH_PERCENTILE))\n",
        "    return ret\n",
        "\n",
        "  def _fill_vocab(self, min_freq=2):\n",
        "    self.vocab.insert_token(self.CLS)\n",
        "    self.vocab.insert_token(self.PAD)\n",
        "    self.vocab.insert_token(self.MASK)\n",
        "    self.vocab.insert_token(self.SEP)\n",
        "    self.vocab.insert_token(self.UNK)\n",
        "\n",
        "    token_to_counts = self.counter.get()\n",
        "    for t, counts in tqdm(token_to_counts.items()):\n",
        "      if counts >= min_freq:\n",
        "        self.vocab.insert_token(t)\n",
        "\n",
        "  def _create_item(self,\n",
        "                   first: List[str],\n",
        "                   second: List[str],\n",
        "                   target: int) -> (List[str], List[int], List[str], List[int], List[bool], int):\n",
        "    \"\"\"Create an example.\n",
        "\n",
        "    Args:\n",
        "      first: the first sentence\n",
        "      second: the second sentence\n",
        "      target: the NSP label. E.g. if the second is the next sentence of first, true; otherwise, false.\n",
        "\n",
        "    Returns:\n",
        "      nsp_sentence: the example in str format. It is the contatenation of the two input sentences, with masking and padding.\n",
        "      nsp_indices: indices of nsp_sentence.\n",
        "      original_nsp_sentence: nsp_sentence without masking and padding.\n",
        "      original_nsp_indices: indices of original_nsp_sentence.\n",
        "      inverse_token_mask: indicates the corresponding indices are NOT masked. If\n",
        "        the corresponding token is masked, False; otherwise, True. Also see\n",
        "        comment for INVERSE_TOKEN_MASK_COLUMN.\n",
        "      target: the same as target in Args.\n",
        "    \"\"\"\n",
        "    # Create masked and padded sentence item\n",
        "    first_updated, first_inverse_token_mask = self._preprocess_sentence(first.copy(), should_mask=True)\n",
        "    second_updated, second_inverse_token_mask = self._preprocess_sentence(second.copy(), should_mask=True)\n",
        "\n",
        "    # BERT was pretrained using the format [CLS] sen A [SEP] sen B [SEP].\n",
        "    nsp_sentence = [self.CLS] + first_updated + [self.SEP] + second_updated + [self.SEP]\n",
        "    nsp_indices = self.vocab.lookup_indices(nsp_sentence)\n",
        "    inverse_token_mask = [True] + first_inverse_token_mask + [True] + second_inverse_token_mask + [True]\n",
        "\n",
        "    # Create padded only item without masking\n",
        "    first, _ = self._preprocess_sentence(first.copy(), should_mask=False)\n",
        "    second, _ = self._preprocess_sentence(second.copy(), should_mask=False)\n",
        "\n",
        "    original_nsp_sentence = [self.CLS] + first + [self.SEP] + second + [self.SEP]\n",
        "    original_nsp_indices = self.vocab.lookup_indices(original_nsp_sentence)\n",
        "\n",
        "    if self.should_include_text:\n",
        "      return (\n",
        "          nsp_sentence,\n",
        "          nsp_indices,\n",
        "          original_nsp_sentence,\n",
        "          original_nsp_indices,\n",
        "          inverse_token_mask,\n",
        "          target\n",
        "      )\n",
        "    else:\n",
        "      return (\n",
        "          nsp_indices,\n",
        "          original_nsp_indices,\n",
        "          inverse_token_mask,\n",
        "          target\n",
        "      )\n",
        "\n",
        "  def _select_false_nsp_sentences(self, sentences: List[str]):\n",
        "    sentences_len = len(sentences)\n",
        "    i1 = random.randint(0, sentences_len-1)\n",
        "    i2 = random.randint(0, sentences_len-1)\n",
        "\n",
        "    # Make sure they are really not NSP\n",
        "    while i1 == i2 - 1:\n",
        "      i2 = random.randint(0, sentences_len-1)\n",
        "\n",
        "    return sentences[i1], sentences[i2]\n",
        "\n",
        "  def _preprocess_sentence(self,\n",
        "                           sentence: List[str],\n",
        "                           should_mask: bool=True) -> (List[str], List[bool]):\n",
        "    \"\"\"Preprocess the sentence.\n",
        "\n",
        "    1. Mask\n",
        "    2. Pad\n",
        "\n",
        "    Args:\n",
        "      sentence: the sentence to preprocess\n",
        "      should_mask: whether to mask the sentence\n",
        "\n",
        "    Returns:\n",
        "      sentence: the masked sentence.\n",
        "      inverse_token_mask: a list of boolean, which indicates the corresponding\n",
        "        indices are NOT masked. If the corresponding token is masked, false;\n",
        "        otherwise, true. Also see comment for INVERSE_TOKEN_MASK_COLUMN.\n",
        "    \"\"\"\n",
        "    inverse_token_mask = None\n",
        "    if should_mask:\n",
        "      sentence, inverse_token_mask = self._mask_sentence(sentence)\n",
        "\n",
        "    sentence, inverse_token_mask = self._pad_sentence(sentence, inverse_token_mask)\n",
        "\n",
        "    return sentence, inverse_token_mask\n",
        "\n",
        "  def _mask_sentence(self, sentence: List[str]) -> (List[str], List[bool]):\n",
        "    \"\"\"Mask the sentence.\n",
        "\n",
        "    Choose self.MASK_PERCENTAGE tokens randomly to do the mask. Of the masks,\n",
        "    80% are updated to [MASK], 20% are updated to a random token in the vocab.\n",
        "\n",
        "    Args:\n",
        "      sentence: the sentence to mask\n",
        "\n",
        "    Returns:\n",
        "      sentence: the masked sentence.\n",
        "      inverse_token_mask: a list of boolean, which indicates the corresponding\n",
        "        indices are NOT masked. If the corresponding token is masked, false;\n",
        "        otherwise, true. Also see comment for INVERSE_TOKEN_MASK_COLUMN.\n",
        "    \"\"\"\n",
        "    len_s = len(sentence)\n",
        "\n",
        "    # Mask random 15% tokens in the sentence.\n",
        "    mask_amount = round(len_s * self.MASK_PERCENTAGE)\n",
        "    mask_index = random.sample(range(0, len_s), mask_amount)\n",
        "\n",
        "    # False at the index where the token is masked\n",
        "    inverse_token_mask = [True for i in range(len_s)]\n",
        "\n",
        "    assert mask_amount == len(mask_index)\n",
        "    assert len_s == len(inverse_token_mask)\n",
        "\n",
        "    # For the tokens to be masked, 80% update them to [MASK], 20% update to\n",
        "    # random tokens.\n",
        "    for ix in mask_index:\n",
        "      if random.uniform(0, 1) < 0.8:\n",
        "        sentence[ix] = self.MASK\n",
        "      else:\n",
        "        # for 20% avoid update to the special tokens.\n",
        "        token_i = random.randrange(5, len(self.vocab.itot))\n",
        "        sentence[ix] = self.vocab.lookup_token(token_i)\n",
        "\n",
        "      inverse_token_mask[ix] = False\n",
        "\n",
        "    return sentence, inverse_token_mask\n",
        "\n",
        "  def _pad_sentence(self,\n",
        "                    sentence: List[str],\n",
        "                    inverse_token_mask: List[bool]=None) -> (List[str], List[bool]):\n",
        "    len_s = len(sentence)\n",
        "\n",
        "    assert self.optimal_sentence_length != None\n",
        "\n",
        "    if len_s > self.optimal_sentence_length:\n",
        "      sentence = sentence[:self.optimal_sentence_length]\n",
        "    else:\n",
        "      sentence = sentence + [self.PAD] * (self.optimal_sentence_length - len_s)\n",
        "\n",
        "    if inverse_token_mask != None:\n",
        "      assert len_s == len(inverse_token_mask), f\"sentence {len_s} and inverse_token_mask {len(inverse_token_mask)} should have the same length\"\n",
        "\n",
        "      if len_s > self.optimal_sentence_length:\n",
        "        inverse_token_mask = inverse_token_mask[:self.optimal_sentence_length]\n",
        "      else:\n",
        "        # [PAD] is not considered as masked, so set to True here.\n",
        "        inverse_token_mask = inverse_token_mask + [True] * (self.optimal_sentence_length - len_s)\n",
        "\n",
        "    return sentence, inverse_token_mask\n",
        "\n",
        "  def _prepare_dataset(self) -> pd.DataFrame:\n",
        "    \"\"\"Generate the dataset.\n",
        "\n",
        "    Return:\n",
        "      The generated dataset.\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    nsp = []\n",
        "    sentence_lens = []\n",
        "\n",
        "    # split ds on sentences\n",
        "    for review in self.ds:\n",
        "      review_sentences = review.split('. ')\n",
        "      sentences += review_sentences\n",
        "      self._update_length(review_sentences, sentence_lens)\n",
        "\n",
        "    self.optimal_sentence_length = self._find_optimal_sentence_length(sentence_lens)\n",
        "    print(f'{self.optimal_sentence_length=}')\n",
        "\n",
        "    # Create vocab\n",
        "    print(\"Create vocabulary\")\n",
        "    for s in tqdm(sentences):\n",
        "      self.counter.update(self.tokenizer(s))\n",
        "    self._fill_vocab()\n",
        "    print(f'\\nvocab size = {len(self.vocab.ttoi)}')\n",
        "\n",
        "    assert len(sentence_lens) == len(sentences)\n",
        "    # print(self.counter)\n",
        "\n",
        "    iiii = 0\n",
        "    for i in tqdm(range(len(self.ds))):\n",
        "      review = self.ds[i]\n",
        "    # for review in self.ds:\n",
        "      review_sentences = review.split('. ')\n",
        "      # For each review with >1 sentence we create true NSP examples (when the\n",
        "      # 2nd sentence is the next sentence in review) and false NSP examples (\n",
        "      # when the 2nd sentence is any random sentence that is not the next\n",
        "      # sentence in review)\n",
        "      if len(review_sentences) > 1:\n",
        "        for i in range(len(review_sentences) - 1):\n",
        "          # True NSP item\n",
        "          first, second = self.tokenizer(review_sentences[i]), self.tokenizer(review_sentences[i+1])\n",
        "          # print(f'{first=}, {second=}')\n",
        "          nsp.append(self._create_item(first, second, target=1))\n",
        "\n",
        "          # False NSP item\n",
        "          first, second = self._select_false_nsp_sentences(sentences)\n",
        "          first, second = self.tokenizer(first), self.tokenizer(second)\n",
        "          # print(f'{first=}, {second=}')\n",
        "          nsp.append(self._create_item(first, second, target=0))\n",
        "\n",
        "    df = pd.DataFrame(nsp, columns=self.columns)\n",
        "    return df\n",
        "\n",
        "ds = IMDBBertDataset(should_include_text=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183,
          "referenced_widgets": [
            "b539d7b6f69046bab5b05755b7bdc2de",
            "f3909079cd604c4592108a76c2be852d",
            "744b460f18ed4851a5f7b15b1f3acd02",
            "b1d98568b18a4469b47c53aef01c311c",
            "e5edb534c4094ba5b037a2788c7a31a6",
            "a3a0246472d046979e2ffb591eea1475",
            "1ac59d6fa8714ec194a204d37e93312a",
            "08705a50bfd8493d9a918f4134e2dfdd",
            "31295d469ba648e7a20a565f95f742ee",
            "56e9cbe5190b482aafd3cafc8f32b26f",
            "394fba2b69904e06bd8c6526e90ad576",
            "4f43920f035944328f8db936f5d8eb15",
            "35536d72c9b440d0831a68b4ed879056",
            "3001ad4253df42d49d68145b6a19bd12",
            "34d41b64f2f140cdafb926cbe82cdb9b",
            "16260d0621d743f888c5acbe849c5e20",
            "b08a09bb95474bcb9121940f9dee6176",
            "457ab62e3cfe42bf981cb3ebddbe45b6",
            "5b2d63e250b7407ea681e1c3e40db744",
            "ccaf0db493704cab84947414298007e7",
            "ecef6df9b8f4448c924b65cf3f486f9c",
            "51721b0693f04a5daa304d43cacc7e06",
            "f2aa78db0bf9448c96d9b64a70ae9ed5",
            "4ed92f6a575e43e2aca105e7d2b05729",
            "11ad3f3ca0d247459a864a2426d39a27",
            "bf4bfe3b6d41444e924a2b897628ceee",
            "a663aaf0abe340969ca1bd5b42e23544",
            "295fb589493c4540811bdbcc9d47c659",
            "0d3293631b454cc0906b539e68b2ba20",
            "ab1a1bc5dc314a179090a4b869c016ba",
            "53374450f9214885a28a4465a8995d22",
            "65752360fdc84a9c9289187a7866d075",
            "2ef798642f784d838c7c6ece00b92bda"
          ]
        },
        "id": "xdy7uteQzh-K",
        "outputId": "231ed6e7-e8c1-4c05-9244-a862f45e243e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "self.optimal_sentence_length=27\n",
            "Create vocabulary\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/247731 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b539d7b6f69046bab5b05755b7bdc2de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100682 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f43920f035944328f8db936f5d8eb15"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "vocab size = 51721\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/25000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2aa78db0bf9448c96d9b64a70ae9ed5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT was pretrained using the format [CLS] sen A [SEP] sen B [SEP].\n",
        "assert len(ds.df.iloc[0,:]['masked_indices']) == 3 + 2 * ds.optimal_sentence_length"
      ],
      "metadata": {
        "id": "Z7EO8WEKaTgA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds.df.iloc[0]['masked_sentence']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWjq1D2pTTZJ",
        "outputId": "c0e06bde-d1e1-4081-e78d-0f20b8e825f2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'i',\n",
              " 'reason',\n",
              " 'i',\n",
              " 'am',\n",
              " '[MASK]',\n",
              " '[MASK]',\n",
              " 'my',\n",
              " 'video',\n",
              " 'store',\n",
              " 'because',\n",
              " 'of',\n",
              " 'all',\n",
              " 'the',\n",
              " 'controversy',\n",
              " '[MASK]',\n",
              " 'surrounded',\n",
              " 'it',\n",
              " 'when',\n",
              " 'it',\n",
              " 'was',\n",
              " 'first',\n",
              " 'released',\n",
              " 'in',\n",
              " '1967',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[SEP]',\n",
              " 'i',\n",
              " 'also',\n",
              " 'heard',\n",
              " 'that',\n",
              " 'at',\n",
              " 'first',\n",
              " 'it',\n",
              " 'panoramic',\n",
              " 'seized',\n",
              " 'by',\n",
              " '[MASK]',\n",
              " '.',\n",
              " 's',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds.df.iloc[0,:]['sentence']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kv6qJLHQZ4vG",
        "outputId": "06ba51ec-7522-4fa8-a74b-9de8ac08830e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'i',\n",
              " 'rented',\n",
              " 'i',\n",
              " 'am',\n",
              " 'curious-yellow',\n",
              " 'from',\n",
              " 'my',\n",
              " 'video',\n",
              " 'store',\n",
              " 'because',\n",
              " 'of',\n",
              " 'all',\n",
              " 'the',\n",
              " 'controversy',\n",
              " 'that',\n",
              " 'surrounded',\n",
              " 'it',\n",
              " 'when',\n",
              " 'it',\n",
              " 'was',\n",
              " 'first',\n",
              " 'released',\n",
              " 'in',\n",
              " '1967',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[SEP]',\n",
              " 'i',\n",
              " 'also',\n",
              " 'heard',\n",
              " 'that',\n",
              " 'at',\n",
              " 'first',\n",
              " 'it',\n",
              " 'was',\n",
              " 'seized',\n",
              " 'by',\n",
              " 'u',\n",
              " '.',\n",
              " 's',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds.df.iloc[0,:]['inverse_token_mask']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVTSL7VEdAUF",
        "outputId": "46c1a04c-e007-4ad6-a78e-317489f180fb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[True,\n",
              " True,\n",
              " False,\n",
              " True,\n",
              " True,\n",
              " False,\n",
              " False,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " False,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " False,\n",
              " True,\n",
              " True,\n",
              " False,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True,\n",
              " True]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds.df.iloc[0,:]['is_next']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydf77KzkdOGO",
        "outputId": "def5eba8-e087-4416-e702-37eba1a59eb2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "fFiuPDAvJ9Mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMB_DIM = 256 # the dimensionality of the character embedding vectors\n",
        "N_HEADS = 8  # the number of heads in transformer\n",
        "D_HEAD = EMB_DIM * 2  # the dimensionality of the transformer's head\n",
        "D_HIDDEN = EMB_DIM * 4  # the dimensionality of the transformer's hidden FFN layer"
      ],
      "metadata": {
        "id": "Gv5fCSSXJbmf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class JointEmbedding(torch.nn.Module):\n",
        "  \"\"\"Embedding layers for tokens.\n",
        "\n",
        "  Bert uses a combination of three different embeddings:\n",
        "\n",
        "  1. Token embedding\n",
        "  2. Segment embedding\n",
        "  3. Pos embedding\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, vocab_size: int, embedding_dim: int):\n",
        "    super().__init__()\n",
        "\n",
        "    self.token_emb = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
        "    self.seg_emb = torch.nn.Embedding(num_embeddings=2, embedding_dim=embedding_dim)\n",
        "    # Here we don't fix the length of the sen to be handled, so we give\n",
        "    # `num_embeddings=1000` a relatively large number so that it can handle up\n",
        "    # to 1000 length sentence. If the sentence length is fixed, num_embeddings\n",
        "    # should be the fixed sentence length.\n",
        "    self.pos_emb = torch.nn.Embedding(num_embeddings=1000, embedding_dim=embedding_dim)\n",
        "\n",
        "    # TODO: use the norm\n",
        "    self.norm = torch.nn.LayerNorm(embedding_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    sentence_len = x.shape[-1]\n",
        "\n",
        "    # 1st half of sen is 0, 2nd half is 1\n",
        "    seg_tensor = torch.zeros_like(x).to(device)\n",
        "    seg_tensor[:, sentence_len // 2+1:] = 1\n",
        "    # print(f'{seg_tensor=}')\n",
        "\n",
        "    # TODO: use periodic functions to encode positions. See https://coaxsoft.com/blog/building-bert-with-pytorch-from-scratch\n",
        "    pos_tensor = torch.arange(sentence_len).to(device)\n",
        "    # print(f'{pos_tensor=}')\n",
        "\n",
        "    combined = self.token_emb(x) + self.seg_emb(seg_tensor) + self.pos_emb(pos_tensor)\n",
        "\n",
        "    return combined"
      ],
      "metadata": {
        "id": "juR-lBTy_Kvj"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(torch.nn.Module):\n",
        "  \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
        "\n",
        "  def __init__(self, ndim: int, bias: bool):\n",
        "    super().__init__()\n",
        "    self.weight = torch.nn.Parameter(torch.ones(ndim)).to(device)\n",
        "    self.bias = torch.nn.Parameter(torch.zeros(ndim)).to(device) if bias else None\n",
        "\n",
        "  def forward(self, x):\n",
        "    return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)"
      ],
      "metadata": {
        "id": "tl-O9H6VO0j1"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(torch.nn.Module):\n",
        "  def __init__(self, d_in: int, d_head: int, d_hidden: int):\n",
        "    \"\"\"One single Attention head.\n",
        "\n",
        "    Args:\n",
        "      d_in: input dim\n",
        "      d_head: output dim (how many heads)\n",
        "      d_hidden: dim of the hidden FFN\n",
        "    \"\"\"\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_head = d_head\n",
        "\n",
        "    self.query = torch.nn.Linear(d_in, d_head, bias=False)\n",
        "    self.key = torch.nn.Linear(d_in, d_head, bias=False)\n",
        "    self.value = torch.nn.Linear(d_in, d_head, bias=False)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, attention_mask: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      attention_mask: True at the [PAD] positions. (B, T)\n",
        "    \"\"\"\n",
        "\n",
        "    # x: (B, T, d_in)\n",
        "\n",
        "    T = x.shape[-2]\n",
        "\n",
        "    q = self.query(x) # (B, T, d_head)\n",
        "    k = self.key(x)   # (B, T, d_head)\n",
        "    v = self.value(x) # (B, T, d_head)\n",
        "\n",
        "    # Here it doesn't matter if (k @ q^T) or (q @ k^T)\n",
        "    #\n",
        "    # Let's ignore B dim, then\n",
        "    #\n",
        "    # (k @ q^T) = (q @ k^T)^T\n",
        "    #\n",
        "    # Although (k @ q^T) equals the transpose of (q @ k^T), becuase query matrix\n",
        "    # and key matrix are learned, they will just learn to adapt to its setup.\n",
        "    score = k @ q.transpose(-2, -1)   # (B, T, T)\n",
        "    # print(f'{score.shape=}')\n",
        "    score = score * self.d_head**-0.5 # (B, T, T)\n",
        "\n",
        "    # attention_mask is (B, T), and unsqueezed to (B, 1, T), score is (B, T, T).\n",
        "    # When applying masked_fill_(), attention_mask is broadcasted to (B, T, T)\n",
        "    #\n",
        "    # Let's ignore B dim, and say\n",
        "    #\n",
        "    #         a11 a12\n",
        "    # score = a21 a22\n",
        "    #\n",
        "    # Because the -2 dim is broadcast, score can be written\n",
        "    #\n",
        "    #         a1 a2\n",
        "    # score = a1 a2\n",
        "    #\n",
        "    # Say\n",
        "    #\n",
        "    #      b11 b12 b13\n",
        "    # v =  b21 b22 b23\n",
        "    #\n",
        "    # Then\n",
        "    #\n",
        "    #             a1*b11+a2*b21 a1*b12+a2*b22 a1*b13+a2*b23\n",
        "    # score @ v = a1*b11+a2*b21 a1*b12+a2*b22 a1*b13+a2*b23\n",
        "    #\n",
        "    # Lets say T=0 is [PAD] masked, which means a1 == 0, then\n",
        "    #\n",
        "    #             a2*b21 b2*b22 b2*b23\n",
        "    # score @ v = a2*b21 b2*b22 b2*b23\n",
        "    #\n",
        "    # It can be seen that b11, b12, b13 has no contribution. We know that\n",
        "    # b1* is the v (value) for T=0, this means the [PAD]'s embedding has no\n",
        "    # contribution. This proves the effect of attention_mask, is to make the\n",
        "    # corresponding [PAD] has no effect to the result.\n",
        "\n",
        "    attention_mask = attention_mask.unsqueeze(1) # (B, T) unsqueeze ----> (B, 1, T)\n",
        "    score = score.masked_fill(attention_mask, 1e-9)\n",
        "\n",
        "    # print(f'{score.shape=}')\n",
        "    # print(f'{score=}')\n",
        "\n",
        "    score = F.softmax(score, dim=-1)  # (B, T, T)\n",
        "\n",
        "    y = score @ v # (B, T, d_head)\n",
        "\n",
        "    return y"
      ],
      "metadata": {
        "id": "EtAKmDeNMkJA"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: what is the multi-head impl in Causal style?\n",
        "class MultiHeadAttentionBlock(torch.nn.Module):\n",
        "  def __init__(self, n_heads: int, d_in: int, d_head: int, d_hidden: int, dropout: float):\n",
        "    \"\"\"Impl of Multi Head Attention block.\n",
        "\n",
        "    Args:\n",
        "      n_heads: num of heads\n",
        "      d_in: input dim\n",
        "      d_head: output dim (how many heads)\n",
        "      d_hidden: dim of the hidden FFN\n",
        "      dropout: dropout rate\n",
        "    \"\"\"\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.heads = torch.nn.ModuleList([\n",
        "        AttentionHead(d_in, d_head, d_hidden) for _ in range(n_heads)\n",
        "    ])\n",
        "\n",
        "\n",
        "    self.d_head = d_head\n",
        "\n",
        "    self.ln1 = LayerNorm(d_in, bias=True)\n",
        "    self.ln2 = LayerNorm(d_head * n_heads, bias=True)\n",
        "\n",
        "    self.multi_heads_proj = torch.nn.Linear(d_head * n_heads, d_head, bias=True)\n",
        "\n",
        "    self.linear = torch.nn.Linear(d_head, d_hidden, bias=True)\n",
        "    # Bert paper uses GeLU instead of ReLU or tanh. Some study shows GeLU gives\n",
        "    # the best result (for certain task, of course):\n",
        "    # https://browse.arxiv.org/pdf/1710.05941.pdf\n",
        "    self.activation = torch.nn.GELU()\n",
        "    self.proj = torch.nn.Linear(d_hidden, d_head, bias=True)\n",
        "\n",
        "    self.dropout1 = torch.nn.Dropout(dropout)\n",
        "    self.dropout2 = torch.nn.Dropout(dropout)\n",
        "\n",
        "  # TODO: add skip connection for the multi-head attn.\n",
        "  def forward(self, x, attention_mask: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      attention_mask: see AttentionHead\n",
        "    \"\"\"\n",
        "    # x (B, T, d_in)\n",
        "\n",
        "    x = self.ln1(x) # (B, T, d_in)\n",
        "\n",
        "    s = [ah(x, attention_mask) for ah in self.heads]  # [4, (B, T, d_heads)]\n",
        "    # print(f'{len(s)=}, {scores.shape=}')\n",
        "\n",
        "    # The multi-heads' output are concat in the last dim.\n",
        "    scores = torch.cat(s, dim=-1)     # (B, T, d_head * n_heads)\n",
        "    y = self.ln2(scores)              # (B, T, d_head * n_heads)\n",
        "    y = self.multi_heads_proj(y)      # (B, T, d_head)\n",
        "\n",
        "\n",
        "    ffy = self.linear(y)                # (B, T, d_hidden)\n",
        "    ffy = self.dropout1(ffy)\n",
        "    ffy = self.activation(ffy)          # (B, T, d_hidden)\n",
        "    ffy = self.proj(ffy)                # (B, T, d_head)\n",
        "    # Skip connection\n",
        "    ffy = ffy + y                       # (B, T, d_head)\n",
        "    ffy = self.dropout2(ffy)\n",
        "\n",
        "    return ffy"
      ],
      "metadata": {
        "id": "eKKFIzddS-ko"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Bert(torch.nn.Module):\n",
        "  \"\"\"Combine everything together.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               vocab_size: int,\n",
        "               embedding_dim: int,\n",
        "               n_heads: int,\n",
        "               d_in: int,\n",
        "               d_head: int,\n",
        "               d_hidden: int,\n",
        "               dropout: float):\n",
        "    \"\"\"Initiation fn.\n",
        "\n",
        "    Args:\n",
        "      vocab_size: size of the vocab\n",
        "      embedding_dim: dim of the token embedding\n",
        "      n_heads: num of heads\n",
        "      d_in: input dim\n",
        "      d_head: output dim (how many heads)\n",
        "      d_hidden: dim of the hidden FFN\n",
        "      dropout: dropout rate\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.emb = JointEmbedding(vocab_size=vocab_size, embedding_dim=embedding_dim)\n",
        "    self.att = MultiHeadAttentionBlock(n_heads=n_heads,\n",
        "                                       d_in=d_in,\n",
        "                                       d_head=d_head,\n",
        "                                       d_hidden=d_hidden,\n",
        "                                       dropout=dropout)\n",
        "\n",
        "    self.token_prediction_layer = torch.nn.Linear(d_head, vocab_size).to(device)\n",
        "    self.log_softmax = torch.nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    self.nsp_classification_layer = torch.nn.Linear(d_head, 2)\n",
        "\n",
        "\n",
        "  def forward(self, x, attention_mask: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      attention_mask: see AttentionHead.\n",
        "    \"\"\"\n",
        "    y = self.emb(x)                   # (B, T, embedding_dim)\n",
        "    y = self.att(y, attention_mask)   # (B, T, d_head)\n",
        "\n",
        "    token_predictions_logits = self.token_prediction_layer(y)             # (B, T, vocab_size)\n",
        "    # Log likehood; note NLL is the classification loss\n",
        "    token_predictions_ll = self.log_softmax(token_predictions_logits)       # (B, T, vocab_size)\n",
        "\n",
        "    # Here we don't calculate the \"prob\" of the logits by using sigmoid (or\n",
        "    # softmax in binary classification setup). This is because when calculating\n",
        "    # loss, we use BCEWithLogitsLoss, which is a combination of sigmoid layer\n",
        "    # and BCE (Binary Cross Entropy) loss.\n",
        "    #\n",
        "    #   argmax(NSP output) = [1, 0] is NOT next sentence\n",
        "    #   argmax(NSP output) = [0, 1] is next sentence\n",
        "    nsp_classification_logits = self.nsp_classification_layer(y[:, 0, :]) # (B, 1, 2)\n",
        "\n",
        "    return token_predictions_ll, nsp_classification_logits"
      ],
      "metadata": {
        "id": "gnGGnidhYVPu"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_x = torch.Tensor([ds.df.iloc[0].masked_indices]).int()\n",
        "test_input_attention_mask = torch.ones_like(test_input_x).bool()\n",
        "test_input_attention_mask[0, 0] = False\n",
        "\n",
        "bert = Bert(vocab_size=len(ds.vocab.ttoi),\n",
        "            embedding_dim=EMB_DIM,n_heads=N_HEADS,\n",
        "            d_in=EMB_DIM,\n",
        "            d_head=D_HEAD,\n",
        "            d_hidden=D_HIDDEN,\n",
        "            dropout = 0.2).to(device)\n",
        "# token_predictions_ll, nsp_classification_prob = bert(test_input_x, test_input_attention_mask)\n",
        "# print(f'{token_predictions_ll.shape=}, {nsp_classification_prob=}')"
      ],
      "metadata": {
        "id": "zHC_R8owZNHK"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_total_params = 0\n",
        "\n",
        "for p in bert.parameters():\n",
        "  _total_params += p.nelement()\n",
        "\n",
        "print(f'Total params = {_total_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SteO0PwhXj4",
        "outputId": "d78c2c18-1761-4b27-b9c9-fc13fe1e2f96"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total params = 46325003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss and Optimization"
      ],
      "metadata": {
        "id": "x9pHh0avJ-rH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.SGD(bert.parameters(), lr=0.001, momentum=0.9)\n",
        "# optimizer = optim.Adam(net.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "PmbheEpthQsI"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nsp_criterion = torch.nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "# In the dataset, the mlm_target set the non-masked position's target to be 0,\n",
        "# so that the mlm loss can ignore the non-masked positions and only calculate\n",
        "# loss for the masked positions.\n",
        "mlm_criterion = torch.nn.NLLLoss(ignore_index=0).to(device)"
      ],
      "metadata": {
        "id": "LDOuVSlEh_5a"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "5HHmVcAOKAkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(batch_size):\n",
        "  iii = torch.randint(0, len(ds), (batch_size,)).to(device)\n",
        "  # print(f'{len(ds)=}')\n",
        "  bbb = [ds[i.item()] for i in iii]\n",
        "\n",
        "  inp_b = torch.stack([torch.Tensor(item[0]) for item in bbb]).to(device)\n",
        "  attention_mask_b = torch.stack([torch.Tensor(item[1]) for item in bbb]).to(device)\n",
        "  inverse_token_mask_b = torch.stack([torch.Tensor(item[2]) for item in bbb]).to(device)\n",
        "  mask_target_b = torch.stack([torch.Tensor(item[3]) for item in bbb]).to(device)\n",
        "  nsp_target_b = torch.stack([torch.Tensor(item[4]) for item in bbb]).to(device)\n",
        "\n",
        "  return (inp_b, attention_mask_b, inverse_token_mask_b, mask_target_b, nsp_target_b)\n",
        "\n",
        "inp_b, attention_mask_b, inverse_token_mask_b, mask_target_b, nsp_target_b = get_batch(7)\n",
        "inp_b.shape, attention_mask_b.shape, inverse_token_mask_b.shape, mask_target_b.shape, nsp_target_b.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oe7j-Wc8P7VJ",
        "outputId": "dc7f13c2-44bb-46f0-9c76-512307c1e822"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([7, 57]),\n",
              " torch.Size([7, 57]),\n",
              " torch.Size([7, 57]),\n",
              " torch.Size([7, 57]),\n",
              " torch.Size([7, 2]))"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_steps = 1000000\n",
        "batch_size = 32\n",
        "mlm_lossi = []\n",
        "nsp_lossi = []\n",
        "ud = []\n",
        "log_interval = 50\n",
        "\n",
        "running_mlm_loss = 0.0\n",
        "running_nsp_loss = 0.0\n",
        "running_loss_dev = 0.0\n",
        "running_loss_steps = 0\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # Reset grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # Forward\n",
        "\n",
        "  # inverse_token_mask_b ---> (B, T)\n",
        "  # mlm_target_b ---> (B, T)\n",
        "  # nsp_target_b ---> (B, 2)\n",
        "  inp_b, attention_mask_b, inverse_token_mask_b, mlm_target_b, nsp_target_b = get_batch(batch_size)\n",
        "\n",
        "  # token_predictions_ll ---> (B, T, vocab_size)\n",
        "  # nsp_classification_logits ----> (B, 2)\n",
        "  token_predictions_ll, nsp_classification_logits = bert(inp_b, attention_mask_b)\n",
        "\n",
        "  # Calculate MLM loss\n",
        "  mlm_mask = inverse_token_mask_b.unsqueeze(-1).expand_as(token_predictions_ll)\n",
        "  # Note: the masked_fill_ is of no use, because the NLLLoss already ignores the\n",
        "  # target 0. `NLLLoss(ignore_index=0)`. I verified it by comparing the loss\n",
        "  # with and without the masked_fill_.\n",
        "  #\n",
        "  # Note: `masked_fill` is the out-of-place version of `masked_fill_()`. Using\n",
        "  # the later causes error:\n",
        "  #   \"one of the variables needed for gradient computation has been modified by an inplace operation\"\n",
        "  masked_token_predictions_ll = token_predictions_ll.masked_fill(mlm_mask, 0.0)\n",
        "  # The `transpose` is because NLLLoss expects class dim (here is vocab_size) is\n",
        "  # before the time dim.\n",
        "  mlm_loss = mlm_criterion(masked_token_predictions_ll.transpose(-1, -2),\n",
        "                           mlm_target_b)\n",
        "  running_mlm_loss += mlm_loss.item()\n",
        "  # Calculate NSP loss\n",
        "  nsp_loss = nsp_criterion(nsp_classification_logits, nsp_target_b)\n",
        "  running_nsp_loss += nsp_loss.item()\n",
        "  # Total loss\n",
        "  loss = mlm_loss + nsp_loss\n",
        "  running_loss_steps += 1\n",
        "\n",
        "  # Backward\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if i % log_interval == 0:\n",
        "    print(f'{i} / {max_steps}: mlm_loss={running_mlm_loss/running_loss_steps: .4f}, nsp_loss={running_nsp_loss/running_loss_steps: .8f}')\n",
        "    running_mlm_loss = 0.0\n",
        "    running_nsp_loss = 0.0\n",
        "    running_loss_steps = 0\n",
        "\n",
        "  mlm_lossi.append(mlm_loss.log10().item())\n",
        "  nsp_lossi.append(nsp_loss.log10().item())\n",
        "\n",
        "  # break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8TCc7OrPEFO",
        "outputId": "d976229e-bd39-4fbb-992a-da45688889cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 / 1000000: mlm_loss= 10.8657, nsp_loss= 0.15214130\n",
            "50 / 1000000: mlm_loss= 9.3387, nsp_loss= 0.00398252\n",
            "100 / 1000000: mlm_loss= 8.2279, nsp_loss= 0.00156469\n",
            "150 / 1000000: mlm_loss= 8.0208, nsp_loss= 0.00111197\n",
            "200 / 1000000: mlm_loss= 7.7657, nsp_loss= 0.00080882\n",
            "250 / 1000000: mlm_loss= 7.6690, nsp_loss= 0.00068373\n",
            "300 / 1000000: mlm_loss= 7.5994, nsp_loss= 0.00062842\n",
            "350 / 1000000: mlm_loss= 7.5072, nsp_loss= 0.00050424\n",
            "400 / 1000000: mlm_loss= 7.4690, nsp_loss= 0.00058792\n",
            "450 / 1000000: mlm_loss= 7.4295, nsp_loss= 0.00053921\n",
            "500 / 1000000: mlm_loss= 7.4184, nsp_loss= 0.00052484\n",
            "550 / 1000000: mlm_loss= 7.3474, nsp_loss= 0.00052572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(mlm_lossi)"
      ],
      "metadata": {
        "id": "ebqLFmqBIGr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(nsp_lossi)"
      ],
      "metadata": {
        "id": "qyJgw7jxIBQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "xhW8S9w6vhvE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}