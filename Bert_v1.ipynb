{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2zF2MQY15X281nBA6JD1+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizhieffe/language_model/blob/main/Bert_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Good tutorial: https://coaxsoft.com/blog/building-bert-with-pytorch-from-scratch\n",
        "\n",
        "Another tutorial: https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial\n"
      ],
      "metadata": {
        "id": "vgugZfdOJzaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import re\n",
        "\n",
        "from typing import Dict, List\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "!pip install tqdm\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wX-MYEZUKnwa",
        "outputId": "bde4908d-6daa-490e-e3af-660820dea7fa"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "USE_GPU = True\n",
        "\n",
        "BLOCK_SIZE = 96 # Context length: how many chars do we take to predict the next one?\n",
        "\n",
        "# number of workers in .map() call\n",
        "# good number to use is ~order number of cpu cores // 2\n",
        "NUM_PROC = 24"
      ],
      "metadata": {
        "id": "cXhfykcByLoi"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer\n",
        "\n",
        "- **TODO**: the tokenizer in IMDBBertDataset doesn't convert the word to id. It similar to splitting the sentence to words. Integrate with a more advanced one."
      ],
      "metadata": {
        "id": "eIpZ2CQ52aC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Dataset"
      ],
      "metadata": {
        "id": "P3kYTRN8yAk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download data - openwebtext\n",
        "\n",
        "!pip install datasets # Since we are running in colab docker image, install it here.\n",
        "\n",
        "from datasets import load_dataset # huggingface datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLi0aaU9yB4G",
        "outputId": "cc659149-113f-4c8d-e0fe-1d73cb1481ec"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Using cached datasets-2.14.5-py3-none-any.whl (519 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Using cached dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Using cached xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Using cached multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "Successfully installed datasets-2.14.5 dill-0.3.7 huggingface-hub-0.17.3 multiprocess-0.70.15 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"imdb\", num_proc=NUM_PROC)"
      ],
      "metadata": {
        "id": "23ytzd1UyGgC"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = dataset['train']"
      ],
      "metadata": {
        "id": "P7JmJd0WybG5"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for it  in train_ds:\n",
        "  print(it)\n",
        "  i += 1\n",
        "  if i > 4:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWdkk8TbyhyC",
        "outputId": "2eeb396c-6154-4eb1-b5f4-4a4afee5f27a"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'review': \"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\", 'sentiment': 'positive'}\n",
            "{'review': 'A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master\\'s of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell\\'s murals decorating every surface) are terribly well done.', 'sentiment': 'positive'}\n",
            "{'review': 'I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.<br /><br />This was the most I\\'d laughed at one of Woody\\'s comedies in years (dare I say a decade?). While I\\'ve never been impressed with Scarlet Johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.<br /><br />This may not be the crown jewel of his career, but it was wittier than \"Devil Wears Prada\" and more interesting than \"Superman\" a great comedy to go see with friends.', 'sentiment': 'positive'}\n",
            "{'review': \"Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.\", 'sentiment': 'negative'}\n",
            "{'review': 'Petter Mattei\\'s \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. <br /><br />This being a variation on the Arthur Schnitzler\\'s play about the same theme, the director transfers the action to the present time New York where all these different characters meet and connect. Each one is connected in one way, or another to the next person, but no one seems to know the previous point of contact. Stylishly, the film has a sophisticated luxurious look. We are taken to see how these people live and the world they live in their own habitat.<br /><br />The only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits. A big city is not exactly the best place in which human relations find sincere fulfillment, as one discerns is the case with most of the people we encounter.<br /><br />The acting is good under Mr. Mattei\\'s direction. Steve Buscemi, Rosario Dawson, Carol Kane, Michael Imperioli, Adrian Grenier, and the rest of the talented cast, make these characters come alive.<br /><br />We wish Mr. Mattei good luck and await anxiously for his next work.', 'sentiment': 'positive'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drZrdM6PIJoz",
        "outputId": "59d416c0-4703-4e23-efbe-e656ed29559c"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['review', 'sentiment'],\n",
              "    num_rows: 50000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare DS\n",
        "\n",
        "- The original BERT uses BooksCorpus (800M words) and English Wikipedia (2,500M words) for pre-training.\n",
        "- We use IMDB reviews data with ~72k words."
      ],
      "metadata": {
        "id": "UrlatiLTy2RW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Counter():\n",
        "  \"\"\"Store the counts for individual tokens.\"\"\"\n",
        "  def __init__(self):\n",
        "    self.token_to_counts = {}\n",
        "\n",
        "  def update(self, tokens:  List[int]):\n",
        "    \"\"\"Update the counts with new tokens\"\"\"\n",
        "    for t in tokens:\n",
        "      if t in self.token_to_counts:\n",
        "        self.token_to_counts[t] += 1\n",
        "      else:\n",
        "        self.token_to_counts[t] = 1\n",
        "\n",
        "  def get(self) -> Dict[str, int]:\n",
        "    return self.token_to_counts.copy()\n",
        "\n",
        "  def __str__(self):\n",
        "    s = sorted(self.token_to_counts.items(), key=lambda x:x[1], reverse=True)\n",
        "    return s.__str__()"
      ],
      "metadata": {
        "id": "jk9z_0d_Kwvb"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab:\n",
        "  def __init__(self):\n",
        "    self.ttoi = {}\n",
        "    self.itot = {}\n",
        "\n",
        "  def insert_token(self, t: str):\n",
        "    assert t not in self.ttoi\n",
        "    i = len(self.ttoi)\n",
        "    self.ttoi[t] = i\n",
        "    self.itot[i] = t\n",
        "\n",
        "  def lookup_indices(self, tokens: List[str]):\n",
        "    return [self.ttoi[t] for t in tokens]"
      ],
      "metadata": {
        "id": "dhdTZ-KmQIyA"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "from torchtext.data import get_tokenizer\n",
        "import random\n",
        "\n",
        "class IMDBBertDataset(Dataset):\n",
        "  # Define special tokens as attributes of class\n",
        "  CLS = '[CLS]'\n",
        "  PAD = '[PAD]'\n",
        "  SEP = '[SEP]'\n",
        "  MASK = '[MASK]'\n",
        "  UNK = '[UNK]'\n",
        "\n",
        "  MASK_PERCENTAGE = 0.15\n",
        "\n",
        "  MASKED_INDICES_COLUMN = 'masked_indices'\n",
        "  TARGET_COLUMN = 'indices'\n",
        "  NSP_TARGET_COLUMN = 'is_next'\n",
        "  TOKEN_MASK_COLUMN = 'token_mask'\n",
        "\n",
        "  OPTIMAL_LENGTH_PERCENTILE = 70\n",
        "\n",
        "  def __init__(self,\n",
        "               ds_from=None,\n",
        "               ds_to=None,\n",
        "               should_include_text: bool=False):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      should_include_text: if true, include the raw text in the dataset. This\n",
        "        should only be used for debugging purpose.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.ds = []\n",
        "    for it in dataset['train']:\n",
        "      self.ds.append(it['text'])\n",
        "\n",
        "    self.tokenizer = get_tokenizer('basic_english')\n",
        "    self.counter = Counter()\n",
        "    self.vocab = Vocab()\n",
        "\n",
        "    self.optimal_sentence_length = None\n",
        "    self.should_include_text = should_include_text\n",
        "\n",
        "    if self.should_include_text:\n",
        "      self.columns = [\n",
        "          'masked_sentence',\n",
        "          self.MASKED_INDICES_COLUMN,\n",
        "          'sentence',\n",
        "          self.TARGET_COLUMN,\n",
        "          self.TOKEN_MASK_COLUMN,\n",
        "          self.NSP_TARGET_COLUMN,\n",
        "      ]\n",
        "    else:\n",
        "      self.columns = [\n",
        "          self.MASKED_INDICES_COLUMN,\n",
        "          self.TARGET_COLUMN,\n",
        "          self.TOKEN_MASK_COLUMN,\n",
        "          self.NSP_TARGET_COLUMN,\n",
        "      ]\n",
        "\n",
        "    self.df = self._prepare_dataset()\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.df)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return None\n",
        "\n",
        "  def _update_length(self,\n",
        "                     review_sentences: List[str],\n",
        "                     sentence_lens: List[int]):\n",
        "    for s in review_sentences:\n",
        "      sentence_lens.append(len(s.split()))\n",
        "\n",
        "  def _find_optimal_sentence_length(self, sentence_lens: List[int]):\n",
        "    arr = np.array(sentence_lens)\n",
        "    ret = int(np.percentile(arr, self.OPTIMAL_LENGTH_PERCENTILE))\n",
        "    return ret\n",
        "\n",
        "  def _fill_vocab(self, min_freq=2):\n",
        "    self.vocab.insert_token(self.CLS)\n",
        "    self.vocab.insert_token(self.PAD)\n",
        "    self.vocab.insert_token(self.MASK)\n",
        "    self.vocab.insert_token(self.SEP)\n",
        "    self.vocab.insert_token(self.UNK)\n",
        "\n",
        "    token_to_counts = self.counter.get()\n",
        "    for t, counts in tqdm(token_to_counts.items()):\n",
        "      if counts >= min_freq:\n",
        "        self.vocab.insert_token(t)\n",
        "\n",
        "  def _create_item(self, first: List[int], second: List[int], target: int):\n",
        "    return None\n",
        "\n",
        "  def _select_false_nsp_sentences(self, sentences: List[str]):\n",
        "    sentences_len = len(sentences)\n",
        "    i1 = random.randint(0, sentences_len-1)\n",
        "    i2 = random.randint(0, sentences_len-1)\n",
        "\n",
        "    # Make sure they are really not NSP\n",
        "    while i1 == i2 - 1:\n",
        "      i2 = random.randint(0, sentences_len-1)\n",
        "\n",
        "    return sentences[i1], sentences[i2]\n",
        "\n",
        "  def _prepare_dataset(self) -> pd.DataFrame:\n",
        "    sentences = []\n",
        "    nsp = []\n",
        "    sentence_lens = []\n",
        "\n",
        "    # split ds on sentences\n",
        "    for review in self.ds:\n",
        "      review_sentences = review.split('. ')\n",
        "      sentences += review_sentences\n",
        "      self._update_length(review_sentences, sentence_lens)\n",
        "\n",
        "    self.optimal_sentence_length = self._find_optimal_sentence_length(sentence_lens)\n",
        "    print(f'{self.optimal_sentence_length=}')\n",
        "\n",
        "    # Create vocab\n",
        "    print(\"Create vocabulary\")\n",
        "    for s in tqdm(sentences):\n",
        "      self.counter.update(self.tokenizer(s))\n",
        "    self._fill_vocab()\n",
        "    print(f'\\nvocab size = {len(self.vocab.ttoi)}')\n",
        "\n",
        "    assert len(sentence_lens) == len(sentences)\n",
        "    # print(self.counter)\n",
        "\n",
        "    for review in self.ds:\n",
        "      review_sentences = review.split('. ')\n",
        "      if len(review_sentences) > 1:\n",
        "        for i in range(len(review_sentences) - 1):\n",
        "          # True NSP item\n",
        "          first, second = self.tokenizer(review_sentences[i]), self.tokenizer(review_sentences[i+1])\n",
        "          print(f'{first=}, {second=}')\n",
        "          nsp.append(self._create_item(first, second, target=1))\n",
        "\n",
        "          # False NSP item\n",
        "          first, second = self._select_false_nsp_sentences(sentences)\n",
        "          first, second = self.tokenizer(first), self.tokenizer(second)\n",
        "          print(f'{first=}, {second=}')\n",
        "          nsp.append(self._create_item(first, second, target=0))\n",
        "\n",
        "          # break\n",
        "      # break\n",
        "\n",
        "    # df = pd.DataFrame(nsp, columns=self.columns)\n",
        "\n",
        "ds = IMDBBertDataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdy7uteQzh-K",
        "outputId": "fbd18dfa-e3ca-4b12-f6f2-9ac22771abbe"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "self.optimal_sentence_length=27\n",
            "Create vocabulary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 247731/247731 [00:09<00:00, 27400.76it/s]\n",
            "100%|██████████| 100682/100682 [00:00<00:00, 1162720.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "vocab size = 51721\n",
            "first=['i', 'rented', 'i', 'am', 'curious-yellow', 'from', 'my', 'video', 'store', 'because', 'of', 'all', 'the', 'controversy', 'that', 'surrounded', 'it', 'when', 'it', 'was', 'first', 'released', 'in', '1967'], second=['i', 'also', 'heard', 'that', 'at', 'first', 'it', 'was', 'seized', 'by', 'u', '.', 's']\n",
            "first=['this', 'show', 'is', 'highly', 'overrated', ',', 'and', 'less', 'worthy', 'of', 'your', 'channel', 'surfing', 'time', 'than', 'saturday', 'night', 'live', ',', 'another', 'horrible', 'show'], second=['all', 'of', 'fred', 'and', 'ginger', \"'\", 's', 'movies', 'had', 'sub', 'plots', 'that', 'depended', 'on', 'other', 'actors', 'to', 'fill', 'in', 'the', 'space', 'between', 'the', 'musical', 'numbers', ',', 'otherwise', 'the', 'movie', 'would', 'have', 'to', 'be', 'shortened', 'by', 'about', 'a', 'half', 'hour']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds.vocab.lookup_indices([\"[CLS]\", \"this\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWjq1D2pTTZJ",
        "outputId": "88e33395-0957-47b0-8668-f79313000c85"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 17]"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "s_rL4USpJ5mc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QovUcvF3x_wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\n",
        "       'Hello, how are you? I am Romeo.n'\n",
        "       'Hello, Romeo My name is Juliet. Nice to meet you.n'\n",
        "       'Nice meet you too. How are you today?n'\n",
        "       'Great. My baseball team won the competition.n'\n",
        "       'Oh Congratulations, Julietn'\n",
        "       'Thanks you Romeo'\n",
        "   )\n",
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObVzaIPRKH0D",
        "outputId": "295b474d-7a50-4057-932e-064482bbcfe2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "208"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = text.lower()\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "9WmKUxF0KILu",
        "outputId": "9cb51035-6448-45b3-ad66-3f9fda154828"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello, how are you? i am romeo.nhello, romeo my name is juliet. nice to meet you.nnice meet you too. how are you today?ngreat. my baseball team won the competition.noh congratulations, julietnthanks you romeo'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) filter '.', ',', '?', '!'\n",
        "# 2) create new line at 'n'\n",
        "sentences = re.sub(\"[.,!?-]\", '', text.lower()).split('n')\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cg7_rGilKky5",
        "outputId": "22028d77-d2c8-4b20-b5e7-9b51352f0602"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello how are you i am romeo',\n",
              " 'hello romeo my ',\n",
              " 'ame is juliet ',\n",
              " 'ice to meet you',\n",
              " '',\n",
              " 'ice meet you too how are you today',\n",
              " 'great my baseball team wo',\n",
              " ' the competitio',\n",
              " '',\n",
              " 'oh co',\n",
              " 'gratulatio',\n",
              " 's juliet',\n",
              " 'tha',\n",
              " 'ks you romeo']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_list = list(set(\" \".join(sentences).split()))\n",
        "word_list[:8]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3V0zL5xK3jg",
        "outputId": "4aa15a30-8596-45ea-c7c9-0515be43a236"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['juliet', 'romeo', 'ks', 'gratulatio', 'the', 'to', 'competitio', 'my']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wtoi = {\n",
        "    '[PAD]': 0,\n",
        "    '[CLS]': 1,\n",
        "    '[SEP]': 2,\n",
        "    '[MASK]': 3\n",
        "}\n",
        "\n",
        "for i, w in enumerate(word_list):\n",
        "  wtoi[w] = len(wtoi)\n",
        "\n",
        "itow = {}\n",
        "for w, i in wtoi.items():\n",
        "  itow[i] = w\n",
        "\n",
        "vocab_size = len(wtoi)\n",
        "\n",
        "print(f'{vocab_size=}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5ZqpwCWL36O",
        "outputId": "a7a0dbfd-87dc-4a9e-d117-25349e19bc98"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size=32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Masking: Bert randomly assigns [MASK] to 15% of the sequence. Note that it\n",
        "#   is not assigned to special tokens.\n",
        "#\n",
        "# 2. [PAD] is used to make sure all the sentences are of equal length.\n",
        "#\n",
        "#   For instance, if we take the sentence :\n",
        "#       “The cat is walking. The dog is barking at the tree”\n",
        "#   then with padding, it will look like this:\n",
        "#       “[CLS] The cat is walking [PAD] [PAD] [PAD]. [CLS] The dog is barking at the tree.”\n",
        "#\n",
        "#   The length of the 1st sentence is equal to the length of the 2nd sentence.\n",
        "\n",
        "def make_batch(sentences, batch_size: int, sentence_length: int):\n",
        "  \"\"\"Make a batch.\n",
        "\n",
        "  Args:\n",
        "    sentences: array of str\n",
        "    batch_size: batch size\n",
        "    sentence_length: the length of a sentence. Note that in each example there are\n",
        "    two sentences.\n",
        "  \"\"\"\n",
        "  batch = []\n",
        "  positive = negative = 0\n",
        "\n",
        "\n",
        "\n",
        "  while positive != batch_size / 2 or negative != batch_size / 2:\n",
        "    tokens_a_index = torch.randint(0, len(sentences), (batch_size,))\n",
        "    tokens_b_index = torch.randint(0, len(sentences), (batch_size,))\n",
        "\n",
        "    tokens_a = sentences"
      ],
      "metadata": {
        "id": "5QrKTdi2onEx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.randint(0, len(sentences), (1,)).item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENVtQW2Dso_5",
        "outputId": "79b3681c-f46f-4ce0-a5a0-66267e01e5b3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "fFiuPDAvJ9Mi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss and Optimization"
      ],
      "metadata": {
        "id": "x9pHh0avJ-rH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "5HHmVcAOKAkt"
      }
    }
  ]
}