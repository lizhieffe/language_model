{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNph4nbWvgtS+TPt0oAfX5h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizhieffe/language_model/blob/main/Shakespeare_LM_v4_Causal_Mask_%2B_Customized_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "qUDLOQdFcErB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "fRMxsDXmpbMf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "USE_GPU = True\n",
        "\n",
        "BLOCK_SIZE = 96 # Context length: how many chars do we take to predict the next one?"
      ],
      "metadata": {
        "id": "vDHUDjtsph4G"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup GPU"
      ],
      "metadata": {
        "id": "NZIvnPdcps3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_GPU:\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  assert device.type != 'cpu', \"GPU is not available\"\n",
        "else:\n",
        "  device = 'cpu'\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sd95wthQptwk",
        "outputId": "da938539-2006-498c-89c6-24ef348daf37"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g_cpu = torch.Generator(device='cpu').manual_seed(2147483647) # for reproducibility\n",
        "g_device = torch.Generator(device=device).manual_seed(2147483647) # for reproducibility"
      ],
      "metadata": {
        "id": "FDyu_mmdxQNC"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Util Functions"
      ],
      "metadata": {
        "id": "hHD6RNbQpv4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "CXSgUDhjdCxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the names.txt file from github\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "# read in all the words\n",
        "text = open('input.txt', 'r').read()\n",
        "\n",
        "text[:800]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "Uw1N2UjndEDQ",
        "outputId": "172c3239-5129-458c-9745-2b6ec3782b10"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-02 05:49:16--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.10’\n",
            "\n",
            "input.txt.10        100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2023-10-02 05:49:16 (17.8 MB/s) - ‘input.txt.10’ saved [1115394/1115394]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to p\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'total char # = {len(text)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_Lr-G6NdF62",
        "outputId": "15519758-e9d6-4e15-c6c8-e624c5c68469"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total char # = 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build vocabulary"
      ],
      "metadata": {
        "id": "ZFA5J8NjdHj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(''.join(text))))\n",
        "\n",
        "stoi = {c:i for i,c in enumerate(chars)}\n",
        "itos = {i:c for c,i in stoi.items()}\n",
        "\n",
        "assert len(stoi) == len(itos)\n",
        "\n",
        "vocab_size = len(stoi)\n",
        "\n",
        "EXPECTED_VOCAB_SIZE = 65\n",
        "assert vocab_size == EXPECTED_VOCAB_SIZE, f\"expected vocab size = {EXPECTED_VOCAB_SIZE}, got {vocab_size}\""
      ],
      "metadata": {
        "id": "K31lXht9dIww"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# str ---> list of integer\n",
        "encode = lambda str: [stoi[s] for s in str]\n",
        "\n",
        "# list of integer ---> str\n",
        "decode = lambda l: ''.join(itos[i] for i in l)\n",
        "\n",
        "_test_str = \"adb dfd \\n\"\n",
        "assert _test_str == decode(encode(_test_str))"
      ],
      "metadata": {
        "id": "_wjY-ZmAK_Mt"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create DS"
      ],
      "metadata": {
        "id": "tnC-XqfXdKoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n1 = int(len(text) * 0.9)\n",
        "train_data = encode(text[:n1])\n",
        "dev_data = encode(text[n1:])\n",
        "\n",
        "print(f'{len(train_data)=}, {len(dev_data)=}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vU05Ihvwl_b",
        "outputId": "d0ea9729-20db-4db4-8eb7-e6a261d28452"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(train_data)=1003854, len(dev_data)=111540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(data, batch_size: int, block_size: int):\n",
        "  \"\"\" Sample a batch using Causal style. \"\"\"\n",
        "  x_list = []\n",
        "  y_list = []\n",
        "  ix = torch.randint(0, len(data)-block_size, (batch_size,), generator=g_cpu)\n",
        "  X = torch.stack([torch.tensor(data[i:i+block_size]) for i in ix]).to(device)\n",
        "  Y = torch.stack([torch.tensor(data[i+1:i+1+block_size]) for i in ix]).to(device)\n",
        "  return X, Y"
      ],
      "metadata": {
        "id": "eWvuZyaOwXai"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = get_batch(train_data, 16, BLOCK_SIZE)\n",
        "\n",
        "for b in range(3):\n",
        "  it = 0\n",
        "  for t in range(X.shape[1]):\n",
        "    print(f'{decode(X[b, :t+1].tolist())} ---> {decode([Y[b, t].item()])}')\n",
        "    it += 1\n",
        "    if it > 7:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2EL4GRCyDcr",
        "outputId": "95ab0571-d6c4-4c73-dab5-26fee87dd016"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e ---> v\n",
            "ev ---> e\n",
            "eve ---> r\n",
            "ever ---> y\n",
            "every --->  \n",
            "every  ---> d\n",
            "every d ---> a\n",
            "every da ---> n\n",
            "e ---> s\n",
            "es ---> h\n",
            "esh --->  \n",
            "esh  ---> c\n",
            "esh c ---> o\n",
            "esh co ---> m\n",
            "esh com ---> p\n",
            "esh comp ---> l\n",
            "e --->  \n",
            "e  ---> r\n",
            "e r ---> e\n",
            "e re ---> t\n",
            "e ret ---> i\n",
            "e reti ---> r\n",
            "e retir ---> e\n",
            "e retire ---> d\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "bm2SpJ6CfOHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_embd = 768 # the dimensionality of the character embedding vectors\n",
        "d_head = 2 * n_embd # the dim of the transformer's head\n",
        "N_HIDDEN = n_embd * 4 # the number of neurons in the hidden layer of the MLP\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility"
      ],
      "metadata": {
        "id": "lAE6DUi2qcNO"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model class"
      ],
      "metadata": {
        "id": "o2LWaIYbri0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(torch.nn.Module):\n",
        "  \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
        "\n",
        "  def __init__(self, ndim: int, bias: bool):\n",
        "    super().__init__()\n",
        "    self.weight = torch.nn.Parameter(torch.ones(ndim))\n",
        "    self.bias = torch.nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "  def forward(self, x):\n",
        "    return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)"
      ],
      "metadata": {
        "id": "R-mApi-YzvvD"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, d_in, d_hidden, d_head):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      vocab_size: volabulary size\n",
        "      d_in: dim of input. If this is the immediate next layer of the token\n",
        "        embedding layer, this is the dim of the embedding for a token.\n",
        "      d_hidden: dim of hidden FFN layers\n",
        "      d_head: dim of the transformer head\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_head = d_head\n",
        "    self.key1 = torch.nn.Linear(d_in, d_head, bias=False)\n",
        "    self.query1 = torch.nn.Linear(d_in, d_head, bias=False)\n",
        "    self.value1 = torch.nn.Linear(d_in, d_head, bias=False)\n",
        "\n",
        "    self.linear1 = torch.nn.Linear(d_head, d_hidden, bias=True)\n",
        "    self.tanh1 = torch.nn.Tanh()\n",
        "    # Project d_hidden back to d_head as output\n",
        "    self.proj = torch.nn.Linear(d_hidden, d_head, bias=True)\n",
        "\n",
        "    self.ln1 = LayerNorm(d_in, bias=True)\n",
        "    self.ln2 = LayerNorm(d_head, bias=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      x: (B, T, C). The input to the model.\n",
        "    Returns:\n",
        "      loss: int\n",
        "      logits: (B, T, C)\n",
        "    \"\"\"\n",
        "\n",
        "    T = x.shape[-2]\n",
        "    tril = torch.tril(torch.ones(T, T)).to(device)\n",
        "\n",
        "    x = self.ln1(x)\n",
        "\n",
        "    k = self.key1(x)  # (B, T, d_head)\n",
        "    q = self.query1(x) # (B, T, d_head)\n",
        "    wei = k @ q.transpose(-2, -1) # (B, T, d_head) @ (B, d_head, T) = (B, T, T)\n",
        "    wei = wei.masked_fill(tril==0, -float('inf')) # (B, T, T)\n",
        "    wei = wei * self.d_head**-0.5\n",
        "    wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "\n",
        "    v = self.value1(x) # (B, T, d_head)\n",
        "    # This makes the y[:,t,:], to have the information of the embedding (v)\n",
        "    # v[:, u (u<=t), :], but not have the infomration of v[:, w (w>t), :]\n",
        "    #\n",
        "    # 1. Spread v's information at all t to y[:, any t, :]\n",
        "    #\n",
        "    # Because v is on rhs of @, its information at different T are spread out to\n",
        "    # the different T in y\n",
        "    #\n",
        "    # Think about (T, T) @ (T, d_head) = (T, d_head)\n",
        "    #\n",
        "    # a11, a12     b1   a11*b1+a12*b2\n",
        "    # a21, a22  @  b2 = a21*b1+a22*b2\n",
        "    #\n",
        "    # In the result, at T=1, it has b1 and b2, which are the rhs of @'s info at\n",
        "    # different T\n",
        "    #\n",
        "    # 2. Limit y[:, t, :] to not access v[:, w (w>t), :].\n",
        "    #\n",
        "    # This is done by `tril`\n",
        "    y = wei @ v # (B, T, d_head)\n",
        "\n",
        "    y = self.ln2(y)\n",
        "\n",
        "    # It doesn't need tril here, because the lhs and rhs doesn't exchange\n",
        "    # information at different T.\n",
        "    #\n",
        "    # Let's say:\n",
        "    # - input is y (B, T, d_head)\n",
        "    # - Linear(d_in, d_out) is a matrix l(d_in, d_out), here dim is l(d_head, d_hidden)\n",
        "    # - result is z (B, T, d_hidden)\n",
        "    #\n",
        "    # linear(y) = y @ l = z\n",
        "    #\n",
        "    # To simplify, ignore B, T=3, d_head=2, d_hidden=1\n",
        "    #\n",
        "    #         y11 y12         y11*l1+y12*l2\n",
        "    # y @ l = y21 y22 @ l1  = y21*l1+y22*l2\n",
        "    #         y31 y32   l2    y31*l1+y32*l2\n",
        "    #\n",
        "    # We can see z[:, T, :] only contains y[:, T, :]'s info\n",
        "    #\n",
        "    # To summarize this and the previous section\n",
        "    #\n",
        "    # Z = X @ Y\n",
        "    #\n",
        "    # Z[T, :] only contains X[T, :]'s info, doesn't contain X[S != T, :]'s infor\n",
        "    # Z[T, :] contains Y[at any index, :]'s info\n",
        "    y = self.linear1(y) # (B, T, d_hidden)\n",
        "    y = self.tanh1(y) # (B, T, d_hidden)\n",
        "    y = self.proj(y) # (B, T, d_head)\n",
        "\n",
        "    return y\n",
        "\n",
        "  def generate(self, idx, max_new_tokens: int):\n",
        "    \"\"\"Generate new tokens given idx as context.\n",
        "\n",
        "    Args:\n",
        "      idx: (B, T)\n",
        "      max_new_tokens: number of new tokens to generate\n",
        "\n",
        "    Returns:\n",
        "      (B,T+max_new_tokens)\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, loss = self(idx, targets=None)  # logits: (B, T, vocab_size), loss: None\n",
        "      logits = logits[:, -1, :] # (B, vocab_size)\n",
        "      prob = F.softmax(logits, dim=1) # (B, vocab_size)\n",
        "      idx_next = torch.multinomial(prob, num_samples=1, replacement=False)  # (B, 1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "LT6iVQP14kXa"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, d_embd, d_hidden, d_head):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      vocab_size: volabulary size\n",
        "      d_embd: dim of embedding for the token\n",
        "      d_hidden: dim of hidden FFN layers\n",
        "      d_head: dim of the transformer head\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_head = d_head\n",
        "\n",
        "    self.embd = torch.nn.Embedding(\n",
        "        num_embeddings=vocab_size,\n",
        "        embedding_dim=d_embd\n",
        "    )\n",
        "    self.attn1 = AttentionBlock(vocab_size, d_embd, d_hidden, d_head)\n",
        "    self.attn2 = AttentionBlock(vocab_size, d_head, d_hidden, d_head)\n",
        "    self.linear_logit = torch.nn.Linear(d_head, vocab_size, bias=True)\n",
        "\n",
        "\n",
        "  def forward(self, x, targets=None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      x: (B, T). The input to the model.\n",
        "      targets: (B, T). When it is not None, the func calculates and return the\n",
        "        loss in additional to other returned item(s)\n",
        "    Returns:\n",
        "      loss: int\n",
        "      logits: (B, T, C)\n",
        "    \"\"\"\n",
        "\n",
        "    T = x.shape[-1]\n",
        "    tril = torch.tril(torch.ones(T, T)).to(device)\n",
        "\n",
        "    xemb = self.embd(x) # (B, T, C)\n",
        "\n",
        "    y = self.attn1(xemb)\n",
        "    y = self.attn2(y)\n",
        "\n",
        "    logits = self.linear_logit(y) # (B, T, vocab_size)\n",
        "    logits = logits.view(-1, logits.shape[-1]) # (B, T, vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      loss = F.cross_entropy(logits, targets.view(-1))\n",
        "\n",
        "    return logits.view(-1, T, logits.shape[1]), loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens: int):\n",
        "    \"\"\"Generate new tokens given idx as context.\n",
        "\n",
        "    Args:\n",
        "      idx: (B, T)\n",
        "      max_new_tokens: number of new tokens to generate\n",
        "\n",
        "    Returns:\n",
        "      (B, T+max_new_tokens)\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, loss = self(idx, targets=None)  # logits: (B, T, vocab_size), loss: None\n",
        "      logits = logits[:, -1, :] # (B, vocab_size)\n",
        "      prob = F.softmax(logits, dim=1) # (B, vocab_size)\n",
        "      idx_next = torch.multinomial(prob, num_samples=1, replacement=False)  # (B, 1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "net = Net(vocab_size, d_embd=n_embd, d_hidden=N_HIDDEN, d_head=N_HIDDEN).to(device)"
      ],
      "metadata": {
        "id": "6QusOiY5HEBH"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_total_params = 0\n",
        "\n",
        "for p in net.parameters():\n",
        "  _total_params += p.nelement()\n",
        "\n",
        "print(f'Total params = {_total_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eMi8TezrkY4",
        "outputId": "21f7c798-eb19-4ec2-8921-b38ae5d42b60"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total params = 73420097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define optimizer"
      ],
      "metadata": {
        "id": "m04BZ-9f26w7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "# optimizer = optim.Adam(net.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "5xJsIU2z28us"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "5ObPwkVj3MFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_steps = 700000\n",
        "batch_size = 64\n",
        "lossi = []\n",
        "lossi_dev = []\n",
        "ud = []\n",
        "log_interval = 50\n",
        "\n",
        "running_loss = 0.0\n",
        "running_loss_dev = 0.0\n",
        "running_loss_steps = 0\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # Forward\n",
        "  Xb, Yb = get_batch(train_data, batch_size, BLOCK_SIZE)\n",
        "  optimizer.zero_grad()\n",
        "  logits, loss = net(Xb, targets=Yb)\n",
        "\n",
        "  # Loss\n",
        "  # print(f'{outputs.shape=}, {Yb.shape=}')\n",
        "  running_loss += loss.item()\n",
        "  running_loss_steps += 1\n",
        "\n",
        "  # Eval dev DS\n",
        "  Xb_dev, Yb_dev = get_batch(dev_data, batch_size, BLOCK_SIZE)\n",
        "  logits_dev, loss_dev = net(Xb_dev, targets=Yb_dev)\n",
        "  running_loss_dev += loss_dev.item()\n",
        "\n",
        "  # Update\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # Track status\n",
        "  if i % log_interval == 0:\n",
        "    print(f'{i}/{max_steps}: training loss={running_loss/running_loss_steps:.4f}, dev loss={running_loss_dev/running_loss_steps:.4f}')\n",
        "    running_loss = 0.0\n",
        "    running_loss_dev = 0.0\n",
        "    running_loss_steps = 0\n",
        "\n",
        "  lossi.append(loss.log10().item())\n",
        "  lossi_dev.append(loss_dev.log10().item())"
      ],
      "metadata": {
        "id": "3BtGf_U93M9V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "75831bd5-d446-455e-97e9-1b72774dbafe"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0/700000: training loss=4.2490, dev loss=4.2361\n",
            "50/700000: training loss=3.4579, dev loss=3.5016\n",
            "100/700000: training loss=3.3144, dev loss=3.3549\n",
            "150/700000: training loss=3.3120, dev loss=3.3579\n",
            "200/700000: training loss=3.3101, dev loss=3.3502\n",
            "250/700000: training loss=3.2792, dev loss=3.3124\n",
            "300/700000: training loss=3.2136, dev loss=3.2372\n",
            "350/700000: training loss=3.1404, dev loss=3.1704\n",
            "400/700000: training loss=3.0274, dev loss=3.0506\n",
            "450/700000: training loss=2.9462, dev loss=2.9725\n",
            "500/700000: training loss=2.9040, dev loss=2.9264\n",
            "550/700000: training loss=2.8776, dev loss=2.8941\n",
            "600/700000: training loss=2.8482, dev loss=2.8740\n",
            "650/700000: training loss=2.8131, dev loss=2.8368\n",
            "700/700000: training loss=2.7768, dev loss=2.7869\n",
            "750/700000: training loss=2.7363, dev loss=2.7533\n",
            "800/700000: training loss=2.7154, dev loss=2.7208\n",
            "850/700000: training loss=2.6926, dev loss=2.7048\n",
            "900/700000: training loss=2.6718, dev loss=2.6847\n",
            "950/700000: training loss=2.6597, dev loss=2.6708\n",
            "1000/700000: training loss=2.6406, dev loss=2.6525\n",
            "1050/700000: training loss=2.6247, dev loss=2.6350\n",
            "1100/700000: training loss=2.6140, dev loss=2.6255\n",
            "1150/700000: training loss=2.6090, dev loss=2.6200\n",
            "1200/700000: training loss=2.6017, dev loss=2.6000\n",
            "1250/700000: training loss=2.5857, dev loss=2.5973\n",
            "1300/700000: training loss=2.5836, dev loss=2.5910\n",
            "1350/700000: training loss=2.5794, dev loss=2.5843\n",
            "1400/700000: training loss=2.5729, dev loss=2.5728\n",
            "1450/700000: training loss=2.5657, dev loss=2.5758\n",
            "1500/700000: training loss=2.5563, dev loss=2.5705\n",
            "1550/700000: training loss=2.5541, dev loss=2.5655\n",
            "1600/700000: training loss=2.5515, dev loss=2.5583\n",
            "1650/700000: training loss=2.5474, dev loss=2.5511\n",
            "1700/700000: training loss=2.5355, dev loss=2.5417\n",
            "1750/700000: training loss=2.5337, dev loss=2.5338\n",
            "1800/700000: training loss=2.5214, dev loss=2.5251\n",
            "1850/700000: training loss=2.5209, dev loss=2.5160\n",
            "1900/700000: training loss=2.5164, dev loss=2.5123\n",
            "1950/700000: training loss=2.5134, dev loss=2.5128\n",
            "2000/700000: training loss=2.5088, dev loss=2.5050\n",
            "2050/700000: training loss=2.5045, dev loss=2.5035\n",
            "2100/700000: training loss=2.5043, dev loss=2.4980\n",
            "2150/700000: training loss=2.4961, dev loss=2.4967\n",
            "2200/700000: training loss=2.4988, dev loss=2.4981\n",
            "2250/700000: training loss=2.4898, dev loss=2.4915\n",
            "2300/700000: training loss=2.4864, dev loss=2.4862\n",
            "2350/700000: training loss=2.4861, dev loss=2.4909\n",
            "2400/700000: training loss=2.4839, dev loss=2.4829\n",
            "2450/700000: training loss=2.4812, dev loss=2.4746\n",
            "2500/700000: training loss=2.4787, dev loss=2.4797\n",
            "2550/700000: training loss=2.4780, dev loss=2.4761\n",
            "2600/700000: training loss=2.4719, dev loss=2.4704\n",
            "2650/700000: training loss=2.4701, dev loss=2.4710\n",
            "2700/700000: training loss=2.4678, dev loss=2.4676\n",
            "2750/700000: training loss=2.4675, dev loss=2.4697\n",
            "2800/700000: training loss=2.4654, dev loss=2.4705\n",
            "2850/700000: training loss=2.4650, dev loss=2.4613\n",
            "2900/700000: training loss=2.4639, dev loss=2.4657\n",
            "2950/700000: training loss=2.4621, dev loss=2.4633\n",
            "3000/700000: training loss=2.4593, dev loss=2.4654\n",
            "3050/700000: training loss=2.4522, dev loss=2.4620\n",
            "3100/700000: training loss=2.4544, dev loss=2.4581\n",
            "3150/700000: training loss=2.4531, dev loss=2.4600\n",
            "3200/700000: training loss=2.4552, dev loss=2.4510\n",
            "3250/700000: training loss=2.4496, dev loss=2.4546\n",
            "3300/700000: training loss=2.4533, dev loss=2.4563\n",
            "3350/700000: training loss=2.4510, dev loss=2.4617\n",
            "3400/700000: training loss=2.4477, dev loss=2.4492\n",
            "3450/700000: training loss=2.4479, dev loss=2.4547\n",
            "3500/700000: training loss=2.4467, dev loss=2.4519\n",
            "3550/700000: training loss=2.4408, dev loss=2.4505\n",
            "3600/700000: training loss=2.4470, dev loss=2.4524\n",
            "3650/700000: training loss=2.4453, dev loss=2.4537\n",
            "3700/700000: training loss=2.4390, dev loss=2.4509\n",
            "3750/700000: training loss=2.4370, dev loss=2.4426\n",
            "3800/700000: training loss=2.4402, dev loss=2.4510\n",
            "3850/700000: training loss=2.4436, dev loss=2.4403\n",
            "3900/700000: training loss=2.4356, dev loss=2.4397\n",
            "3950/700000: training loss=2.4381, dev loss=2.4432\n",
            "4000/700000: training loss=2.4296, dev loss=2.4427\n",
            "4050/700000: training loss=2.4339, dev loss=2.4442\n",
            "4100/700000: training loss=2.4326, dev loss=2.4398\n",
            "4150/700000: training loss=2.4282, dev loss=2.4410\n",
            "4200/700000: training loss=2.4305, dev loss=2.4389\n",
            "4250/700000: training loss=2.4255, dev loss=2.4408\n",
            "4300/700000: training loss=2.4334, dev loss=2.4382\n",
            "4350/700000: training loss=2.4263, dev loss=2.4397\n",
            "4400/700000: training loss=2.4279, dev loss=2.4377\n",
            "4450/700000: training loss=2.4236, dev loss=2.4381\n",
            "4500/700000: training loss=2.4223, dev loss=2.4401\n",
            "4550/700000: training loss=2.4218, dev loss=2.4417\n",
            "4600/700000: training loss=2.4180, dev loss=2.4427\n",
            "4650/700000: training loss=2.4233, dev loss=2.4363\n",
            "4700/700000: training loss=2.4178, dev loss=2.4345\n",
            "4750/700000: training loss=2.4177, dev loss=2.4415\n",
            "4800/700000: training loss=2.4176, dev loss=2.4344\n",
            "4850/700000: training loss=2.4193, dev loss=2.4326\n",
            "4900/700000: training loss=2.4160, dev loss=2.4391\n",
            "4950/700000: training loss=2.4131, dev loss=2.4334\n",
            "5000/700000: training loss=2.4119, dev loss=2.4392\n",
            "5050/700000: training loss=2.4132, dev loss=2.4353\n",
            "5100/700000: training loss=2.4109, dev loss=2.4306\n",
            "5150/700000: training loss=2.4142, dev loss=2.4336\n",
            "5200/700000: training loss=2.4166, dev loss=2.4309\n",
            "5250/700000: training loss=2.4115, dev loss=2.4322\n",
            "5300/700000: training loss=2.4122, dev loss=2.4290\n",
            "5350/700000: training loss=2.4138, dev loss=2.4299\n",
            "5400/700000: training loss=2.4115, dev loss=2.4315\n",
            "5450/700000: training loss=2.4078, dev loss=2.4313\n",
            "5500/700000: training loss=2.4109, dev loss=2.4320\n",
            "5550/700000: training loss=2.4052, dev loss=2.4250\n",
            "5600/700000: training loss=2.4099, dev loss=2.4317\n",
            "5650/700000: training loss=2.4062, dev loss=2.4301\n",
            "5700/700000: training loss=2.4054, dev loss=2.4308\n",
            "5750/700000: training loss=2.4069, dev loss=2.4302\n",
            "5800/700000: training loss=2.4040, dev loss=2.4337\n",
            "5850/700000: training loss=2.4021, dev loss=2.4260\n",
            "5900/700000: training loss=2.3978, dev loss=2.4288\n",
            "5950/700000: training loss=2.4022, dev loss=2.4336\n",
            "6000/700000: training loss=2.4033, dev loss=2.4308\n",
            "6050/700000: training loss=2.4075, dev loss=2.4335\n",
            "6100/700000: training loss=2.4019, dev loss=2.4345\n",
            "6150/700000: training loss=2.4003, dev loss=2.4276\n",
            "6200/700000: training loss=2.4004, dev loss=2.4298\n",
            "6250/700000: training loss=2.3991, dev loss=2.4292\n",
            "6300/700000: training loss=2.4036, dev loss=2.4305\n",
            "6350/700000: training loss=2.3964, dev loss=2.4263\n",
            "6400/700000: training loss=2.3988, dev loss=2.4280\n",
            "6450/700000: training loss=2.3972, dev loss=2.4355\n",
            "6500/700000: training loss=2.3958, dev loss=2.4322\n",
            "6550/700000: training loss=2.3974, dev loss=2.4318\n",
            "6600/700000: training loss=2.3998, dev loss=2.4364\n",
            "6650/700000: training loss=2.3988, dev loss=2.4301\n",
            "6700/700000: training loss=2.3939, dev loss=2.4347\n",
            "6750/700000: training loss=2.3958, dev loss=2.4302\n",
            "6800/700000: training loss=2.3942, dev loss=2.4260\n",
            "6850/700000: training loss=2.3930, dev loss=2.4285\n",
            "6900/700000: training loss=2.3908, dev loss=2.4296\n",
            "6950/700000: training loss=2.3885, dev loss=2.4217\n",
            "7000/700000: training loss=2.3943, dev loss=2.4262\n",
            "7050/700000: training loss=2.3934, dev loss=2.4288\n",
            "7100/700000: training loss=2.3913, dev loss=2.4258\n",
            "7150/700000: training loss=2.3862, dev loss=2.4312\n",
            "7200/700000: training loss=2.3903, dev loss=2.4283\n",
            "7250/700000: training loss=2.3898, dev loss=2.4348\n",
            "7300/700000: training loss=2.3893, dev loss=2.4276\n",
            "7350/700000: training loss=2.3868, dev loss=2.4301\n",
            "7400/700000: training loss=2.3852, dev loss=2.4270\n",
            "7450/700000: training loss=2.3844, dev loss=2.4334\n",
            "7500/700000: training loss=2.3879, dev loss=2.4285\n",
            "7550/700000: training loss=2.3895, dev loss=2.4287\n",
            "7600/700000: training loss=2.3781, dev loss=2.4249\n",
            "7650/700000: training loss=2.3865, dev loss=2.4290\n",
            "7700/700000: training loss=2.3842, dev loss=2.4269\n",
            "7750/700000: training loss=2.3832, dev loss=2.4215\n",
            "7800/700000: training loss=2.3787, dev loss=2.4245\n",
            "7850/700000: training loss=2.3825, dev loss=2.4291\n",
            "7900/700000: training loss=2.3823, dev loss=2.4274\n",
            "7950/700000: training loss=2.3777, dev loss=2.4238\n",
            "8000/700000: training loss=2.3874, dev loss=2.4216\n",
            "8050/700000: training loss=2.3792, dev loss=2.4229\n",
            "8100/700000: training loss=2.3830, dev loss=2.4214\n",
            "8150/700000: training loss=2.3827, dev loss=2.4270\n",
            "8200/700000: training loss=2.3811, dev loss=2.4274\n",
            "8250/700000: training loss=2.3795, dev loss=2.4261\n",
            "8300/700000: training loss=2.3826, dev loss=2.4269\n",
            "8350/700000: training loss=2.3834, dev loss=2.4271\n",
            "8400/700000: training loss=2.3818, dev loss=2.4234\n",
            "8450/700000: training loss=2.3733, dev loss=2.4277\n",
            "8500/700000: training loss=2.3779, dev loss=2.4285\n",
            "8550/700000: training loss=2.3809, dev loss=2.4243\n",
            "8600/700000: training loss=2.3767, dev loss=2.4269\n",
            "8650/700000: training loss=2.3739, dev loss=2.4215\n",
            "8700/700000: training loss=2.3758, dev loss=2.4250\n",
            "8750/700000: training loss=2.3763, dev loss=2.4256\n",
            "8800/700000: training loss=2.3748, dev loss=2.4280\n",
            "8850/700000: training loss=2.3735, dev loss=2.4259\n",
            "8900/700000: training loss=2.3731, dev loss=2.4265\n",
            "8950/700000: training loss=2.3717, dev loss=2.4187\n",
            "9000/700000: training loss=2.3756, dev loss=2.4227\n",
            "9050/700000: training loss=2.3752, dev loss=2.4232\n",
            "9100/700000: training loss=2.3767, dev loss=2.4287\n",
            "9150/700000: training loss=2.3724, dev loss=2.4250\n",
            "9200/700000: training loss=2.3737, dev loss=2.4192\n",
            "9250/700000: training loss=2.3704, dev loss=2.4214\n",
            "9300/700000: training loss=2.3689, dev loss=2.4194\n",
            "9350/700000: training loss=2.3655, dev loss=2.4298\n",
            "9400/700000: training loss=2.3655, dev loss=2.4262\n",
            "9450/700000: training loss=2.3656, dev loss=2.4204\n",
            "9500/700000: training loss=2.3673, dev loss=2.4194\n",
            "9550/700000: training loss=2.3715, dev loss=2.4229\n",
            "9600/700000: training loss=2.3719, dev loss=2.4187\n",
            "9650/700000: training loss=2.3648, dev loss=2.4163\n",
            "9700/700000: training loss=2.3665, dev loss=2.4181\n",
            "9750/700000: training loss=2.3696, dev loss=2.4247\n",
            "9800/700000: training loss=2.3684, dev loss=2.4198\n",
            "9850/700000: training loss=2.3681, dev loss=2.4198\n",
            "9900/700000: training loss=2.3705, dev loss=2.4252\n",
            "9950/700000: training loss=2.3685, dev loss=2.4201\n",
            "10000/700000: training loss=2.3617, dev loss=2.4220\n",
            "10050/700000: training loss=2.3704, dev loss=2.4218\n",
            "10100/700000: training loss=2.3641, dev loss=2.4200\n",
            "10150/700000: training loss=2.3631, dev loss=2.4211\n",
            "10200/700000: training loss=2.3660, dev loss=2.4132\n",
            "10250/700000: training loss=2.3623, dev loss=2.4118\n",
            "10300/700000: training loss=2.3629, dev loss=2.4201\n",
            "10350/700000: training loss=2.3658, dev loss=2.4132\n",
            "10400/700000: training loss=2.3645, dev loss=2.4096\n",
            "10450/700000: training loss=2.3654, dev loss=2.4167\n",
            "10500/700000: training loss=2.3620, dev loss=2.4157\n",
            "10550/700000: training loss=2.3554, dev loss=2.4149\n",
            "10600/700000: training loss=2.3632, dev loss=2.4179\n",
            "10650/700000: training loss=2.3624, dev loss=2.4164\n",
            "10700/700000: training loss=2.3619, dev loss=2.4158\n",
            "10750/700000: training loss=2.3582, dev loss=2.4140\n",
            "10800/700000: training loss=2.3606, dev loss=2.4119\n",
            "10850/700000: training loss=2.3578, dev loss=2.4178\n",
            "10900/700000: training loss=2.3554, dev loss=2.4154\n",
            "10950/700000: training loss=2.3552, dev loss=2.4188\n",
            "11000/700000: training loss=2.3580, dev loss=2.4154\n",
            "11050/700000: training loss=2.3578, dev loss=2.4186\n",
            "11100/700000: training loss=2.3494, dev loss=2.4143\n",
            "11150/700000: training loss=2.3544, dev loss=2.4146\n",
            "11200/700000: training loss=2.3544, dev loss=2.4099\n",
            "11250/700000: training loss=2.3572, dev loss=2.4100\n",
            "11300/700000: training loss=2.3536, dev loss=2.4102\n",
            "11350/700000: training loss=2.3491, dev loss=2.4066\n",
            "11400/700000: training loss=2.3519, dev loss=2.4136\n",
            "11450/700000: training loss=2.3556, dev loss=2.4087\n",
            "11500/700000: training loss=2.3513, dev loss=2.4096\n",
            "11550/700000: training loss=2.3503, dev loss=2.4103\n",
            "11600/700000: training loss=2.3512, dev loss=2.4127\n",
            "11650/700000: training loss=2.3545, dev loss=2.4078\n",
            "11700/700000: training loss=2.3497, dev loss=2.4141\n",
            "11750/700000: training loss=2.3465, dev loss=2.4091\n",
            "11800/700000: training loss=2.3499, dev loss=2.4116\n",
            "11850/700000: training loss=2.3475, dev loss=2.4161\n",
            "11900/700000: training loss=2.3498, dev loss=2.4118\n",
            "11950/700000: training loss=2.3512, dev loss=2.4084\n",
            "12000/700000: training loss=2.3466, dev loss=2.4129\n",
            "12050/700000: training loss=2.3445, dev loss=2.4128\n",
            "12100/700000: training loss=2.3504, dev loss=2.4106\n",
            "12150/700000: training loss=2.3467, dev loss=2.4071\n",
            "12200/700000: training loss=2.3459, dev loss=2.4137\n",
            "12250/700000: training loss=2.3407, dev loss=2.4075\n",
            "12300/700000: training loss=2.3402, dev loss=2.4088\n",
            "12350/700000: training loss=2.3481, dev loss=2.4071\n",
            "12400/700000: training loss=2.3432, dev loss=2.4094\n",
            "12450/700000: training loss=2.3425, dev loss=2.4097\n",
            "12500/700000: training loss=2.3459, dev loss=2.4069\n",
            "12550/700000: training loss=2.3462, dev loss=2.4037\n",
            "12600/700000: training loss=2.3482, dev loss=2.4044\n",
            "12650/700000: training loss=2.3424, dev loss=2.4052\n",
            "12700/700000: training loss=2.3412, dev loss=2.4066\n",
            "12750/700000: training loss=2.3426, dev loss=2.4020\n",
            "12800/700000: training loss=2.3452, dev loss=2.4050\n",
            "12850/700000: training loss=2.3406, dev loss=2.4054\n",
            "12900/700000: training loss=2.3409, dev loss=2.4069\n",
            "12950/700000: training loss=2.3404, dev loss=2.4051\n",
            "13000/700000: training loss=2.3399, dev loss=2.4079\n",
            "13050/700000: training loss=2.3344, dev loss=2.4068\n",
            "13100/700000: training loss=2.3394, dev loss=2.4063\n",
            "13150/700000: training loss=2.3352, dev loss=2.4037\n",
            "13200/700000: training loss=2.3360, dev loss=2.4068\n",
            "13250/700000: training loss=2.3366, dev loss=2.4030\n",
            "13300/700000: training loss=2.3397, dev loss=2.3992\n",
            "13350/700000: training loss=2.3404, dev loss=2.4069\n",
            "13400/700000: training loss=2.3410, dev loss=2.4020\n",
            "13450/700000: training loss=2.3356, dev loss=2.3996\n",
            "13500/700000: training loss=2.3370, dev loss=2.3979\n",
            "13550/700000: training loss=2.3333, dev loss=2.4040\n",
            "13600/700000: training loss=2.3364, dev loss=2.3988\n",
            "13650/700000: training loss=2.3342, dev loss=2.3994\n",
            "13700/700000: training loss=2.3322, dev loss=2.4020\n",
            "13750/700000: training loss=2.3325, dev loss=2.4029\n",
            "13800/700000: training loss=2.3345, dev loss=2.4022\n",
            "13850/700000: training loss=2.3345, dev loss=2.4026\n",
            "13900/700000: training loss=2.3353, dev loss=2.4007\n",
            "13950/700000: training loss=2.3357, dev loss=2.3935\n",
            "14000/700000: training loss=2.3337, dev loss=2.3970\n",
            "14050/700000: training loss=2.3354, dev loss=2.3984\n",
            "14100/700000: training loss=2.3315, dev loss=2.3989\n",
            "14150/700000: training loss=2.3326, dev loss=2.3939\n",
            "14200/700000: training loss=2.3277, dev loss=2.3988\n",
            "14250/700000: training loss=2.3337, dev loss=2.3970\n",
            "14300/700000: training loss=2.3280, dev loss=2.4010\n",
            "14350/700000: training loss=2.3274, dev loss=2.3988\n",
            "14400/700000: training loss=2.3289, dev loss=2.3951\n",
            "14450/700000: training loss=2.3303, dev loss=2.3986\n",
            "14500/700000: training loss=2.3302, dev loss=2.3950\n",
            "14550/700000: training loss=2.3258, dev loss=2.3997\n",
            "14600/700000: training loss=2.3338, dev loss=2.3950\n",
            "14650/700000: training loss=2.3298, dev loss=2.3951\n",
            "14700/700000: training loss=2.3274, dev loss=2.3961\n",
            "14750/700000: training loss=2.3263, dev loss=2.3910\n",
            "14800/700000: training loss=2.3265, dev loss=2.3960\n",
            "14850/700000: training loss=2.3298, dev loss=2.3906\n",
            "14900/700000: training loss=2.3251, dev loss=2.3958\n",
            "14950/700000: training loss=2.3336, dev loss=2.3939\n",
            "15000/700000: training loss=2.3256, dev loss=2.3987\n",
            "15050/700000: training loss=2.3231, dev loss=2.3944\n",
            "15100/700000: training loss=2.3234, dev loss=2.3889\n",
            "15150/700000: training loss=2.3267, dev loss=2.3925\n",
            "15200/700000: training loss=2.3270, dev loss=2.3967\n",
            "15250/700000: training loss=2.3175, dev loss=2.3931\n",
            "15300/700000: training loss=2.3169, dev loss=2.3929\n",
            "15350/700000: training loss=2.3174, dev loss=2.3912\n",
            "15400/700000: training loss=2.3183, dev loss=2.3950\n",
            "15450/700000: training loss=2.3213, dev loss=2.3913\n",
            "15500/700000: training loss=2.3203, dev loss=2.3995\n",
            "15550/700000: training loss=2.3182, dev loss=2.3926\n",
            "15600/700000: training loss=2.3149, dev loss=2.3871\n",
            "15650/700000: training loss=2.3212, dev loss=2.3942\n",
            "15700/700000: training loss=2.3163, dev loss=2.3955\n",
            "15750/700000: training loss=2.3185, dev loss=2.3926\n",
            "15800/700000: training loss=2.3217, dev loss=2.3911\n",
            "15850/700000: training loss=2.3167, dev loss=2.3841\n",
            "15900/700000: training loss=2.3177, dev loss=2.3953\n",
            "15950/700000: training loss=2.3174, dev loss=2.3855\n",
            "16000/700000: training loss=2.3171, dev loss=2.3905\n",
            "16050/700000: training loss=2.3156, dev loss=2.3878\n",
            "16100/700000: training loss=2.3205, dev loss=2.3880\n",
            "16150/700000: training loss=2.3160, dev loss=2.3907\n",
            "16200/700000: training loss=2.3133, dev loss=2.3857\n",
            "16250/700000: training loss=2.3132, dev loss=2.3914\n",
            "16300/700000: training loss=2.3120, dev loss=2.3906\n",
            "16350/700000: training loss=2.3160, dev loss=2.3920\n",
            "16400/700000: training loss=2.3139, dev loss=2.3906\n",
            "16450/700000: training loss=2.3072, dev loss=2.3831\n",
            "16500/700000: training loss=2.3146, dev loss=2.3865\n",
            "16550/700000: training loss=2.3128, dev loss=2.3904\n",
            "16600/700000: training loss=2.3113, dev loss=2.3880\n",
            "16650/700000: training loss=2.3120, dev loss=2.3879\n",
            "16700/700000: training loss=2.3133, dev loss=2.3880\n",
            "16750/700000: training loss=2.3098, dev loss=2.3789\n",
            "16800/700000: training loss=2.3145, dev loss=2.3859\n",
            "16850/700000: training loss=2.3090, dev loss=2.3891\n",
            "16900/700000: training loss=2.3068, dev loss=2.3795\n",
            "16950/700000: training loss=2.3094, dev loss=2.3826\n",
            "17000/700000: training loss=2.3033, dev loss=2.3761\n",
            "17050/700000: training loss=2.3115, dev loss=2.3788\n",
            "17100/700000: training loss=2.3000, dev loss=2.3803\n",
            "17150/700000: training loss=2.3075, dev loss=2.3849\n",
            "17200/700000: training loss=2.3084, dev loss=2.3822\n",
            "17250/700000: training loss=2.3048, dev loss=2.3840\n",
            "17300/700000: training loss=2.3077, dev loss=2.3811\n",
            "17350/700000: training loss=2.3057, dev loss=2.3827\n",
            "17400/700000: training loss=2.3089, dev loss=2.3841\n",
            "17450/700000: training loss=2.3072, dev loss=2.3847\n",
            "17500/700000: training loss=2.3071, dev loss=2.3829\n",
            "17550/700000: training loss=2.3073, dev loss=2.3842\n",
            "17600/700000: training loss=2.3021, dev loss=2.3782\n",
            "17650/700000: training loss=2.3026, dev loss=2.3801\n",
            "17700/700000: training loss=2.3056, dev loss=2.3823\n",
            "17750/700000: training loss=2.2993, dev loss=2.3803\n",
            "17800/700000: training loss=2.3021, dev loss=2.3812\n",
            "17850/700000: training loss=2.3006, dev loss=2.3806\n",
            "17900/700000: training loss=2.2927, dev loss=2.3777\n",
            "17950/700000: training loss=2.2990, dev loss=2.3778\n",
            "18000/700000: training loss=2.3004, dev loss=2.3779\n",
            "18050/700000: training loss=2.2933, dev loss=2.3772\n",
            "18100/700000: training loss=2.2905, dev loss=2.3796\n",
            "18150/700000: training loss=2.2962, dev loss=2.3797\n",
            "18200/700000: training loss=2.3013, dev loss=2.3796\n",
            "18250/700000: training loss=2.2936, dev loss=2.3775\n",
            "18300/700000: training loss=2.2964, dev loss=2.3780\n",
            "18350/700000: training loss=2.2914, dev loss=2.3783\n",
            "18400/700000: training loss=2.2910, dev loss=2.3673\n",
            "18450/700000: training loss=2.2898, dev loss=2.3716\n",
            "18500/700000: training loss=2.2973, dev loss=2.3708\n",
            "18550/700000: training loss=2.2937, dev loss=2.3723\n",
            "18600/700000: training loss=2.2960, dev loss=2.3706\n",
            "18650/700000: training loss=2.2985, dev loss=2.3791\n",
            "18700/700000: training loss=2.2944, dev loss=2.3703\n",
            "18750/700000: training loss=2.2977, dev loss=2.3769\n",
            "18800/700000: training loss=2.2956, dev loss=2.3806\n",
            "18850/700000: training loss=2.2938, dev loss=2.3766\n",
            "18900/700000: training loss=2.2859, dev loss=2.3698\n",
            "18950/700000: training loss=2.2882, dev loss=2.3773\n",
            "19000/700000: training loss=2.2923, dev loss=2.3747\n",
            "19050/700000: training loss=2.2942, dev loss=2.3692\n",
            "19100/700000: training loss=2.2868, dev loss=2.3694\n",
            "19150/700000: training loss=2.2895, dev loss=2.3704\n",
            "19200/700000: training loss=2.2942, dev loss=2.3699\n",
            "19250/700000: training loss=2.2947, dev loss=2.3713\n",
            "19300/700000: training loss=2.2848, dev loss=2.3701\n",
            "19350/700000: training loss=2.2903, dev loss=2.3712\n",
            "19400/700000: training loss=2.2921, dev loss=2.3672\n",
            "19450/700000: training loss=2.2917, dev loss=2.3625\n",
            "19500/700000: training loss=2.2910, dev loss=2.3675\n",
            "19550/700000: training loss=2.2905, dev loss=2.3657\n",
            "19600/700000: training loss=2.2858, dev loss=2.3700\n",
            "19650/700000: training loss=2.2906, dev loss=2.3650\n",
            "19700/700000: training loss=2.2817, dev loss=2.3659\n",
            "19750/700000: training loss=2.2854, dev loss=2.3656\n",
            "19800/700000: training loss=2.2869, dev loss=2.3705\n",
            "19850/700000: training loss=2.2810, dev loss=2.3644\n",
            "19900/700000: training loss=2.2860, dev loss=2.3663\n",
            "19950/700000: training loss=2.2825, dev loss=2.3632\n",
            "20000/700000: training loss=2.2803, dev loss=2.3594\n",
            "20050/700000: training loss=2.2889, dev loss=2.3669\n",
            "20100/700000: training loss=2.2832, dev loss=2.3613\n",
            "20150/700000: training loss=2.2803, dev loss=2.3626\n",
            "20200/700000: training loss=2.2808, dev loss=2.3633\n",
            "20250/700000: training loss=2.2828, dev loss=2.3648\n",
            "20300/700000: training loss=2.2798, dev loss=2.3644\n",
            "20350/700000: training loss=2.2801, dev loss=2.3584\n",
            "20400/700000: training loss=2.2845, dev loss=2.3660\n",
            "20450/700000: training loss=2.2816, dev loss=2.3611\n",
            "20500/700000: training loss=2.2776, dev loss=2.3641\n",
            "20550/700000: training loss=2.2752, dev loss=2.3693\n",
            "20600/700000: training loss=2.2860, dev loss=2.3652\n",
            "20650/700000: training loss=2.2765, dev loss=2.3614\n",
            "20700/700000: training loss=2.2729, dev loss=2.3580\n",
            "20750/700000: training loss=2.2732, dev loss=2.3634\n",
            "20800/700000: training loss=2.2744, dev loss=2.3660\n",
            "20850/700000: training loss=2.2786, dev loss=2.3590\n",
            "20900/700000: training loss=2.2778, dev loss=2.3601\n",
            "20950/700000: training loss=2.2779, dev loss=2.3599\n",
            "21000/700000: training loss=2.2769, dev loss=2.3652\n",
            "21050/700000: training loss=2.2777, dev loss=2.3603\n",
            "21100/700000: training loss=2.2834, dev loss=2.3632\n",
            "21150/700000: training loss=2.2755, dev loss=2.3565\n",
            "21200/700000: training loss=2.2827, dev loss=2.3627\n",
            "21250/700000: training loss=2.2777, dev loss=2.3586\n",
            "21300/700000: training loss=2.2773, dev loss=2.3609\n",
            "21350/700000: training loss=2.2765, dev loss=2.3617\n",
            "21400/700000: training loss=2.2744, dev loss=2.3618\n",
            "21450/700000: training loss=2.2743, dev loss=2.3540\n",
            "21500/700000: training loss=2.2677, dev loss=2.3590\n",
            "21550/700000: training loss=2.2812, dev loss=2.3592\n",
            "21600/700000: training loss=2.2664, dev loss=2.3591\n",
            "21650/700000: training loss=2.2737, dev loss=2.3589\n",
            "21700/700000: training loss=2.2732, dev loss=2.3587\n",
            "21750/700000: training loss=2.2712, dev loss=2.3497\n",
            "21800/700000: training loss=2.2704, dev loss=2.3575\n",
            "21850/700000: training loss=2.2721, dev loss=2.3574\n",
            "21900/700000: training loss=2.2708, dev loss=2.3537\n",
            "21950/700000: training loss=2.2665, dev loss=2.3542\n",
            "22000/700000: training loss=2.2707, dev loss=2.3563\n",
            "22050/700000: training loss=2.2692, dev loss=2.3567\n",
            "22100/700000: training loss=2.2696, dev loss=2.3533\n",
            "22150/700000: training loss=2.2566, dev loss=2.3547\n",
            "22200/700000: training loss=2.2669, dev loss=2.3540\n",
            "22250/700000: training loss=2.2672, dev loss=2.3519\n",
            "22300/700000: training loss=2.2661, dev loss=2.3537\n",
            "22350/700000: training loss=2.2692, dev loss=2.3539\n",
            "22400/700000: training loss=2.2733, dev loss=2.3511\n",
            "22450/700000: training loss=2.2696, dev loss=2.3516\n",
            "22500/700000: training loss=2.2639, dev loss=2.3502\n",
            "22550/700000: training loss=2.2640, dev loss=2.3548\n",
            "22600/700000: training loss=2.2676, dev loss=2.3477\n",
            "22650/700000: training loss=2.2669, dev loss=2.3505\n",
            "22700/700000: training loss=2.2630, dev loss=2.3472\n",
            "22750/700000: training loss=2.2659, dev loss=2.3473\n",
            "22800/700000: training loss=2.2584, dev loss=2.3438\n",
            "22850/700000: training loss=2.2629, dev loss=2.3472\n",
            "22900/700000: training loss=2.2561, dev loss=2.3441\n",
            "22950/700000: training loss=2.2627, dev loss=2.3483\n",
            "23000/700000: training loss=2.2618, dev loss=2.3445\n",
            "23050/700000: training loss=2.2633, dev loss=2.3468\n",
            "23100/700000: training loss=2.2596, dev loss=2.3504\n",
            "23150/700000: training loss=2.2649, dev loss=2.3449\n",
            "23200/700000: training loss=2.2621, dev loss=2.3469\n",
            "23250/700000: training loss=2.2604, dev loss=2.3492\n",
            "23300/700000: training loss=2.2614, dev loss=2.3405\n",
            "23350/700000: training loss=2.2620, dev loss=2.3431\n",
            "23400/700000: training loss=2.2630, dev loss=2.3402\n",
            "23450/700000: training loss=2.2542, dev loss=2.3411\n",
            "23500/700000: training loss=2.2569, dev loss=2.3456\n",
            "23550/700000: training loss=2.2660, dev loss=2.3465\n",
            "23600/700000: training loss=2.2572, dev loss=2.3487\n",
            "23650/700000: training loss=2.2524, dev loss=2.3400\n",
            "23700/700000: training loss=2.2573, dev loss=2.3407\n",
            "23750/700000: training loss=2.2465, dev loss=2.3455\n",
            "23800/700000: training loss=2.2522, dev loss=2.3425\n",
            "23850/700000: training loss=2.2540, dev loss=2.3399\n",
            "23900/700000: training loss=2.2537, dev loss=2.3414\n",
            "23950/700000: training loss=2.2497, dev loss=2.3402\n",
            "24000/700000: training loss=2.2522, dev loss=2.3412\n",
            "24050/700000: training loss=2.2484, dev loss=2.3434\n",
            "24100/700000: training loss=2.2493, dev loss=2.3430\n",
            "24150/700000: training loss=2.2468, dev loss=2.3338\n",
            "24200/700000: training loss=2.2468, dev loss=2.3403\n",
            "24250/700000: training loss=2.2427, dev loss=2.3351\n",
            "24300/700000: training loss=2.2512, dev loss=2.3393\n",
            "24350/700000: training loss=2.2501, dev loss=2.3343\n",
            "24400/700000: training loss=2.2501, dev loss=2.3425\n",
            "24450/700000: training loss=2.2443, dev loss=2.3352\n",
            "24500/700000: training loss=2.2478, dev loss=2.3409\n",
            "24550/700000: training loss=2.2486, dev loss=2.3385\n",
            "24600/700000: training loss=2.2430, dev loss=2.3337\n",
            "24650/700000: training loss=2.2455, dev loss=2.3360\n",
            "24700/700000: training loss=2.2489, dev loss=2.3311\n",
            "24750/700000: training loss=2.2498, dev loss=2.3420\n",
            "24800/700000: training loss=2.2383, dev loss=2.3345\n",
            "24850/700000: training loss=2.2460, dev loss=2.3332\n",
            "24900/700000: training loss=2.2402, dev loss=2.3371\n",
            "24950/700000: training loss=2.2457, dev loss=2.3383\n",
            "25000/700000: training loss=2.2438, dev loss=2.3414\n",
            "25050/700000: training loss=2.2403, dev loss=2.3303\n",
            "25100/700000: training loss=2.2521, dev loss=2.3348\n",
            "25150/700000: training loss=2.2386, dev loss=2.3333\n",
            "25200/700000: training loss=2.2405, dev loss=2.3296\n",
            "25250/700000: training loss=2.2467, dev loss=2.3393\n",
            "25300/700000: training loss=2.2412, dev loss=2.3342\n",
            "25350/700000: training loss=2.2368, dev loss=2.3377\n",
            "25400/700000: training loss=2.2362, dev loss=2.3296\n",
            "25450/700000: training loss=2.2467, dev loss=2.3324\n",
            "25500/700000: training loss=2.2379, dev loss=2.3300\n",
            "25550/700000: training loss=2.2429, dev loss=2.3295\n",
            "25600/700000: training loss=2.2389, dev loss=2.3347\n",
            "25650/700000: training loss=2.2438, dev loss=2.3335\n",
            "25700/700000: training loss=2.2311, dev loss=2.3294\n",
            "25750/700000: training loss=2.2393, dev loss=2.3224\n",
            "25800/700000: training loss=2.2311, dev loss=2.3269\n",
            "25850/700000: training loss=2.2400, dev loss=2.3264\n",
            "25900/700000: training loss=2.2365, dev loss=2.3201\n",
            "25950/700000: training loss=2.2338, dev loss=2.3246\n",
            "26000/700000: training loss=2.2357, dev loss=2.3263\n",
            "26050/700000: training loss=2.2312, dev loss=2.3347\n",
            "26100/700000: training loss=2.2347, dev loss=2.3301\n",
            "26150/700000: training loss=2.2313, dev loss=2.3271\n",
            "26200/700000: training loss=2.2262, dev loss=2.3224\n",
            "26250/700000: training loss=2.2299, dev loss=2.3220\n",
            "26300/700000: training loss=2.2259, dev loss=2.3265\n",
            "26350/700000: training loss=2.2353, dev loss=2.3200\n",
            "26400/700000: training loss=2.2327, dev loss=2.3259\n",
            "26450/700000: training loss=2.2277, dev loss=2.3213\n",
            "26500/700000: training loss=2.2300, dev loss=2.3256\n",
            "26550/700000: training loss=2.2223, dev loss=2.3158\n",
            "26600/700000: training loss=2.2223, dev loss=2.3171\n",
            "26650/700000: training loss=2.2228, dev loss=2.3193\n",
            "26700/700000: training loss=2.2298, dev loss=2.3223\n",
            "26750/700000: training loss=2.2264, dev loss=2.3249\n",
            "26800/700000: training loss=2.2295, dev loss=2.3219\n",
            "26850/700000: training loss=2.2277, dev loss=2.3245\n",
            "26900/700000: training loss=2.2277, dev loss=2.3173\n",
            "26950/700000: training loss=2.2293, dev loss=2.3188\n",
            "27000/700000: training loss=2.2237, dev loss=2.3218\n",
            "27050/700000: training loss=2.2215, dev loss=2.3124\n",
            "27100/700000: training loss=2.2210, dev loss=2.3133\n",
            "27150/700000: training loss=2.2229, dev loss=2.3219\n",
            "27200/700000: training loss=2.2181, dev loss=2.3158\n",
            "27250/700000: training loss=2.2202, dev loss=2.3157\n",
            "27300/700000: training loss=2.2154, dev loss=2.3113\n",
            "27350/700000: training loss=2.2199, dev loss=2.3126\n",
            "27400/700000: training loss=2.2248, dev loss=2.3242\n",
            "27450/700000: training loss=2.2210, dev loss=2.3141\n",
            "27500/700000: training loss=2.2152, dev loss=2.3168\n",
            "27550/700000: training loss=2.2167, dev loss=2.3156\n",
            "27600/700000: training loss=2.2139, dev loss=2.3138\n",
            "27650/700000: training loss=2.2200, dev loss=2.3114\n",
            "27700/700000: training loss=2.2211, dev loss=2.3118\n",
            "27750/700000: training loss=2.2150, dev loss=2.3159\n",
            "27800/700000: training loss=2.2115, dev loss=2.3140\n",
            "27850/700000: training loss=2.2101, dev loss=2.3169\n",
            "27900/700000: training loss=2.2156, dev loss=2.3042\n",
            "27950/700000: training loss=2.2176, dev loss=2.3041\n",
            "28000/700000: training loss=2.2132, dev loss=2.3161\n",
            "28050/700000: training loss=2.2127, dev loss=2.3119\n",
            "28100/700000: training loss=2.2072, dev loss=2.3127\n",
            "28150/700000: training loss=2.2097, dev loss=2.3091\n",
            "28200/700000: training loss=2.2138, dev loss=2.3157\n",
            "28250/700000: training loss=2.2089, dev loss=2.3041\n",
            "28300/700000: training loss=2.2141, dev loss=2.3131\n",
            "28350/700000: training loss=2.2082, dev loss=2.3107\n",
            "28400/700000: training loss=2.2113, dev loss=2.3057\n",
            "28450/700000: training loss=2.2088, dev loss=2.3082\n",
            "28500/700000: training loss=2.2065, dev loss=2.3083\n",
            "28550/700000: training loss=2.1991, dev loss=2.3056\n",
            "28600/700000: training loss=2.2036, dev loss=2.3069\n",
            "28650/700000: training loss=2.2060, dev loss=2.3096\n",
            "28700/700000: training loss=2.2075, dev loss=2.3083\n",
            "28750/700000: training loss=2.2072, dev loss=2.3014\n",
            "28800/700000: training loss=2.2051, dev loss=2.3109\n",
            "28850/700000: training loss=2.2082, dev loss=2.3087\n",
            "28900/700000: training loss=2.1965, dev loss=2.2983\n",
            "28950/700000: training loss=2.2027, dev loss=2.3040\n",
            "29000/700000: training loss=2.1981, dev loss=2.3024\n",
            "29050/700000: training loss=2.1976, dev loss=2.2967\n",
            "29100/700000: training loss=2.2027, dev loss=2.3012\n",
            "29150/700000: training loss=2.2047, dev loss=2.3067\n",
            "29200/700000: training loss=2.2055, dev loss=2.2963\n",
            "29250/700000: training loss=2.2049, dev loss=2.3000\n",
            "29300/700000: training loss=2.1966, dev loss=2.2994\n",
            "29350/700000: training loss=2.2052, dev loss=2.3026\n",
            "29400/700000: training loss=2.1999, dev loss=2.2991\n",
            "29450/700000: training loss=2.1938, dev loss=2.2951\n",
            "29500/700000: training loss=2.2020, dev loss=2.3086\n",
            "29550/700000: training loss=2.1959, dev loss=2.2969\n",
            "29600/700000: training loss=2.1927, dev loss=2.2998\n",
            "29650/700000: training loss=2.1914, dev loss=2.3006\n",
            "29700/700000: training loss=2.1901, dev loss=2.2999\n",
            "29750/700000: training loss=2.1981, dev loss=2.2973\n",
            "29800/700000: training loss=2.1886, dev loss=2.2978\n",
            "29850/700000: training loss=2.1919, dev loss=2.3012\n",
            "29900/700000: training loss=2.1893, dev loss=2.2942\n",
            "29950/700000: training loss=2.1960, dev loss=2.2958\n",
            "30000/700000: training loss=2.1888, dev loss=2.2964\n",
            "30050/700000: training loss=2.1905, dev loss=2.2947\n",
            "30100/700000: training loss=2.1924, dev loss=2.2974\n",
            "30150/700000: training loss=2.1957, dev loss=2.3028\n",
            "30200/700000: training loss=2.1841, dev loss=2.2903\n",
            "30250/700000: training loss=2.1914, dev loss=2.2928\n",
            "30300/700000: training loss=2.1938, dev loss=2.2913\n",
            "30350/700000: training loss=2.1884, dev loss=2.2981\n",
            "30400/700000: training loss=2.1947, dev loss=2.2990\n",
            "30450/700000: training loss=2.1873, dev loss=2.2960\n",
            "30500/700000: training loss=2.1770, dev loss=2.2923\n",
            "30550/700000: training loss=2.1915, dev loss=2.2891\n",
            "30600/700000: training loss=2.1854, dev loss=2.2908\n",
            "30650/700000: training loss=2.1833, dev loss=2.2911\n",
            "30700/700000: training loss=2.1898, dev loss=2.2927\n",
            "30750/700000: training loss=2.1883, dev loss=2.2953\n",
            "30800/700000: training loss=2.1839, dev loss=2.2851\n",
            "30850/700000: training loss=2.1878, dev loss=2.2865\n",
            "30900/700000: training loss=2.1797, dev loss=2.2834\n",
            "30950/700000: training loss=2.1801, dev loss=2.2962\n",
            "31000/700000: training loss=2.1799, dev loss=2.2856\n",
            "31050/700000: training loss=2.1863, dev loss=2.2884\n",
            "31100/700000: training loss=2.1789, dev loss=2.2878\n",
            "31150/700000: training loss=2.1793, dev loss=2.2877\n",
            "31200/700000: training loss=2.1810, dev loss=2.2885\n",
            "31250/700000: training loss=2.1827, dev loss=2.2905\n",
            "31300/700000: training loss=2.1770, dev loss=2.2882\n",
            "31350/700000: training loss=2.1798, dev loss=2.2893\n",
            "31400/700000: training loss=2.1694, dev loss=2.2824\n",
            "31450/700000: training loss=2.1767, dev loss=2.2837\n",
            "31500/700000: training loss=2.1787, dev loss=2.2801\n",
            "31550/700000: training loss=2.1751, dev loss=2.2929\n",
            "31600/700000: training loss=2.1763, dev loss=2.2872\n",
            "31650/700000: training loss=2.1752, dev loss=2.2716\n",
            "31700/700000: training loss=2.1717, dev loss=2.2807\n",
            "31750/700000: training loss=2.1625, dev loss=2.2786\n",
            "31800/700000: training loss=2.1662, dev loss=2.2860\n",
            "31850/700000: training loss=2.1762, dev loss=2.2864\n",
            "31900/700000: training loss=2.1737, dev loss=2.2837\n",
            "31950/700000: training loss=2.1660, dev loss=2.2758\n",
            "32000/700000: training loss=2.1672, dev loss=2.2777\n",
            "32050/700000: training loss=2.1634, dev loss=2.2787\n",
            "32100/700000: training loss=2.1664, dev loss=2.2817\n",
            "32150/700000: training loss=2.1686, dev loss=2.2818\n",
            "32200/700000: training loss=2.1704, dev loss=2.2809\n",
            "32250/700000: training loss=2.1634, dev loss=2.2717\n",
            "32300/700000: training loss=2.1591, dev loss=2.2723\n",
            "32350/700000: training loss=2.1612, dev loss=2.2745\n",
            "32400/700000: training loss=2.1655, dev loss=2.2742\n",
            "32450/700000: training loss=2.1616, dev loss=2.2746\n",
            "32500/700000: training loss=2.1711, dev loss=2.2746\n",
            "32550/700000: training loss=2.1584, dev loss=2.2682\n",
            "32600/700000: training loss=2.1639, dev loss=2.2725\n",
            "32650/700000: training loss=2.1550, dev loss=2.2697\n",
            "32700/700000: training loss=2.1626, dev loss=2.2737\n",
            "32750/700000: training loss=2.1653, dev loss=2.2701\n",
            "32800/700000: training loss=2.1656, dev loss=2.2779\n",
            "32850/700000: training loss=2.1566, dev loss=2.2643\n",
            "32900/700000: training loss=2.1561, dev loss=2.2666\n",
            "32950/700000: training loss=2.1515, dev loss=2.2693\n",
            "33000/700000: training loss=2.1576, dev loss=2.2669\n",
            "33050/700000: training loss=2.1466, dev loss=2.2664\n",
            "33100/700000: training loss=2.1537, dev loss=2.2616\n",
            "33150/700000: training loss=2.1530, dev loss=2.2703\n",
            "33200/700000: training loss=2.1476, dev loss=2.2610\n",
            "33250/700000: training loss=2.1439, dev loss=2.2591\n",
            "33300/700000: training loss=2.1485, dev loss=2.2665\n",
            "33350/700000: training loss=2.1492, dev loss=2.2545\n",
            "33400/700000: training loss=2.1453, dev loss=2.2611\n",
            "33450/700000: training loss=2.1455, dev loss=2.2601\n",
            "33500/700000: training loss=2.1413, dev loss=2.2608\n",
            "33550/700000: training loss=2.1499, dev loss=2.2595\n",
            "33600/700000: training loss=2.1434, dev loss=2.2570\n",
            "33650/700000: training loss=2.1375, dev loss=2.2612\n",
            "33700/700000: training loss=2.1484, dev loss=2.2621\n",
            "33750/700000: training loss=2.1465, dev loss=2.2600\n",
            "33800/700000: training loss=2.1473, dev loss=2.2607\n",
            "33850/700000: training loss=2.1451, dev loss=2.2534\n",
            "33900/700000: training loss=2.1369, dev loss=2.2514\n",
            "33950/700000: training loss=2.1326, dev loss=2.2477\n",
            "34000/700000: training loss=2.1377, dev loss=2.2632\n",
            "34050/700000: training loss=2.1300, dev loss=2.2546\n",
            "34100/700000: training loss=2.1386, dev loss=2.2562\n",
            "34150/700000: training loss=2.1439, dev loss=2.2528\n",
            "34200/700000: training loss=2.1385, dev loss=2.2481\n",
            "34250/700000: training loss=2.1370, dev loss=2.2532\n",
            "34300/700000: training loss=2.1472, dev loss=2.2494\n",
            "34350/700000: training loss=2.1376, dev loss=2.2396\n",
            "34400/700000: training loss=2.1298, dev loss=2.2468\n",
            "34450/700000: training loss=2.1295, dev loss=2.2448\n",
            "34500/700000: training loss=2.1254, dev loss=2.2459\n",
            "34550/700000: training loss=2.1360, dev loss=2.2522\n",
            "34600/700000: training loss=2.1289, dev loss=2.2414\n",
            "34650/700000: training loss=2.1330, dev loss=2.2472\n",
            "34700/700000: training loss=2.1279, dev loss=2.2458\n",
            "34750/700000: training loss=2.1194, dev loss=2.2404\n",
            "34800/700000: training loss=2.1234, dev loss=2.2396\n",
            "34850/700000: training loss=2.1267, dev loss=2.2465\n",
            "34900/700000: training loss=2.1322, dev loss=2.2525\n",
            "34950/700000: training loss=2.1267, dev loss=2.2438\n",
            "35000/700000: training loss=2.1175, dev loss=2.2356\n",
            "35050/700000: training loss=2.1181, dev loss=2.2386\n",
            "35100/700000: training loss=2.1227, dev loss=2.2376\n",
            "35150/700000: training loss=2.1304, dev loss=2.2425\n",
            "35200/700000: training loss=2.1218, dev loss=2.2412\n",
            "35250/700000: training loss=2.1130, dev loss=2.2385\n",
            "35300/700000: training loss=2.1192, dev loss=2.2425\n",
            "35350/700000: training loss=2.1200, dev loss=2.2385\n",
            "35400/700000: training loss=2.1211, dev loss=2.2381\n",
            "35450/700000: training loss=2.1120, dev loss=2.2342\n",
            "35500/700000: training loss=2.1149, dev loss=2.2475\n",
            "35550/700000: training loss=2.1210, dev loss=2.2302\n",
            "35600/700000: training loss=2.1199, dev loss=2.2322\n",
            "35650/700000: training loss=2.1178, dev loss=2.2324\n",
            "35700/700000: training loss=2.1083, dev loss=2.2263\n",
            "35750/700000: training loss=2.1080, dev loss=2.2331\n",
            "35800/700000: training loss=2.1006, dev loss=2.2310\n",
            "35850/700000: training loss=2.1087, dev loss=2.2310\n",
            "35900/700000: training loss=2.1092, dev loss=2.2346\n",
            "35950/700000: training loss=2.1123, dev loss=2.2280\n",
            "36000/700000: training loss=2.1106, dev loss=2.2257\n",
            "36050/700000: training loss=2.1082, dev loss=2.2322\n",
            "36100/700000: training loss=2.1044, dev loss=2.2250\n",
            "36150/700000: training loss=2.1068, dev loss=2.2201\n",
            "36200/700000: training loss=2.1055, dev loss=2.2265\n",
            "36250/700000: training loss=2.1053, dev loss=2.2318\n",
            "36300/700000: training loss=2.1081, dev loss=2.2283\n",
            "36350/700000: training loss=2.1049, dev loss=2.2250\n",
            "36400/700000: training loss=2.1088, dev loss=2.2252\n",
            "36450/700000: training loss=2.1090, dev loss=2.2233\n",
            "36500/700000: training loss=2.0966, dev loss=2.2205\n",
            "36550/700000: training loss=2.1103, dev loss=2.2259\n",
            "36600/700000: training loss=2.0907, dev loss=2.2186\n",
            "36650/700000: training loss=2.1034, dev loss=2.2223\n",
            "36700/700000: training loss=2.0902, dev loss=2.2219\n",
            "36750/700000: training loss=2.0999, dev loss=2.2227\n",
            "36800/700000: training loss=2.1029, dev loss=2.2196\n",
            "36850/700000: training loss=2.0916, dev loss=2.2188\n",
            "36900/700000: training loss=2.0982, dev loss=2.2143\n",
            "36950/700000: training loss=2.0962, dev loss=2.2200\n",
            "37000/700000: training loss=2.0893, dev loss=2.2263\n",
            "37050/700000: training loss=2.0979, dev loss=2.2126\n",
            "37100/700000: training loss=2.0887, dev loss=2.2190\n",
            "37150/700000: training loss=2.0902, dev loss=2.2134\n",
            "37200/700000: training loss=2.0960, dev loss=2.2128\n",
            "37250/700000: training loss=2.0917, dev loss=2.2075\n",
            "37300/700000: training loss=2.0846, dev loss=2.2096\n",
            "37350/700000: training loss=2.0832, dev loss=2.2076\n",
            "37400/700000: training loss=2.0924, dev loss=2.2145\n",
            "37450/700000: training loss=2.0944, dev loss=2.2123\n",
            "37500/700000: training loss=2.0892, dev loss=2.2135\n",
            "37550/700000: training loss=2.0874, dev loss=2.2111\n",
            "37600/700000: training loss=2.0824, dev loss=2.2179\n",
            "37650/700000: training loss=2.0840, dev loss=2.2081\n",
            "37700/700000: training loss=2.0788, dev loss=2.1996\n",
            "37750/700000: training loss=2.0826, dev loss=2.2078\n",
            "37800/700000: training loss=2.0831, dev loss=2.2124\n",
            "37850/700000: training loss=2.0831, dev loss=2.2033\n",
            "37900/700000: training loss=2.0867, dev loss=2.2099\n",
            "37950/700000: training loss=2.0800, dev loss=2.2074\n",
            "38000/700000: training loss=2.0850, dev loss=2.1938\n",
            "38050/700000: training loss=2.0826, dev loss=2.2004\n",
            "38100/700000: training loss=2.0868, dev loss=2.2007\n",
            "38150/700000: training loss=2.0793, dev loss=2.2055\n",
            "38200/700000: training loss=2.0757, dev loss=2.2038\n",
            "38250/700000: training loss=2.0721, dev loss=2.2015\n",
            "38300/700000: training loss=2.0812, dev loss=2.2061\n",
            "38350/700000: training loss=2.0774, dev loss=2.2001\n",
            "38400/700000: training loss=2.0716, dev loss=2.2048\n",
            "38450/700000: training loss=2.0785, dev loss=2.2023\n",
            "38500/700000: training loss=2.0677, dev loss=2.1987\n",
            "38550/700000: training loss=2.0710, dev loss=2.2037\n",
            "38600/700000: training loss=2.0612, dev loss=2.1881\n",
            "38650/700000: training loss=2.0621, dev loss=2.1987\n",
            "38700/700000: training loss=2.0678, dev loss=2.1914\n",
            "38750/700000: training loss=2.0565, dev loss=2.1872\n",
            "38800/700000: training loss=2.0644, dev loss=2.1922\n",
            "38850/700000: training loss=2.0608, dev loss=2.1861\n",
            "38900/700000: training loss=2.0632, dev loss=2.1886\n",
            "38950/700000: training loss=2.0639, dev loss=2.1999\n",
            "39000/700000: training loss=2.0622, dev loss=2.1847\n",
            "39050/700000: training loss=2.0604, dev loss=2.1863\n",
            "39100/700000: training loss=2.0550, dev loss=2.1875\n",
            "39150/700000: training loss=2.0613, dev loss=2.1952\n",
            "39200/700000: training loss=2.0492, dev loss=2.1870\n",
            "39250/700000: training loss=2.0539, dev loss=2.1894\n",
            "39300/700000: training loss=2.0546, dev loss=2.1859\n",
            "39350/700000: training loss=2.0558, dev loss=2.1813\n",
            "39400/700000: training loss=2.0522, dev loss=2.1806\n",
            "39450/700000: training loss=2.0563, dev loss=2.1844\n",
            "39500/700000: training loss=2.0525, dev loss=2.1861\n",
            "39550/700000: training loss=2.0487, dev loss=2.1771\n",
            "39600/700000: training loss=2.0500, dev loss=2.1747\n",
            "39650/700000: training loss=2.0553, dev loss=2.1843\n",
            "39700/700000: training loss=2.0624, dev loss=2.1859\n",
            "39750/700000: training loss=2.0392, dev loss=2.1742\n",
            "39800/700000: training loss=2.0517, dev loss=2.1782\n",
            "39850/700000: training loss=2.0509, dev loss=2.1867\n",
            "39900/700000: training loss=2.0490, dev loss=2.1766\n",
            "39950/700000: training loss=2.0412, dev loss=2.1732\n",
            "40000/700000: training loss=2.0394, dev loss=2.1785\n",
            "40050/700000: training loss=2.0454, dev loss=2.1764\n",
            "40100/700000: training loss=2.0412, dev loss=2.1763\n",
            "40150/700000: training loss=2.0300, dev loss=2.1652\n",
            "40200/700000: training loss=2.0373, dev loss=2.1708\n",
            "40250/700000: training loss=2.0380, dev loss=2.1735\n",
            "40300/700000: training loss=2.0295, dev loss=2.1679\n",
            "40350/700000: training loss=2.0352, dev loss=2.1711\n",
            "40400/700000: training loss=2.0314, dev loss=2.1662\n",
            "40450/700000: training loss=2.0374, dev loss=2.1768\n",
            "40500/700000: training loss=2.0305, dev loss=2.1667\n",
            "40550/700000: training loss=2.0358, dev loss=2.1666\n",
            "40600/700000: training loss=2.0407, dev loss=2.1789\n",
            "40650/700000: training loss=2.0313, dev loss=2.1566\n",
            "40700/700000: training loss=2.0289, dev loss=2.1672\n",
            "40750/700000: training loss=2.0319, dev loss=2.1744\n",
            "40800/700000: training loss=2.0271, dev loss=2.1638\n",
            "40850/700000: training loss=2.0308, dev loss=2.1673\n",
            "40900/700000: training loss=2.0281, dev loss=2.1613\n",
            "40950/700000: training loss=2.0276, dev loss=2.1670\n",
            "41000/700000: training loss=2.0350, dev loss=2.1759\n",
            "41050/700000: training loss=2.0228, dev loss=2.1593\n",
            "41100/700000: training loss=2.0252, dev loss=2.1674\n",
            "41150/700000: training loss=2.0313, dev loss=2.1581\n",
            "41200/700000: training loss=2.0228, dev loss=2.1504\n",
            "41250/700000: training loss=2.0290, dev loss=2.1591\n",
            "41300/700000: training loss=2.0208, dev loss=2.1610\n",
            "41350/700000: training loss=2.0151, dev loss=2.1596\n",
            "41400/700000: training loss=2.0212, dev loss=2.1546\n",
            "41450/700000: training loss=2.0234, dev loss=2.1673\n",
            "41500/700000: training loss=2.0128, dev loss=2.1526\n",
            "41550/700000: training loss=2.0201, dev loss=2.1510\n",
            "41600/700000: training loss=2.0122, dev loss=2.1522\n",
            "41650/700000: training loss=2.0180, dev loss=2.1525\n",
            "41700/700000: training loss=2.0253, dev loss=2.1598\n",
            "41750/700000: training loss=2.0237, dev loss=2.1691\n",
            "41800/700000: training loss=2.0195, dev loss=2.1603\n",
            "41850/700000: training loss=2.0142, dev loss=2.1553\n",
            "41900/700000: training loss=2.0092, dev loss=2.1501\n",
            "41950/700000: training loss=2.0143, dev loss=2.1471\n",
            "42000/700000: training loss=2.0160, dev loss=2.1551\n",
            "42050/700000: training loss=2.0088, dev loss=2.1491\n",
            "42100/700000: training loss=2.0138, dev loss=2.1507\n",
            "42150/700000: training loss=1.9973, dev loss=2.1386\n",
            "42200/700000: training loss=2.0020, dev loss=2.1432\n",
            "42250/700000: training loss=2.0087, dev loss=2.1481\n",
            "42300/700000: training loss=2.0062, dev loss=2.1467\n",
            "42350/700000: training loss=2.0095, dev loss=2.1355\n",
            "42400/700000: training loss=2.0005, dev loss=2.1457\n",
            "42450/700000: training loss=2.0061, dev loss=2.1480\n",
            "42500/700000: training loss=2.0038, dev loss=2.1469\n",
            "42550/700000: training loss=2.0001, dev loss=2.1375\n",
            "42600/700000: training loss=2.0053, dev loss=2.1421\n",
            "42650/700000: training loss=1.9993, dev loss=2.1353\n",
            "42700/700000: training loss=2.0030, dev loss=2.1436\n",
            "42750/700000: training loss=1.9954, dev loss=2.1351\n",
            "42800/700000: training loss=1.9922, dev loss=2.1371\n",
            "42850/700000: training loss=1.9940, dev loss=2.1290\n",
            "42900/700000: training loss=1.9879, dev loss=2.1350\n",
            "42950/700000: training loss=1.9966, dev loss=2.1451\n",
            "43000/700000: training loss=1.9857, dev loss=2.1379\n",
            "43050/700000: training loss=1.9970, dev loss=2.1323\n",
            "43100/700000: training loss=1.9973, dev loss=2.1378\n",
            "43150/700000: training loss=1.9900, dev loss=2.1276\n",
            "43200/700000: training loss=1.9940, dev loss=2.1426\n",
            "43250/700000: training loss=1.9934, dev loss=2.1372\n",
            "43300/700000: training loss=1.9872, dev loss=2.1335\n",
            "43350/700000: training loss=1.9875, dev loss=2.1341\n",
            "43400/700000: training loss=1.9900, dev loss=2.1319\n",
            "43450/700000: training loss=1.9807, dev loss=2.1241\n",
            "43500/700000: training loss=1.9897, dev loss=2.1318\n",
            "43550/700000: training loss=1.9855, dev loss=2.1291\n",
            "43600/700000: training loss=1.9812, dev loss=2.1334\n",
            "43650/700000: training loss=1.9845, dev loss=2.1296\n",
            "43700/700000: training loss=1.9847, dev loss=2.1236\n",
            "43750/700000: training loss=1.9790, dev loss=2.1264\n",
            "43800/700000: training loss=1.9851, dev loss=2.1232\n",
            "43850/700000: training loss=1.9751, dev loss=2.1219\n",
            "43900/700000: training loss=1.9774, dev loss=2.1224\n",
            "43950/700000: training loss=1.9775, dev loss=2.1197\n",
            "44000/700000: training loss=1.9778, dev loss=2.1145\n",
            "44050/700000: training loss=1.9900, dev loss=2.1363\n",
            "44100/700000: training loss=1.9815, dev loss=2.1149\n",
            "44150/700000: training loss=1.9638, dev loss=2.1090\n",
            "44200/700000: training loss=1.9686, dev loss=2.1108\n",
            "44250/700000: training loss=1.9745, dev loss=2.1138\n",
            "44300/700000: training loss=1.9718, dev loss=2.1176\n",
            "44350/700000: training loss=1.9659, dev loss=2.1219\n",
            "44400/700000: training loss=1.9780, dev loss=2.1158\n",
            "44450/700000: training loss=1.9816, dev loss=2.1155\n",
            "44500/700000: training loss=1.9675, dev loss=2.1116\n",
            "44550/700000: training loss=1.9712, dev loss=2.1134\n",
            "44600/700000: training loss=1.9620, dev loss=2.1124\n",
            "44650/700000: training loss=1.9627, dev loss=2.1096\n",
            "44700/700000: training loss=1.9641, dev loss=2.1155\n",
            "44750/700000: training loss=1.9634, dev loss=2.1051\n",
            "44800/700000: training loss=1.9647, dev loss=2.1164\n",
            "44850/700000: training loss=1.9566, dev loss=2.1070\n",
            "44900/700000: training loss=1.9758, dev loss=2.1128\n",
            "44950/700000: training loss=1.9614, dev loss=2.0959\n",
            "45000/700000: training loss=1.9607, dev loss=2.1048\n",
            "45050/700000: training loss=1.9682, dev loss=2.1140\n",
            "45100/700000: training loss=1.9631, dev loss=2.1046\n",
            "45150/700000: training loss=1.9593, dev loss=2.1063\n",
            "45200/700000: training loss=1.9620, dev loss=2.1025\n",
            "45250/700000: training loss=1.9680, dev loss=2.1092\n",
            "45300/700000: training loss=1.9513, dev loss=2.1050\n",
            "45350/700000: training loss=1.9620, dev loss=2.1113\n",
            "45400/700000: training loss=1.9500, dev loss=2.1039\n",
            "45450/700000: training loss=1.9461, dev loss=2.0976\n",
            "45500/700000: training loss=1.9668, dev loss=2.1101\n",
            "45550/700000: training loss=1.9612, dev loss=2.0999\n",
            "45600/700000: training loss=1.9437, dev loss=2.0988\n",
            "45650/700000: training loss=1.9580, dev loss=2.1057\n",
            "45700/700000: training loss=1.9505, dev loss=2.0932\n",
            "45750/700000: training loss=1.9532, dev loss=2.1055\n",
            "45800/700000: training loss=1.9550, dev loss=2.1001\n",
            "45850/700000: training loss=1.9482, dev loss=2.0886\n",
            "45900/700000: training loss=1.9451, dev loss=2.0971\n",
            "45950/700000: training loss=1.9441, dev loss=2.0894\n",
            "46000/700000: training loss=1.9520, dev loss=2.1000\n",
            "46050/700000: training loss=1.9413, dev loss=2.0853\n",
            "46100/700000: training loss=1.9396, dev loss=2.0937\n",
            "46150/700000: training loss=1.9428, dev loss=2.0950\n",
            "46200/700000: training loss=1.9457, dev loss=2.0906\n",
            "46250/700000: training loss=1.9483, dev loss=2.1027\n",
            "46300/700000: training loss=1.9393, dev loss=2.0948\n",
            "46350/700000: training loss=1.9479, dev loss=2.0969\n",
            "46400/700000: training loss=1.9448, dev loss=2.0971\n",
            "46450/700000: training loss=1.9340, dev loss=2.0856\n",
            "46500/700000: training loss=1.9353, dev loss=2.0837\n",
            "46550/700000: training loss=1.9388, dev loss=2.0918\n",
            "46600/700000: training loss=1.9317, dev loss=2.0902\n",
            "46650/700000: training loss=1.9420, dev loss=2.0959\n",
            "46700/700000: training loss=1.9308, dev loss=2.0886\n",
            "46750/700000: training loss=1.9290, dev loss=2.0866\n",
            "46800/700000: training loss=1.9403, dev loss=2.0965\n",
            "46850/700000: training loss=1.9372, dev loss=2.0895\n",
            "46900/700000: training loss=1.9317, dev loss=2.0812\n",
            "46950/700000: training loss=1.9348, dev loss=2.0846\n",
            "47000/700000: training loss=1.9487, dev loss=2.1002\n",
            "47050/700000: training loss=1.9293, dev loss=2.0816\n",
            "47100/700000: training loss=1.9297, dev loss=2.0924\n",
            "47150/700000: training loss=1.9328, dev loss=2.0852\n",
            "47200/700000: training loss=1.9242, dev loss=2.0830\n",
            "47250/700000: training loss=1.9187, dev loss=2.0731\n",
            "47300/700000: training loss=1.9303, dev loss=2.0824\n",
            "47350/700000: training loss=1.9285, dev loss=2.0811\n",
            "47400/700000: training loss=1.9212, dev loss=2.0779\n",
            "47450/700000: training loss=1.9282, dev loss=2.0775\n",
            "47500/700000: training loss=1.9185, dev loss=2.0693\n",
            "47550/700000: training loss=1.9215, dev loss=2.0788\n",
            "47600/700000: training loss=1.9208, dev loss=2.0799\n",
            "47650/700000: training loss=1.9211, dev loss=2.0854\n",
            "47700/700000: training loss=1.9263, dev loss=2.0864\n",
            "47750/700000: training loss=1.9167, dev loss=2.0743\n",
            "47800/700000: training loss=1.9253, dev loss=2.0783\n",
            "47850/700000: training loss=1.9195, dev loss=2.0758\n",
            "47900/700000: training loss=1.9141, dev loss=2.0665\n",
            "47950/700000: training loss=1.9094, dev loss=2.0655\n",
            "48000/700000: training loss=1.9125, dev loss=2.0724\n",
            "48050/700000: training loss=1.9272, dev loss=2.0752\n",
            "48100/700000: training loss=1.9275, dev loss=2.0787\n",
            "48150/700000: training loss=1.9231, dev loss=2.0812\n",
            "48200/700000: training loss=1.9005, dev loss=2.0698\n",
            "48250/700000: training loss=1.9066, dev loss=2.0623\n",
            "48300/700000: training loss=1.9037, dev loss=2.0683\n",
            "48350/700000: training loss=1.9096, dev loss=2.0728\n",
            "48400/700000: training loss=1.9176, dev loss=2.0736\n",
            "48450/700000: training loss=1.9042, dev loss=2.0688\n",
            "48500/700000: training loss=1.9207, dev loss=2.0722\n",
            "48550/700000: training loss=1.8989, dev loss=2.0690\n",
            "48600/700000: training loss=1.9052, dev loss=2.0666\n",
            "48650/700000: training loss=1.9046, dev loss=2.0601\n",
            "48700/700000: training loss=1.9060, dev loss=2.0638\n",
            "48750/700000: training loss=1.9036, dev loss=2.0599\n",
            "48800/700000: training loss=1.9066, dev loss=2.0546\n",
            "48850/700000: training loss=1.9130, dev loss=2.0619\n",
            "48900/700000: training loss=1.8917, dev loss=2.0549\n",
            "48950/700000: training loss=1.8960, dev loss=2.0656\n",
            "49000/700000: training loss=1.9015, dev loss=2.0661\n",
            "49050/700000: training loss=1.8913, dev loss=2.0536\n",
            "49100/700000: training loss=1.9014, dev loss=2.0608\n",
            "49150/700000: training loss=1.8973, dev loss=2.0596\n",
            "49200/700000: training loss=1.9066, dev loss=2.0656\n",
            "49250/700000: training loss=1.9035, dev loss=2.0618\n",
            "49300/700000: training loss=1.8940, dev loss=2.0547\n",
            "49350/700000: training loss=1.8857, dev loss=2.0512\n",
            "49400/700000: training loss=1.8990, dev loss=2.0581\n",
            "49450/700000: training loss=1.8919, dev loss=2.0574\n",
            "49500/700000: training loss=1.8893, dev loss=2.0532\n",
            "49550/700000: training loss=1.8834, dev loss=2.0551\n",
            "49600/700000: training loss=1.8900, dev loss=2.0400\n",
            "49650/700000: training loss=1.9005, dev loss=2.0513\n",
            "49700/700000: training loss=1.9098, dev loss=2.0613\n",
            "49750/700000: training loss=1.8934, dev loss=2.0451\n",
            "49800/700000: training loss=1.8895, dev loss=2.0546\n",
            "49850/700000: training loss=1.8984, dev loss=2.0596\n",
            "49900/700000: training loss=1.8909, dev loss=2.0501\n",
            "49950/700000: training loss=1.8928, dev loss=2.0496\n",
            "50000/700000: training loss=1.8893, dev loss=2.0574\n",
            "50050/700000: training loss=1.8838, dev loss=2.0547\n",
            "50100/700000: training loss=1.8849, dev loss=2.0524\n",
            "50150/700000: training loss=1.8821, dev loss=2.0501\n",
            "50200/700000: training loss=1.8781, dev loss=2.0487\n",
            "50250/700000: training loss=1.8979, dev loss=2.0532\n",
            "50300/700000: training loss=1.8893, dev loss=2.0584\n",
            "50350/700000: training loss=1.8641, dev loss=2.0380\n",
            "50400/700000: training loss=1.8821, dev loss=2.0515\n",
            "50450/700000: training loss=1.8845, dev loss=2.0467\n",
            "50500/700000: training loss=1.8828, dev loss=2.0461\n",
            "50550/700000: training loss=1.8748, dev loss=2.0449\n",
            "50600/700000: training loss=1.8826, dev loss=2.0473\n",
            "50650/700000: training loss=1.8772, dev loss=2.0439\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-102-9451e1964cf6>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;31m# Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;31m# print(f'{outputs.shape=}, {Yb.shape=}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m   \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m   \u001b[0mrunning_loss_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(lossi)"
      ],
      "metadata": {
        "id": "jjTgq78BCCRd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "1c14564a-265f-4c3d-b150-0ada59bd12d6"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f88149fa500>]"
            ]
          },
          "metadata": {},
          "execution_count": 103
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(lossi_dev)"
      ],
      "metadata": {
        "id": "c6QgPaXDCHha",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "b77459d5-d1d4-4dab-d179-a9aa15efd974"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f8814b3a7d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 104
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "fY3YqGX2TMmv"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample the model"
      ],
      "metadata": {
        "id": "7kTWwn5Q8x4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g_gpu = torch.Generator(device=device).manual_seed(2147483647) # for reproducibility\n",
        "\n",
        "# 0, which is new line char, is a reasonable start (seed) char\n",
        "idx = torch.tensor([[0]]).to(device)\n",
        "new_idx = net.generate(idx, 1000)\n",
        "\n",
        "print(decode(new_idx.view(-1).tolist()))"
      ],
      "metadata": {
        "id": "CGiFf5b280CQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65d0f48f-0ce0-4251-e483-b25ea71ef1db"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pardisher, what I make bust no Mastimentilfengurnt;\n",
            "I not hig-aner; ibuneng nestr og a ncouruteve orusery;\n",
            "Thuank t han akinge mary ury t.\n",
            "\n",
            "Lo not my maleou to no manncuce ot s baje fa locile ran liv s athe then 'gkups ip wf aartss n t!,\n",
            "Ho unin act d y r jiloeta t de t lchtheabe baroiaya thyowinme f m wedhewan 'w ttid s n wh teayik\n",
            "Mls d es s bin:\n",
            "Tain m h tare t wisthes: qen, itong hinglefor, thin y,,,t, hole,\n",
            "w I wen:\n",
            "Thoust'tifly hama ip ou er e, h,ld;w niunoul\n",
            "b e. b es?,at hin.\n",
            "\n",
            "\n",
            "\n",
            "Tho t Whaloun l 'Tito wt ad Iis fy.\n",
            "MI Wha\n",
            "\n",
            "Ai nWhia mcnggeyp h Wathwi'ot Iin:\n",
            "\n",
            "Fhos Ot tho imichy Hca ana an,cli Bacchoun intad e,rthen im ten,ap Obe s mn e t  s y,' osaas An t f m a l f Tou \n",
            "S wo I, ir d n,er,anit m n. I\n",
            "Lap O Oipan t BI s ce,, r wu yac s y sh O h yu am wem ay.\n",
            "\n",
            "P my- pp f Bh I\n",
            "G s,, l Phe aov W g m ul s aiw t t u am y oo y s pt I s oes Oeh aur L M p wtt h th w f wa th he\n",
            "Jl m b iv sw au f u ad i I bi asip to, l su I ennr d, d p hev t Iasr he hanese f f san Ioapsoutk c', cif f hag bu \n"
          ]
        }
      ]
    }
  ]
}