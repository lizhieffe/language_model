{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1ZL2ZKS+WYdzf24CWX14u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizhieffe/language_model/blob/main/Shakespeare_LM_v4_Causal_Mask_%2B_Customized_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "qUDLOQdFcErB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "fRMxsDXmpbMf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "USE_GPU = True\n",
        "\n",
        "BLOCK_SIZE = 96 # Context length: how many chars do we take to predict the next one?"
      ],
      "metadata": {
        "id": "vDHUDjtsph4G"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup GPU"
      ],
      "metadata": {
        "id": "NZIvnPdcps3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_GPU:\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  assert device.type != 'cpu', \"GPU is not available\"\n",
        "else:\n",
        "  device = 'cpu'\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sd95wthQptwk",
        "outputId": "da938539-2006-498c-89c6-24ef348daf37"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g_cpu = torch.Generator(device='cpu').manual_seed(2147483647) # for reproducibility\n",
        "g_device = torch.Generator(device=device).manual_seed(2147483647) # for reproducibility"
      ],
      "metadata": {
        "id": "FDyu_mmdxQNC"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Util Functions"
      ],
      "metadata": {
        "id": "hHD6RNbQpv4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "CXSgUDhjdCxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the names.txt file from github\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "# read in all the words\n",
        "text = open('input.txt', 'r').read()\n",
        "\n",
        "text[:800]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "Uw1N2UjndEDQ",
        "outputId": "172c3239-5129-458c-9745-2b6ec3782b10"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-02 05:49:16--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.10’\n",
            "\n",
            "input.txt.10        100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2023-10-02 05:49:16 (17.8 MB/s) - ‘input.txt.10’ saved [1115394/1115394]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to p\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'total char # = {len(text)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_Lr-G6NdF62",
        "outputId": "15519758-e9d6-4e15-c6c8-e624c5c68469"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total char # = 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build vocabulary"
      ],
      "metadata": {
        "id": "ZFA5J8NjdHj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(''.join(text))))\n",
        "\n",
        "stoi = {c:i for i,c in enumerate(chars)}\n",
        "itos = {i:c for c,i in stoi.items()}\n",
        "\n",
        "assert len(stoi) == len(itos)\n",
        "\n",
        "vocab_size = len(stoi)\n",
        "\n",
        "EXPECTED_VOCAB_SIZE = 65\n",
        "assert vocab_size == EXPECTED_VOCAB_SIZE, f\"expected vocab size = {EXPECTED_VOCAB_SIZE}, got {vocab_size}\""
      ],
      "metadata": {
        "id": "K31lXht9dIww"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# str ---> list of integer\n",
        "encode = lambda str: [stoi[s] for s in str]\n",
        "\n",
        "# list of integer ---> str\n",
        "decode = lambda l: ''.join(itos[i] for i in l)\n",
        "\n",
        "_test_str = \"adb dfd \\n\"\n",
        "assert _test_str == decode(encode(_test_str))"
      ],
      "metadata": {
        "id": "_wjY-ZmAK_Mt"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create DS"
      ],
      "metadata": {
        "id": "tnC-XqfXdKoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n1 = int(len(text) * 0.9)\n",
        "train_data = encode(text[:n1])\n",
        "dev_data = encode(text[n1:])\n",
        "\n",
        "print(f'{len(train_data)=}, {len(dev_data)=}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vU05Ihvwl_b",
        "outputId": "d0ea9729-20db-4db4-8eb7-e6a261d28452"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(train_data)=1003854, len(dev_data)=111540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(data, batch_size: int, block_size: int):\n",
        "  \"\"\" Sample a batch using Causal style. \"\"\"\n",
        "  x_list = []\n",
        "  y_list = []\n",
        "  ix = torch.randint(0, len(data)-block_size, (batch_size,), generator=g_cpu)\n",
        "  X = torch.stack([torch.tensor(data[i:i+block_size]) for i in ix]).to(device)\n",
        "  Y = torch.stack([torch.tensor(data[i+1:i+1+block_size]) for i in ix]).to(device)\n",
        "  return X, Y"
      ],
      "metadata": {
        "id": "eWvuZyaOwXai"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = get_batch(train_data, 16, BLOCK_SIZE)\n",
        "\n",
        "for b in range(3):\n",
        "  it = 0\n",
        "  for t in range(X.shape[1]):\n",
        "    print(f'{decode(X[b, :t+1].tolist())} ---> {decode([Y[b, t].item()])}')\n",
        "    it += 1\n",
        "    if it > 7:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2EL4GRCyDcr",
        "outputId": "95ab0571-d6c4-4c73-dab5-26fee87dd016"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e ---> v\n",
            "ev ---> e\n",
            "eve ---> r\n",
            "ever ---> y\n",
            "every --->  \n",
            "every  ---> d\n",
            "every d ---> a\n",
            "every da ---> n\n",
            "e ---> s\n",
            "es ---> h\n",
            "esh --->  \n",
            "esh  ---> c\n",
            "esh c ---> o\n",
            "esh co ---> m\n",
            "esh com ---> p\n",
            "esh comp ---> l\n",
            "e --->  \n",
            "e  ---> r\n",
            "e r ---> e\n",
            "e re ---> t\n",
            "e ret ---> i\n",
            "e reti ---> r\n",
            "e retir ---> e\n",
            "e retire ---> d\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "bm2SpJ6CfOHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_embd = 768 # the dimensionality of the character embedding vectors\n",
        "d_head = 2 * n_embd # the dim of the transformer's head\n",
        "N_HIDDEN = n_embd * 4 # the number of neurons in the hidden layer of the MLP\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility"
      ],
      "metadata": {
        "id": "lAE6DUi2qcNO"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model class"
      ],
      "metadata": {
        "id": "o2LWaIYbri0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class Net(torch.nn.Module):\n",
        "\n",
        "#   def __init__(self, vocab_size, d_embd, d_hidden, d_head):\n",
        "#     \"\"\"\n",
        "#     Args:\n",
        "#       vocab_size: volabulary size\n",
        "#       d_embd: dim of embedding for the token\n",
        "#       d_hidden: dim of hidden FFN layers\n",
        "#       d_head: dim of the transformer head\n",
        "#     \"\"\"\n",
        "#     super().__init__()\n",
        "\n",
        "#     self.d_head = d_head\n",
        "\n",
        "#     self.embd = torch.nn.Embedding(\n",
        "#         num_embeddings=vocab_size,\n",
        "#         embedding_dim=d_embd\n",
        "#     )\n",
        "\n",
        "#     self.key1 = torch.nn.Linear(d_embd, d_head, bias=False)\n",
        "#     self.query1 = torch.nn.Linear(d_embd, d_head, bias=False)\n",
        "#     self.value1 = torch.nn.Linear(d_embd, d_head, bias=False)\n",
        "\n",
        "#     self.linear1 = torch.nn.Linear(d_head, d_hidden, bias=True)\n",
        "#     self.tanh1 = torch.nn.Tanh()\n",
        "\n",
        "#     # self.linear1 = torch.nn.Linear(d_embd, d_hidden, bias=True)\n",
        "#     # self.tanh1 = torch.nn.Tanh()\n",
        "#     # self.linear2 = torch.nn.Linear(d_hidden, d_hidden, bias=True)\n",
        "#     # self.tanh2 = torch.nn.Tanh()\n",
        "#     # self.linear3 = torch.nn.Linear(d_hidden, d_hidden, bias=True)\n",
        "#     # self.tanh3 = torch.nn.Tanh()\n",
        "#     self.linear_logit = torch.nn.Linear(d_hidden, vocab_size, bias=True)\n",
        "\n",
        "\n",
        "#   def forward(self, x, targets=None):\n",
        "#     \"\"\"\n",
        "#     Args:\n",
        "#       x: (B, T). The input to the model.\n",
        "#       targets: (B, T). When it is not None, the func calculates and return the\n",
        "#         loss in additional to other returned item(s)\n",
        "#     Returns:\n",
        "#       loss: int\n",
        "#       logits: (B, T, C)\n",
        "#     \"\"\"\n",
        "\n",
        "#     T = x.shape[-1]\n",
        "#     tril = torch.tril(torch.ones(T, T)).to(device)\n",
        "\n",
        "#     xemb = self.embd(x) # (B, T, C)\n",
        "\n",
        "#     k = self.key1(xemb)  # (B, T, d_head)\n",
        "#     q = self.query1(xemb) # (B, T, d_head)\n",
        "#     wei = k @ q.transpose(-2, -1) # (B, T, d_head) @ (B, d_head, T) = (B, T, T)\n",
        "#     wei = wei.masked_fill(tril==0, float('-inf')) # (B, T, T)\n",
        "#     wei = wei * self.d_head**-0.5\n",
        "#     wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "\n",
        "#     v = self.value1(xemb) # (B, T, d_head)\n",
        "#     # This makes the y[:,t,:], to have the information of the embedding (v)\n",
        "#     # v[:, u (u<=t), :], but not have the infomration of v[:, w (w>t), :]\n",
        "#     #\n",
        "#     # 1. Spread v's information at all t to y[:, any t, :]\n",
        "#     #\n",
        "#     # Because v is on rhs of @, its information at different T are spread out to\n",
        "#     # the different T in y\n",
        "#     #\n",
        "#     # Think about (T, T) @ (T, d_head) = (T, d_head)\n",
        "#     #\n",
        "#     # a11, a12     b1   a11*b1+a12*b2\n",
        "#     # a21, a22  @  b2 = a21*b1+a22*b2\n",
        "#     #\n",
        "#     # In the result, at T=1, it has b1 and b2, which are the rhs of @'s info at\n",
        "#     # different T\n",
        "#     #\n",
        "#     # 2. Limit y[:, t, :] to not access v[:, w (w>t), :].\n",
        "#     #\n",
        "#     # This is done by `tril`\n",
        "#     y = wei @ v # (B, T, d_head)\n",
        "\n",
        "#     # It doesn't need tril here, because the lhs and rhs doesn't exchange\n",
        "#     # information at different T.\n",
        "#     #\n",
        "#     # Let's say:\n",
        "#     # - input is y (B, T, d_head)\n",
        "#     # - Linear(d_in, d_out) is a matrix l(d_in, d_out), here dim is l(d_head, d_hidden)\n",
        "#     # - result is z (B, T, d_hidden)\n",
        "#     #\n",
        "#     # linear(y) = y @ l = z\n",
        "#     #\n",
        "#     # To simplify, ignore B, T=3, d_head=2, d_hidden=1\n",
        "#     #\n",
        "#     #         y11 y12         y11*l1+y12*l2\n",
        "#     # y @ l = y21 y22 @ l1  = y21*l1+y22*l2\n",
        "#     #         y31 y32   l2    y31*l1+y32*l2\n",
        "#     #\n",
        "#     # We can see z[:, T, :] only contains y[:, T, :]'s info\n",
        "#     #\n",
        "#     # To summarize this and the previous section\n",
        "#     #\n",
        "#     # Z = X @ Y\n",
        "#     #\n",
        "#     # Z[T, :] only contains X[T, :]'s info, doesn't contain X[S != T, :]'s infor\n",
        "#     # Z[T, :] contains Y[at any index, :]'s info\n",
        "#     y = self.linear1(y) # (B, T, d_hidden)\n",
        "#     y = self.tanh1(y)\n",
        "\n",
        "#     logits = self.linear_logit(y) # (B, T, vocab_size)\n",
        "#     logits = logits.view(-1, logits.shape[-1]) # (B, T, vocab_size)\n",
        "\n",
        "#     if targets is None:\n",
        "#       loss = None\n",
        "#     else:\n",
        "#       loss = F.cross_entropy(logits, targets.view(-1))\n",
        "\n",
        "#     return logits.view(-1, T, logits.shape[1]), loss\n",
        "\n",
        "#   def generate(self, idx, max_new_tokens: int):\n",
        "#     \"\"\"Generate new tokens given idx as context.\n",
        "\n",
        "#     Args:\n",
        "#       idx: (B, T)\n",
        "#       max_new_tokens: number of new tokens to generate\n",
        "\n",
        "#     Returns:\n",
        "#       (B,T+max_new_tokens)\n",
        "#     \"\"\"\n",
        "#     for _ in range(max_new_tokens):\n",
        "#       logits, loss = self(idx, targets=None)  # logits: (B, T, vocab_size), loss: None\n",
        "#       logits = logits[:, -1, :] # (B, vocab_size)\n",
        "#       prob = F.softmax(logits, dim=1) # (B, vocab_size)\n",
        "#       idx_next = torch.multinomial(prob, num_samples=1, replacement=False)  # (B, 1)\n",
        "#       idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "#     return idx\n",
        "\n",
        "# net = Net(vocab_size, d_embd=n_embd, d_hidden=N_HIDDEN, d_head=N_HIDDEN).to(device)"
      ],
      "metadata": {
        "id": "N_dU8pf64jMC"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, d_in, d_hidden, d_head):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      vocab_size: volabulary size\n",
        "      d_in: dim of input. If this is the immediate next layer of the token\n",
        "        embedding layer, this is the dim of the embedding for a token.\n",
        "      d_hidden: dim of hidden FFN layers\n",
        "      d_head: dim of the transformer head\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_head = d_head\n",
        "    self.key1 = torch.nn.Linear(d_in, d_head, bias=False)\n",
        "    self.query1 = torch.nn.Linear(d_in, d_head, bias=False)\n",
        "    self.value1 = torch.nn.Linear(d_in, d_head, bias=False)\n",
        "\n",
        "    self.linear1 = torch.nn.Linear(d_head, d_hidden, bias=True)\n",
        "    self.tanh1 = torch.nn.Tanh()\n",
        "    # Project d_hidden back to d_head as output\n",
        "    self.proj = torch.nn.Linear(d_hidden, d_head, bias=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      x: (B, T, C). The input to the model.\n",
        "    Returns:\n",
        "      loss: int\n",
        "      logits: (B, T, C)\n",
        "    \"\"\"\n",
        "\n",
        "    T = x.shape[-2]\n",
        "    tril = torch.tril(torch.ones(T, T)).to(device)\n",
        "\n",
        "    k = self.key1(x)  # (B, T, d_head)\n",
        "    q = self.query1(x) # (B, T, d_head)\n",
        "    wei = k @ q.transpose(-2, -1) # (B, T, d_head) @ (B, d_head, T) = (B, T, T)\n",
        "    wei = wei.masked_fill(tril==0, float('-inf')) # (B, T, T)\n",
        "    wei = wei * self.d_head**-0.5\n",
        "    wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "\n",
        "    v = self.value1(x) # (B, T, d_head)\n",
        "    # This makes the y[:,t,:], to have the information of the embedding (v)\n",
        "    # v[:, u (u<=t), :], but not have the infomration of v[:, w (w>t), :]\n",
        "    #\n",
        "    # 1. Spread v's information at all t to y[:, any t, :]\n",
        "    #\n",
        "    # Because v is on rhs of @, its information at different T are spread out to\n",
        "    # the different T in y\n",
        "    #\n",
        "    # Think about (T, T) @ (T, d_head) = (T, d_head)\n",
        "    #\n",
        "    # a11, a12     b1   a11*b1+a12*b2\n",
        "    # a21, a22  @  b2 = a21*b1+a22*b2\n",
        "    #\n",
        "    # In the result, at T=1, it has b1 and b2, which are the rhs of @'s info at\n",
        "    # different T\n",
        "    #\n",
        "    # 2. Limit y[:, t, :] to not access v[:, w (w>t), :].\n",
        "    #\n",
        "    # This is done by `tril`\n",
        "    y = wei @ v # (B, T, d_head)\n",
        "\n",
        "    # It doesn't need tril here, because the lhs and rhs doesn't exchange\n",
        "    # information at different T.\n",
        "    #\n",
        "    # Let's say:\n",
        "    # - input is y (B, T, d_head)\n",
        "    # - Linear(d_in, d_out) is a matrix l(d_in, d_out), here dim is l(d_head, d_hidden)\n",
        "    # - result is z (B, T, d_hidden)\n",
        "    #\n",
        "    # linear(y) = y @ l = z\n",
        "    #\n",
        "    # To simplify, ignore B, T=3, d_head=2, d_hidden=1\n",
        "    #\n",
        "    #         y11 y12         y11*l1+y12*l2\n",
        "    # y @ l = y21 y22 @ l1  = y21*l1+y22*l2\n",
        "    #         y31 y32   l2    y31*l1+y32*l2\n",
        "    #\n",
        "    # We can see z[:, T, :] only contains y[:, T, :]'s info\n",
        "    #\n",
        "    # To summarize this and the previous section\n",
        "    #\n",
        "    # Z = X @ Y\n",
        "    #\n",
        "    # Z[T, :] only contains X[T, :]'s info, doesn't contain X[S != T, :]'s infor\n",
        "    # Z[T, :] contains Y[at any index, :]'s info\n",
        "    y = self.linear1(y) # (B, T, d_hidden)\n",
        "    y = self.tanh1(y) # (B, T, d_hidden)\n",
        "    y = self.proj(y) # (B, T, d_head)\n",
        "\n",
        "    return y\n",
        "\n",
        "  def generate(self, idx, max_new_tokens: int):\n",
        "    \"\"\"Generate new tokens given idx as context.\n",
        "\n",
        "    Args:\n",
        "      idx: (B, T)\n",
        "      max_new_tokens: number of new tokens to generate\n",
        "\n",
        "    Returns:\n",
        "      (B,T+max_new_tokens)\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, loss = self(idx, targets=None)  # logits: (B, T, vocab_size), loss: None\n",
        "      logits = logits[:, -1, :] # (B, vocab_size)\n",
        "      prob = F.softmax(logits, dim=1) # (B, vocab_size)\n",
        "      idx_next = torch.multinomial(prob, num_samples=1, replacement=False)  # (B, 1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "LT6iVQP14kXa"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, d_embd, d_hidden, d_head):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      vocab_size: volabulary size\n",
        "      d_embd: dim of embedding for the token\n",
        "      d_hidden: dim of hidden FFN layers\n",
        "      d_head: dim of the transformer head\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_head = d_head\n",
        "\n",
        "    self.embd = torch.nn.Embedding(\n",
        "        num_embeddings=vocab_size,\n",
        "        embedding_dim=d_embd\n",
        "    )\n",
        "    self.attn1 = AttentionBlock(vocab_size, d_embd, d_hidden, d_head)\n",
        "    self.attn2 = AttentionBlock(vocab_size, d_head, d_hidden, d_head)\n",
        "    self.linear_logit = torch.nn.Linear(d_head, vocab_size, bias=True)\n",
        "\n",
        "\n",
        "  def forward(self, x, targets=None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      x: (B, T). The input to the model.\n",
        "      targets: (B, T). When it is not None, the func calculates and return the\n",
        "        loss in additional to other returned item(s)\n",
        "    Returns:\n",
        "      loss: int\n",
        "      logits: (B, T, C)\n",
        "    \"\"\"\n",
        "\n",
        "    T = x.shape[-1]\n",
        "    tril = torch.tril(torch.ones(T, T)).to(device)\n",
        "\n",
        "    xemb = self.embd(x) # (B, T, C)\n",
        "\n",
        "    y = self.attn1(xemb)\n",
        "    y = self.attn2(y)\n",
        "\n",
        "    logits = self.linear_logit(y) # (B, T, vocab_size)\n",
        "    logits = logits.view(-1, logits.shape[-1]) # (B, T, vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      loss = F.cross_entropy(logits, targets.view(-1))\n",
        "\n",
        "    return logits.view(-1, T, logits.shape[1]), loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens: int):\n",
        "    \"\"\"Generate new tokens given idx as context.\n",
        "\n",
        "    Args:\n",
        "      idx: (B, T)\n",
        "      max_new_tokens: number of new tokens to generate\n",
        "\n",
        "    Returns:\n",
        "      (B,T+max_new_tokens)\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, loss = self(idx, targets=None)  # logits: (B, T, vocab_size), loss: None\n",
        "      logits = logits[:, -1, :] # (B, vocab_size)\n",
        "      prob = F.softmax(logits, dim=1) # (B, vocab_size)\n",
        "      idx_next = torch.multinomial(prob, num_samples=1, replacement=False)  # (B, 1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "net = Net(vocab_size, d_embd=n_embd, d_hidden=N_HIDDEN, d_head=N_HIDDEN).to(device)"
      ],
      "metadata": {
        "id": "6QusOiY5HEBH"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_total_params = 0\n",
        "\n",
        "for p in net.parameters():\n",
        "  _total_params += p.nelement()\n",
        "\n",
        "print(f'Total params = {_total_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eMi8TezrkY4",
        "outputId": "8dfb4a6b-a480-41ca-f653-c6fd19c851be"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total params = 73400129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define optimizer"
      ],
      "metadata": {
        "id": "m04BZ-9f26w7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "# optimizer = optim.Adam(net.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "5xJsIU2z28us"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "5ObPwkVj3MFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_steps = 700000\n",
        "batch_size = 64\n",
        "lossi = []\n",
        "lossi_dev = []\n",
        "ud = []\n",
        "log_interval = 50\n",
        "\n",
        "running_loss = 0.0\n",
        "running_loss_dev = 0.0\n",
        "running_loss_steps = 0\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # Forward\n",
        "  Xb, Yb = get_batch(train_data, batch_size, BLOCK_SIZE)\n",
        "  optimizer.zero_grad()\n",
        "  logits, loss = net(Xb, targets=Yb)\n",
        "\n",
        "  # Loss\n",
        "  # print(f'{outputs.shape=}, {Yb.shape=}')\n",
        "  running_loss += loss.item()\n",
        "  running_loss_steps += 1\n",
        "\n",
        "  # Eval dev DS\n",
        "  Xb_dev, Yb_dev = get_batch(dev_data, batch_size, BLOCK_SIZE)\n",
        "  logits_dev, loss_dev = net(Xb_dev, targets=Yb_dev)\n",
        "  running_loss_dev += loss_dev.item()\n",
        "\n",
        "  # Update\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # Track status\n",
        "  if i % log_interval == 0:\n",
        "    print(f'{i}/{max_steps}: training loss={running_loss/running_loss_steps:.4f}, dev loss={running_loss_dev/running_loss_steps:.4f}')\n",
        "    running_loss = 0.0\n",
        "    running_loss_dev = 0.0\n",
        "    running_loss_steps = 0\n",
        "\n",
        "  lossi.append(loss.log10().item())\n",
        "  lossi_dev.append(loss_dev.log10().item())"
      ],
      "metadata": {
        "id": "3BtGf_U93M9V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0d95a927-f089-46d5-b6a4-3fdc4434e6f7"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0/700000: training loss=4.1777, dev loss=4.1778\n",
            "50/700000: training loss=4.1522, dev loss=4.1528\n",
            "100/700000: training loss=4.0704, dev loss=4.0746\n",
            "150/700000: training loss=3.8758, dev loss=3.8872\n",
            "200/700000: training loss=3.5036, dev loss=3.5350\n",
            "250/700000: training loss=3.3699, dev loss=3.4177\n",
            "300/700000: training loss=3.3470, dev loss=3.3819\n",
            "350/700000: training loss=3.3295, dev loss=3.3770\n",
            "400/700000: training loss=3.3291, dev loss=3.3656\n",
            "450/700000: training loss=3.3246, dev loss=3.3606\n",
            "500/700000: training loss=3.3146, dev loss=3.3642\n",
            "550/700000: training loss=3.3197, dev loss=3.3579\n",
            "600/700000: training loss=3.3097, dev loss=3.3591\n",
            "650/700000: training loss=3.3105, dev loss=3.3551\n",
            "700/700000: training loss=3.3109, dev loss=3.3501\n",
            "750/700000: training loss=3.3071, dev loss=3.3387\n",
            "800/700000: training loss=3.3127, dev loss=3.3506\n",
            "850/700000: training loss=3.3096, dev loss=3.3486\n",
            "900/700000: training loss=3.3074, dev loss=3.3450\n",
            "950/700000: training loss=3.3091, dev loss=3.3450\n",
            "1000/700000: training loss=3.3073, dev loss=3.3362\n",
            "1050/700000: training loss=3.3079, dev loss=3.3402\n",
            "1100/700000: training loss=3.3036, dev loss=3.3345\n",
            "1150/700000: training loss=3.3025, dev loss=3.3434\n",
            "1200/700000: training loss=3.2981, dev loss=3.3418\n",
            "1250/700000: training loss=3.3022, dev loss=3.3392\n",
            "1300/700000: training loss=3.3079, dev loss=3.3381\n",
            "1350/700000: training loss=3.3044, dev loss=3.3333\n",
            "1400/700000: training loss=3.3040, dev loss=3.3365\n",
            "1450/700000: training loss=3.3012, dev loss=3.3376\n",
            "1500/700000: training loss=3.3011, dev loss=3.3326\n",
            "1550/700000: training loss=3.2966, dev loss=3.3290\n",
            "1600/700000: training loss=3.2952, dev loss=3.3258\n",
            "1650/700000: training loss=3.2899, dev loss=3.3287\n",
            "1700/700000: training loss=3.2923, dev loss=3.3258\n",
            "1750/700000: training loss=3.2854, dev loss=3.3192\n",
            "1800/700000: training loss=3.2865, dev loss=3.3268\n",
            "1850/700000: training loss=3.2847, dev loss=3.3195\n",
            "1900/700000: training loss=3.2741, dev loss=3.3154\n",
            "1950/700000: training loss=3.2754, dev loss=3.3079\n",
            "2000/700000: training loss=3.2572, dev loss=3.2972\n",
            "2050/700000: training loss=3.2469, dev loss=3.2778\n",
            "2100/700000: training loss=3.2362, dev loss=3.2684\n",
            "2150/700000: training loss=3.2183, dev loss=3.2517\n",
            "2200/700000: training loss=3.2055, dev loss=3.2428\n",
            "2250/700000: training loss=3.1977, dev loss=3.2223\n",
            "2300/700000: training loss=3.1815, dev loss=3.2125\n",
            "2350/700000: training loss=3.1617, dev loss=3.1927\n",
            "2400/700000: training loss=3.1430, dev loss=3.1677\n",
            "2450/700000: training loss=3.1165, dev loss=3.1487\n",
            "2500/700000: training loss=3.1067, dev loss=3.1302\n",
            "2550/700000: training loss=3.0998, dev loss=3.1195\n",
            "2600/700000: training loss=3.0927, dev loss=3.1070\n",
            "2650/700000: training loss=3.0887, dev loss=3.0986\n",
            "2700/700000: training loss=3.0780, dev loss=3.0946\n",
            "2750/700000: training loss=3.0777, dev loss=3.0867\n",
            "2800/700000: training loss=3.0688, dev loss=3.0799\n",
            "2850/700000: training loss=3.0704, dev loss=3.0786\n",
            "2900/700000: training loss=3.0596, dev loss=3.0750\n",
            "2950/700000: training loss=3.0657, dev loss=3.0718\n",
            "3000/700000: training loss=3.0662, dev loss=3.0676\n",
            "3050/700000: training loss=3.0567, dev loss=3.0622\n",
            "3100/700000: training loss=3.0554, dev loss=3.0652\n",
            "3150/700000: training loss=3.0530, dev loss=3.0604\n",
            "3200/700000: training loss=3.0509, dev loss=3.0580\n",
            "3250/700000: training loss=3.0450, dev loss=3.0551\n",
            "3300/700000: training loss=3.0481, dev loss=3.0507\n",
            "3350/700000: training loss=3.0382, dev loss=3.0484\n",
            "3400/700000: training loss=3.0302, dev loss=3.0348\n",
            "3450/700000: training loss=3.0258, dev loss=3.0336\n",
            "3500/700000: training loss=3.0237, dev loss=3.0265\n",
            "3550/700000: training loss=3.0269, dev loss=3.0267\n",
            "3600/700000: training loss=3.0128, dev loss=3.0205\n",
            "3650/700000: training loss=3.0056, dev loss=3.0118\n",
            "3700/700000: training loss=3.0010, dev loss=3.0079\n",
            "3750/700000: training loss=2.9989, dev loss=3.0053\n",
            "3800/700000: training loss=2.9943, dev loss=2.9949\n",
            "3850/700000: training loss=2.9847, dev loss=2.9913\n",
            "3900/700000: training loss=2.9826, dev loss=2.9893\n",
            "3950/700000: training loss=2.9801, dev loss=2.9829\n",
            "4000/700000: training loss=2.9747, dev loss=2.9800\n",
            "4050/700000: training loss=2.9662, dev loss=2.9813\n",
            "4100/700000: training loss=2.9725, dev loss=2.9739\n",
            "4150/700000: training loss=2.9683, dev loss=2.9706\n",
            "4200/700000: training loss=2.9638, dev loss=2.9673\n",
            "4250/700000: training loss=2.9609, dev loss=2.9616\n",
            "4300/700000: training loss=2.9581, dev loss=2.9667\n",
            "4350/700000: training loss=2.9565, dev loss=2.9647\n",
            "4400/700000: training loss=2.9582, dev loss=2.9655\n",
            "4450/700000: training loss=2.9590, dev loss=2.9615\n",
            "4500/700000: training loss=2.9565, dev loss=2.9606\n",
            "4550/700000: training loss=2.9541, dev loss=2.9530\n",
            "4600/700000: training loss=2.9511, dev loss=2.9583\n",
            "4650/700000: training loss=2.9456, dev loss=2.9542\n",
            "4700/700000: training loss=2.9485, dev loss=2.9597\n",
            "4750/700000: training loss=2.9468, dev loss=2.9528\n",
            "4800/700000: training loss=2.9464, dev loss=2.9542\n",
            "4850/700000: training loss=2.9432, dev loss=2.9562\n",
            "4900/700000: training loss=2.9461, dev loss=2.9524\n",
            "4950/700000: training loss=2.9448, dev loss=2.9481\n",
            "5000/700000: training loss=2.9377, dev loss=2.9505\n",
            "5050/700000: training loss=2.9411, dev loss=2.9454\n",
            "5100/700000: training loss=2.9379, dev loss=2.9438\n",
            "5150/700000: training loss=2.9387, dev loss=2.9472\n",
            "5200/700000: training loss=2.9359, dev loss=2.9435\n",
            "5250/700000: training loss=2.9384, dev loss=2.9432\n",
            "5300/700000: training loss=2.9366, dev loss=2.9418\n",
            "5350/700000: training loss=2.9369, dev loss=2.9431\n",
            "5400/700000: training loss=2.9336, dev loss=2.9370\n",
            "5450/700000: training loss=2.9314, dev loss=2.9436\n",
            "5500/700000: training loss=2.9342, dev loss=2.9376\n",
            "5550/700000: training loss=2.9263, dev loss=2.9377\n",
            "5600/700000: training loss=2.9323, dev loss=2.9315\n",
            "5650/700000: training loss=2.9277, dev loss=2.9320\n",
            "5700/700000: training loss=2.9257, dev loss=2.9319\n",
            "5750/700000: training loss=2.9256, dev loss=2.9298\n",
            "5800/700000: training loss=2.9249, dev loss=2.9279\n",
            "5850/700000: training loss=2.9158, dev loss=2.9283\n",
            "5900/700000: training loss=2.9176, dev loss=2.9197\n",
            "5950/700000: training loss=2.9118, dev loss=2.9189\n",
            "6000/700000: training loss=2.9078, dev loss=2.9217\n",
            "6050/700000: training loss=2.9092, dev loss=2.9191\n",
            "6100/700000: training loss=2.9067, dev loss=2.9117\n",
            "6150/700000: training loss=2.9076, dev loss=2.9147\n",
            "6200/700000: training loss=2.9039, dev loss=2.9137\n",
            "6250/700000: training loss=2.9033, dev loss=2.9110\n",
            "6300/700000: training loss=2.9049, dev loss=2.9094\n",
            "6350/700000: training loss=2.9003, dev loss=2.9061\n",
            "6400/700000: training loss=2.8981, dev loss=2.9082\n",
            "6450/700000: training loss=2.8955, dev loss=2.9019\n",
            "6500/700000: training loss=2.8953, dev loss=2.9029\n",
            "6550/700000: training loss=2.8930, dev loss=2.8988\n",
            "6600/700000: training loss=2.8892, dev loss=2.8922\n",
            "6650/700000: training loss=2.8865, dev loss=2.8917\n",
            "6700/700000: training loss=2.8870, dev loss=2.8841\n",
            "6750/700000: training loss=2.8842, dev loss=2.8915\n",
            "6800/700000: training loss=2.8835, dev loss=2.8918\n",
            "6850/700000: training loss=2.8765, dev loss=2.8796\n",
            "6900/700000: training loss=2.8681, dev loss=2.8803\n",
            "6950/700000: training loss=2.8708, dev loss=2.8707\n",
            "7000/700000: training loss=2.8661, dev loss=2.8685\n",
            "7050/700000: training loss=2.8676, dev loss=2.8672\n",
            "7100/700000: training loss=2.8593, dev loss=2.8657\n",
            "7150/700000: training loss=2.8628, dev loss=2.8622\n",
            "7200/700000: training loss=2.8539, dev loss=2.8562\n",
            "7250/700000: training loss=2.8534, dev loss=2.8533\n",
            "7300/700000: training loss=2.8459, dev loss=2.8531\n",
            "7350/700000: training loss=2.8461, dev loss=2.8455\n",
            "7400/700000: training loss=2.8434, dev loss=2.8345\n",
            "7450/700000: training loss=2.8384, dev loss=2.8342\n",
            "7500/700000: training loss=2.8265, dev loss=2.8225\n",
            "7550/700000: training loss=2.8287, dev loss=2.8195\n",
            "7600/700000: training loss=2.8216, dev loss=2.8127\n",
            "7650/700000: training loss=2.8134, dev loss=2.8138\n",
            "7700/700000: training loss=2.8120, dev loss=2.8103\n",
            "7750/700000: training loss=2.8104, dev loss=2.8032\n",
            "7800/700000: training loss=2.8087, dev loss=2.7989\n",
            "7850/700000: training loss=2.8031, dev loss=2.7977\n",
            "7900/700000: training loss=2.8051, dev loss=2.7976\n",
            "7950/700000: training loss=2.7985, dev loss=2.7957\n",
            "8000/700000: training loss=2.7901, dev loss=2.7918\n",
            "8050/700000: training loss=2.7910, dev loss=2.7874\n",
            "8100/700000: training loss=2.7921, dev loss=2.7858\n",
            "8150/700000: training loss=2.7868, dev loss=2.7824\n",
            "8200/700000: training loss=2.7872, dev loss=2.7865\n",
            "8250/700000: training loss=2.7822, dev loss=2.7780\n",
            "8300/700000: training loss=2.7804, dev loss=2.7790\n",
            "8350/700000: training loss=2.7845, dev loss=2.7785\n",
            "8400/700000: training loss=2.7777, dev loss=2.7793\n",
            "8450/700000: training loss=2.7772, dev loss=2.7729\n",
            "8500/700000: training loss=2.7745, dev loss=2.7705\n",
            "8550/700000: training loss=2.7744, dev loss=2.7692\n",
            "8600/700000: training loss=2.7727, dev loss=2.7696\n",
            "8650/700000: training loss=2.7742, dev loss=2.7651\n",
            "8700/700000: training loss=2.7701, dev loss=2.7613\n",
            "8750/700000: training loss=2.7711, dev loss=2.7656\n",
            "8800/700000: training loss=2.7667, dev loss=2.7655\n",
            "8850/700000: training loss=2.7683, dev loss=2.7599\n",
            "8900/700000: training loss=2.7658, dev loss=2.7587\n",
            "8950/700000: training loss=2.7603, dev loss=2.7617\n",
            "9000/700000: training loss=2.7645, dev loss=2.7590\n",
            "9050/700000: training loss=2.7556, dev loss=2.7575\n",
            "9100/700000: training loss=2.7583, dev loss=2.7579\n",
            "9150/700000: training loss=2.7553, dev loss=2.7561\n",
            "9200/700000: training loss=2.7573, dev loss=2.7514\n",
            "9250/700000: training loss=2.7518, dev loss=2.7466\n",
            "9300/700000: training loss=2.7531, dev loss=2.7507\n",
            "9350/700000: training loss=2.7543, dev loss=2.7460\n",
            "9400/700000: training loss=2.7535, dev loss=2.7495\n",
            "9450/700000: training loss=2.7482, dev loss=2.7461\n",
            "9500/700000: training loss=2.7550, dev loss=2.7449\n",
            "9550/700000: training loss=2.7447, dev loss=2.7457\n",
            "9600/700000: training loss=2.7407, dev loss=2.7374\n",
            "9650/700000: training loss=2.7410, dev loss=2.7372\n",
            "9700/700000: training loss=2.7429, dev loss=2.7370\n",
            "9750/700000: training loss=2.7451, dev loss=2.7315\n",
            "9800/700000: training loss=2.7401, dev loss=2.7327\n",
            "9850/700000: training loss=2.7391, dev loss=2.7318\n",
            "9900/700000: training loss=2.7318, dev loss=2.7313\n",
            "9950/700000: training loss=2.7317, dev loss=2.7224\n",
            "10000/700000: training loss=2.7247, dev loss=2.7242\n",
            "10050/700000: training loss=2.7284, dev loss=2.7231\n",
            "10100/700000: training loss=2.7278, dev loss=2.7229\n",
            "10150/700000: training loss=2.7225, dev loss=2.7155\n",
            "10200/700000: training loss=2.7221, dev loss=2.7200\n",
            "10250/700000: training loss=2.7205, dev loss=2.7082\n",
            "10300/700000: training loss=2.7204, dev loss=2.7087\n",
            "10350/700000: training loss=2.7153, dev loss=2.7109\n",
            "10400/700000: training loss=2.7187, dev loss=2.7109\n",
            "10450/700000: training loss=2.7176, dev loss=2.7079\n",
            "10500/700000: training loss=2.7139, dev loss=2.7112\n",
            "10550/700000: training loss=2.7158, dev loss=2.7058\n",
            "10600/700000: training loss=2.7092, dev loss=2.7059\n",
            "10650/700000: training loss=2.7085, dev loss=2.7039\n",
            "10700/700000: training loss=2.7080, dev loss=2.7039\n",
            "10750/700000: training loss=2.7093, dev loss=2.7037\n",
            "10800/700000: training loss=2.7059, dev loss=2.7026\n",
            "10850/700000: training loss=2.7050, dev loss=2.6999\n",
            "10900/700000: training loss=2.6988, dev loss=2.7034\n",
            "10950/700000: training loss=2.7023, dev loss=2.6961\n",
            "11000/700000: training loss=2.7023, dev loss=2.6950\n",
            "11050/700000: training loss=2.7032, dev loss=2.6982\n",
            "11100/700000: training loss=2.7060, dev loss=2.6994\n",
            "11150/700000: training loss=2.7013, dev loss=2.6964\n",
            "11200/700000: training loss=2.6989, dev loss=2.6970\n",
            "11250/700000: training loss=2.6949, dev loss=2.6964\n",
            "11300/700000: training loss=2.6967, dev loss=2.6882\n",
            "11350/700000: training loss=2.6964, dev loss=2.6938\n",
            "11400/700000: training loss=2.6928, dev loss=2.6893\n",
            "11450/700000: training loss=2.6963, dev loss=2.6919\n",
            "11500/700000: training loss=2.6927, dev loss=2.6914\n",
            "11550/700000: training loss=2.6892, dev loss=2.6859\n",
            "11600/700000: training loss=2.6858, dev loss=2.6875\n",
            "11650/700000: training loss=2.6934, dev loss=2.6898\n",
            "11700/700000: training loss=2.6823, dev loss=2.6909\n",
            "11750/700000: training loss=2.6896, dev loss=2.6875\n",
            "11800/700000: training loss=2.6885, dev loss=2.6818\n",
            "11850/700000: training loss=2.6863, dev loss=2.6841\n",
            "11900/700000: training loss=2.6807, dev loss=2.6878\n",
            "11950/700000: training loss=2.6802, dev loss=2.6856\n",
            "12000/700000: training loss=2.6821, dev loss=2.6800\n",
            "12050/700000: training loss=2.6830, dev loss=2.6800\n",
            "12100/700000: training loss=2.6834, dev loss=2.6772\n",
            "12150/700000: training loss=2.6845, dev loss=2.6769\n",
            "12200/700000: training loss=2.6774, dev loss=2.6765\n",
            "12250/700000: training loss=2.6752, dev loss=2.6761\n",
            "12300/700000: training loss=2.6770, dev loss=2.6809\n",
            "12350/700000: training loss=2.6750, dev loss=2.6724\n",
            "12400/700000: training loss=2.6737, dev loss=2.6700\n",
            "12450/700000: training loss=2.6713, dev loss=2.6720\n",
            "12500/700000: training loss=2.6743, dev loss=2.6729\n",
            "12550/700000: training loss=2.6762, dev loss=2.6705\n",
            "12600/700000: training loss=2.6745, dev loss=2.6688\n",
            "12650/700000: training loss=2.6773, dev loss=2.6660\n",
            "12700/700000: training loss=2.6709, dev loss=2.6720\n",
            "12750/700000: training loss=2.6708, dev loss=2.6707\n",
            "12800/700000: training loss=2.6719, dev loss=2.6663\n",
            "12850/700000: training loss=2.6687, dev loss=2.6671\n",
            "12900/700000: training loss=2.6655, dev loss=2.6738\n",
            "12950/700000: training loss=2.6611, dev loss=2.6594\n",
            "13000/700000: training loss=2.6641, dev loss=2.6664\n",
            "13050/700000: training loss=2.6686, dev loss=2.6683\n",
            "13100/700000: training loss=2.6662, dev loss=2.6625\n",
            "13150/700000: training loss=2.6642, dev loss=2.6558\n",
            "13200/700000: training loss=2.6592, dev loss=2.6533\n",
            "13250/700000: training loss=2.6608, dev loss=2.6589\n",
            "13300/700000: training loss=2.6614, dev loss=2.6613\n",
            "13350/700000: training loss=2.6606, dev loss=2.6585\n",
            "13400/700000: training loss=2.6583, dev loss=2.6559\n",
            "13450/700000: training loss=2.6580, dev loss=2.6546\n",
            "13500/700000: training loss=2.6567, dev loss=2.6545\n",
            "13550/700000: training loss=2.6513, dev loss=2.6506\n",
            "13600/700000: training loss=2.6558, dev loss=2.6481\n",
            "13650/700000: training loss=2.6569, dev loss=2.6453\n",
            "13700/700000: training loss=2.6539, dev loss=2.6484\n",
            "13750/700000: training loss=2.6527, dev loss=2.6557\n",
            "13800/700000: training loss=2.6468, dev loss=2.6486\n",
            "13850/700000: training loss=2.6488, dev loss=2.6432\n",
            "13900/700000: training loss=2.6517, dev loss=2.6530\n",
            "13950/700000: training loss=2.6410, dev loss=2.6407\n",
            "14000/700000: training loss=2.6448, dev loss=2.6428\n",
            "14050/700000: training loss=2.6489, dev loss=2.6374\n",
            "14100/700000: training loss=2.6476, dev loss=2.6430\n",
            "14150/700000: training loss=2.6494, dev loss=2.6396\n",
            "14200/700000: training loss=2.6461, dev loss=2.6411\n",
            "14250/700000: training loss=2.6430, dev loss=2.6428\n",
            "14300/700000: training loss=2.6449, dev loss=2.6385\n",
            "14350/700000: training loss=2.6453, dev loss=2.6384\n",
            "14400/700000: training loss=2.6446, dev loss=2.6394\n",
            "14450/700000: training loss=2.6426, dev loss=2.6364\n",
            "14500/700000: training loss=2.6387, dev loss=2.6350\n",
            "14550/700000: training loss=2.6413, dev loss=2.6362\n",
            "14600/700000: training loss=2.6377, dev loss=2.6386\n",
            "14650/700000: training loss=2.6331, dev loss=2.6361\n",
            "14700/700000: training loss=2.6340, dev loss=2.6298\n",
            "14750/700000: training loss=2.6396, dev loss=2.6319\n",
            "14800/700000: training loss=2.6340, dev loss=2.6271\n",
            "14850/700000: training loss=2.6376, dev loss=2.6247\n",
            "14900/700000: training loss=2.6302, dev loss=2.6268\n",
            "14950/700000: training loss=2.6335, dev loss=2.6284\n",
            "15000/700000: training loss=2.6314, dev loss=2.6294\n",
            "15050/700000: training loss=2.6277, dev loss=2.6221\n",
            "15100/700000: training loss=2.6295, dev loss=2.6254\n",
            "15150/700000: training loss=2.6246, dev loss=2.6271\n",
            "15200/700000: training loss=2.6284, dev loss=2.6264\n",
            "15250/700000: training loss=2.6278, dev loss=2.6231\n",
            "15300/700000: training loss=2.6285, dev loss=2.6166\n",
            "15350/700000: training loss=2.6297, dev loss=2.6167\n",
            "15400/700000: training loss=2.6246, dev loss=2.6175\n",
            "15450/700000: training loss=2.6263, dev loss=2.6205\n",
            "15500/700000: training loss=2.6240, dev loss=2.6175\n",
            "15550/700000: training loss=2.6212, dev loss=2.6179\n",
            "15600/700000: training loss=2.6240, dev loss=2.6139\n",
            "15650/700000: training loss=2.6212, dev loss=2.6128\n",
            "15700/700000: training loss=2.6215, dev loss=2.6170\n",
            "15750/700000: training loss=2.6226, dev loss=2.6205\n",
            "15800/700000: training loss=2.6209, dev loss=2.6134\n",
            "15850/700000: training loss=2.6154, dev loss=2.6090\n",
            "15900/700000: training loss=2.6208, dev loss=2.6125\n",
            "15950/700000: training loss=2.6145, dev loss=2.6072\n",
            "16000/700000: training loss=2.6143, dev loss=2.6122\n",
            "16050/700000: training loss=2.6157, dev loss=2.6076\n",
            "16100/700000: training loss=2.6103, dev loss=2.6063\n",
            "16150/700000: training loss=2.6092, dev loss=2.6073\n",
            "16200/700000: training loss=2.6108, dev loss=2.6059\n",
            "16250/700000: training loss=2.6120, dev loss=2.6044\n",
            "16300/700000: training loss=2.6130, dev loss=2.6082\n",
            "16350/700000: training loss=2.6075, dev loss=2.6044\n",
            "16400/700000: training loss=2.6077, dev loss=2.6027\n",
            "16450/700000: training loss=2.6086, dev loss=2.6002\n",
            "16500/700000: training loss=2.6081, dev loss=2.6065\n",
            "16550/700000: training loss=2.6088, dev loss=2.6071\n",
            "16600/700000: training loss=2.6078, dev loss=2.6024\n",
            "16650/700000: training loss=2.6084, dev loss=2.6044\n",
            "16700/700000: training loss=2.6073, dev loss=2.6041\n",
            "16750/700000: training loss=2.6017, dev loss=2.5977\n",
            "16800/700000: training loss=2.6021, dev loss=2.6008\n",
            "16850/700000: training loss=2.6008, dev loss=2.6022\n",
            "16900/700000: training loss=2.6018, dev loss=2.5961\n",
            "16950/700000: training loss=2.6010, dev loss=2.6006\n",
            "17000/700000: training loss=2.6008, dev loss=2.5951\n",
            "17050/700000: training loss=2.5985, dev loss=2.5969\n",
            "17100/700000: training loss=2.6008, dev loss=2.5947\n",
            "17150/700000: training loss=2.5996, dev loss=2.5917\n",
            "17200/700000: training loss=2.6007, dev loss=2.5966\n",
            "17250/700000: training loss=2.6039, dev loss=2.5927\n",
            "17300/700000: training loss=2.5973, dev loss=2.5870\n",
            "17350/700000: training loss=2.6001, dev loss=2.5914\n",
            "17400/700000: training loss=2.5930, dev loss=2.5945\n",
            "17450/700000: training loss=2.5933, dev loss=2.5876\n",
            "17500/700000: training loss=2.5931, dev loss=2.5868\n",
            "17550/700000: training loss=2.5982, dev loss=2.5877\n",
            "17600/700000: training loss=2.5904, dev loss=2.5834\n",
            "17650/700000: training loss=2.5911, dev loss=2.5910\n",
            "17700/700000: training loss=2.5950, dev loss=2.5898\n",
            "17750/700000: training loss=2.5926, dev loss=2.5876\n",
            "17800/700000: training loss=2.5935, dev loss=2.5872\n",
            "17850/700000: training loss=2.5885, dev loss=2.5911\n",
            "17900/700000: training loss=2.5883, dev loss=2.5884\n",
            "17950/700000: training loss=2.5868, dev loss=2.5843\n",
            "18000/700000: training loss=2.5896, dev loss=2.5847\n",
            "18050/700000: training loss=2.5924, dev loss=2.5837\n",
            "18100/700000: training loss=2.5893, dev loss=2.5865\n",
            "18150/700000: training loss=2.5827, dev loss=2.5833\n",
            "18200/700000: training loss=2.5858, dev loss=2.5866\n",
            "18250/700000: training loss=2.5833, dev loss=2.5834\n",
            "18300/700000: training loss=2.5898, dev loss=2.5829\n",
            "18350/700000: training loss=2.5869, dev loss=2.5802\n",
            "18400/700000: training loss=2.5875, dev loss=2.5844\n",
            "18450/700000: training loss=2.5823, dev loss=2.5766\n",
            "18500/700000: training loss=2.5831, dev loss=2.5793\n",
            "18550/700000: training loss=2.5872, dev loss=2.5810\n",
            "18600/700000: training loss=2.5839, dev loss=2.5817\n",
            "18650/700000: training loss=2.5825, dev loss=2.5788\n",
            "18700/700000: training loss=2.5819, dev loss=2.5753\n",
            "18750/700000: training loss=2.5847, dev loss=2.5723\n",
            "18800/700000: training loss=2.5816, dev loss=2.5734\n",
            "18850/700000: training loss=2.5810, dev loss=2.5766\n",
            "18900/700000: training loss=2.5787, dev loss=2.5776\n",
            "18950/700000: training loss=2.5773, dev loss=2.5767\n",
            "19000/700000: training loss=2.5781, dev loss=2.5783\n",
            "19050/700000: training loss=2.5724, dev loss=2.5730\n",
            "19100/700000: training loss=2.5762, dev loss=2.5749\n",
            "19150/700000: training loss=2.5745, dev loss=2.5770\n",
            "19200/700000: training loss=2.5789, dev loss=2.5735\n",
            "19250/700000: training loss=2.5780, dev loss=2.5723\n",
            "19300/700000: training loss=2.5775, dev loss=2.5760\n",
            "19350/700000: training loss=2.5788, dev loss=2.5728\n",
            "19400/700000: training loss=2.5711, dev loss=2.5723\n",
            "19450/700000: training loss=2.5752, dev loss=2.5698\n",
            "19500/700000: training loss=2.5745, dev loss=2.5717\n",
            "19550/700000: training loss=2.5767, dev loss=2.5695\n",
            "19600/700000: training loss=2.5711, dev loss=2.5708\n",
            "19650/700000: training loss=2.5710, dev loss=2.5655\n",
            "19700/700000: training loss=2.5719, dev loss=2.5702\n",
            "19750/700000: training loss=2.5759, dev loss=2.5680\n",
            "19800/700000: training loss=2.5727, dev loss=2.5677\n",
            "19850/700000: training loss=2.5718, dev loss=2.5691\n",
            "19900/700000: training loss=2.5763, dev loss=2.5716\n",
            "19950/700000: training loss=2.5742, dev loss=2.5700\n",
            "20000/700000: training loss=2.5700, dev loss=2.5672\n",
            "20050/700000: training loss=2.5699, dev loss=2.5645\n",
            "20100/700000: training loss=2.5730, dev loss=2.5637\n",
            "20150/700000: training loss=2.5672, dev loss=2.5648\n",
            "20200/700000: training loss=2.5646, dev loss=2.5671\n",
            "20250/700000: training loss=2.5717, dev loss=2.5640\n",
            "20300/700000: training loss=2.5666, dev loss=2.5607\n",
            "20350/700000: training loss=2.5672, dev loss=2.5654\n",
            "20400/700000: training loss=2.5642, dev loss=2.5645\n",
            "20450/700000: training loss=2.5662, dev loss=2.5631\n",
            "20500/700000: training loss=2.5639, dev loss=2.5662\n",
            "20550/700000: training loss=2.5646, dev loss=2.5601\n",
            "20600/700000: training loss=2.5631, dev loss=2.5618\n",
            "20650/700000: training loss=2.5617, dev loss=2.5592\n",
            "20700/700000: training loss=2.5617, dev loss=2.5642\n",
            "20750/700000: training loss=2.5605, dev loss=2.5582\n",
            "20800/700000: training loss=2.5626, dev loss=2.5574\n",
            "20850/700000: training loss=2.5668, dev loss=2.5580\n",
            "20900/700000: training loss=2.5646, dev loss=2.5591\n",
            "20950/700000: training loss=2.5620, dev loss=2.5530\n",
            "21000/700000: training loss=2.5614, dev loss=2.5634\n",
            "21050/700000: training loss=2.5623, dev loss=2.5521\n",
            "21100/700000: training loss=2.5556, dev loss=2.5615\n",
            "21150/700000: training loss=2.5590, dev loss=2.5570\n",
            "21200/700000: training loss=2.5616, dev loss=2.5583\n",
            "21250/700000: training loss=2.5549, dev loss=2.5544\n",
            "21300/700000: training loss=2.5627, dev loss=2.5576\n",
            "21350/700000: training loss=2.5589, dev loss=2.5568\n",
            "21400/700000: training loss=2.5568, dev loss=2.5522\n",
            "21450/700000: training loss=2.5613, dev loss=2.5595\n",
            "21500/700000: training loss=2.5598, dev loss=2.5527\n",
            "21550/700000: training loss=2.5562, dev loss=2.5546\n",
            "21600/700000: training loss=2.5531, dev loss=2.5519\n",
            "21650/700000: training loss=2.5560, dev loss=2.5536\n",
            "21700/700000: training loss=2.5494, dev loss=2.5550\n",
            "21750/700000: training loss=2.5554, dev loss=2.5523\n",
            "21800/700000: training loss=2.5546, dev loss=2.5485\n",
            "21850/700000: training loss=2.5542, dev loss=2.5502\n",
            "21900/700000: training loss=2.5510, dev loss=2.5523\n",
            "21950/700000: training loss=2.5549, dev loss=2.5487\n",
            "22000/700000: training loss=2.5523, dev loss=2.5506\n",
            "22050/700000: training loss=2.5489, dev loss=2.5495\n",
            "22100/700000: training loss=2.5497, dev loss=2.5476\n",
            "22150/700000: training loss=2.5530, dev loss=2.5474\n",
            "22200/700000: training loss=2.5491, dev loss=2.5474\n",
            "22250/700000: training loss=2.5528, dev loss=2.5493\n",
            "22300/700000: training loss=2.5518, dev loss=2.5452\n",
            "22350/700000: training loss=2.5511, dev loss=2.5472\n",
            "22400/700000: training loss=2.5501, dev loss=2.5459\n",
            "22450/700000: training loss=2.5464, dev loss=2.5471\n",
            "22500/700000: training loss=2.5454, dev loss=2.5428\n",
            "22550/700000: training loss=2.5456, dev loss=2.5489\n",
            "22600/700000: training loss=2.5499, dev loss=2.5433\n",
            "22650/700000: training loss=2.5476, dev loss=2.5435\n",
            "22700/700000: training loss=2.5449, dev loss=2.5417\n",
            "22750/700000: training loss=2.5444, dev loss=2.5438\n",
            "22800/700000: training loss=2.5445, dev loss=2.5446\n",
            "22850/700000: training loss=2.5418, dev loss=2.5389\n",
            "22900/700000: training loss=2.5456, dev loss=2.5419\n",
            "22950/700000: training loss=2.5432, dev loss=2.5422\n",
            "23000/700000: training loss=2.5439, dev loss=2.5375\n",
            "23050/700000: training loss=2.5465, dev loss=2.5476\n",
            "23100/700000: training loss=2.5430, dev loss=2.5419\n",
            "23150/700000: training loss=2.5402, dev loss=2.5403\n",
            "23200/700000: training loss=2.5444, dev loss=2.5390\n",
            "23250/700000: training loss=2.5427, dev loss=2.5436\n",
            "23300/700000: training loss=2.5400, dev loss=2.5427\n",
            "23350/700000: training loss=2.5436, dev loss=2.5415\n",
            "23400/700000: training loss=2.5387, dev loss=2.5395\n",
            "23450/700000: training loss=2.5432, dev loss=2.5373\n",
            "23500/700000: training loss=2.5381, dev loss=2.5407\n",
            "23550/700000: training loss=2.5431, dev loss=2.5431\n",
            "23600/700000: training loss=2.5391, dev loss=2.5351\n",
            "23650/700000: training loss=2.5326, dev loss=2.5380\n",
            "23700/700000: training loss=2.5352, dev loss=2.5363\n",
            "23750/700000: training loss=2.5372, dev loss=2.5342\n",
            "23800/700000: training loss=2.5421, dev loss=2.5366\n",
            "23850/700000: training loss=2.5390, dev loss=2.5347\n",
            "23900/700000: training loss=2.5371, dev loss=2.5310\n",
            "23950/700000: training loss=2.5360, dev loss=2.5311\n",
            "24000/700000: training loss=2.5391, dev loss=2.5348\n",
            "24050/700000: training loss=2.5374, dev loss=2.5369\n",
            "24100/700000: training loss=2.5392, dev loss=2.5349\n",
            "24150/700000: training loss=2.5355, dev loss=2.5292\n",
            "24200/700000: training loss=2.5319, dev loss=2.5317\n",
            "24250/700000: training loss=2.5370, dev loss=2.5350\n",
            "24300/700000: training loss=2.5362, dev loss=2.5336\n",
            "24350/700000: training loss=2.5355, dev loss=2.5352\n",
            "24400/700000: training loss=2.5321, dev loss=2.5325\n",
            "24450/700000: training loss=2.5322, dev loss=2.5301\n",
            "24500/700000: training loss=2.5371, dev loss=2.5314\n",
            "24550/700000: training loss=2.5354, dev loss=2.5310\n",
            "24600/700000: training loss=2.5368, dev loss=2.5288\n",
            "24650/700000: training loss=2.5301, dev loss=2.5305\n",
            "24700/700000: training loss=2.5283, dev loss=2.5349\n",
            "24750/700000: training loss=2.5328, dev loss=2.5289\n",
            "24800/700000: training loss=2.5320, dev loss=2.5313\n",
            "24850/700000: training loss=2.5306, dev loss=2.5348\n",
            "24900/700000: training loss=2.5313, dev loss=2.5318\n",
            "24950/700000: training loss=2.5316, dev loss=2.5314\n",
            "25000/700000: training loss=2.5328, dev loss=2.5296\n",
            "25050/700000: training loss=2.5366, dev loss=2.5326\n",
            "25100/700000: training loss=2.5277, dev loss=2.5320\n",
            "25150/700000: training loss=2.5305, dev loss=2.5284\n",
            "25200/700000: training loss=2.5306, dev loss=2.5313\n",
            "25250/700000: training loss=2.5287, dev loss=2.5267\n",
            "25300/700000: training loss=2.5320, dev loss=2.5259\n",
            "25350/700000: training loss=2.5279, dev loss=2.5301\n",
            "25400/700000: training loss=2.5281, dev loss=2.5262\n",
            "25450/700000: training loss=2.5318, dev loss=2.5297\n",
            "25500/700000: training loss=2.5287, dev loss=2.5286\n",
            "25550/700000: training loss=2.5313, dev loss=2.5313\n",
            "25600/700000: training loss=2.5274, dev loss=2.5328\n",
            "25650/700000: training loss=2.5300, dev loss=2.5258\n",
            "25700/700000: training loss=2.5285, dev loss=2.5307\n",
            "25750/700000: training loss=2.5276, dev loss=2.5225\n",
            "25800/700000: training loss=2.5238, dev loss=2.5226\n",
            "25850/700000: training loss=2.5295, dev loss=2.5308\n",
            "25900/700000: training loss=2.5230, dev loss=2.5269\n",
            "25950/700000: training loss=2.5263, dev loss=2.5211\n",
            "26000/700000: training loss=2.5260, dev loss=2.5300\n",
            "26050/700000: training loss=2.5249, dev loss=2.5266\n",
            "26100/700000: training loss=2.5231, dev loss=2.5258\n",
            "26150/700000: training loss=2.5252, dev loss=2.5284\n",
            "26200/700000: training loss=2.5214, dev loss=2.5252\n",
            "26250/700000: training loss=2.5256, dev loss=2.5235\n",
            "26300/700000: training loss=2.5219, dev loss=2.5254\n",
            "26350/700000: training loss=2.5206, dev loss=2.5290\n",
            "26400/700000: training loss=2.5242, dev loss=2.5223\n",
            "26450/700000: training loss=2.5200, dev loss=2.5228\n",
            "26500/700000: training loss=2.5268, dev loss=2.5256\n",
            "26550/700000: training loss=2.5216, dev loss=2.5238\n",
            "26600/700000: training loss=2.5237, dev loss=2.5243\n",
            "26650/700000: training loss=2.5270, dev loss=2.5193\n",
            "26700/700000: training loss=2.5211, dev loss=2.5257\n",
            "26750/700000: training loss=2.5223, dev loss=2.5202\n",
            "26800/700000: training loss=2.5174, dev loss=2.5235\n",
            "26850/700000: training loss=2.5214, dev loss=2.5226\n",
            "26900/700000: training loss=2.5222, dev loss=2.5234\n",
            "26950/700000: training loss=2.5266, dev loss=2.5200\n",
            "27000/700000: training loss=2.5221, dev loss=2.5208\n",
            "27050/700000: training loss=2.5175, dev loss=2.5224\n",
            "27100/700000: training loss=2.5191, dev loss=2.5231\n",
            "27150/700000: training loss=2.5221, dev loss=2.5188\n",
            "27200/700000: training loss=2.5193, dev loss=2.5219\n",
            "27250/700000: training loss=2.5213, dev loss=2.5203\n",
            "27300/700000: training loss=2.5151, dev loss=2.5206\n",
            "27350/700000: training loss=2.5212, dev loss=2.5230\n",
            "27400/700000: training loss=2.5186, dev loss=2.5205\n",
            "27450/700000: training loss=2.5158, dev loss=2.5220\n",
            "27500/700000: training loss=2.5199, dev loss=2.5225\n",
            "27550/700000: training loss=2.5176, dev loss=2.5183\n",
            "27600/700000: training loss=2.5164, dev loss=2.5193\n",
            "27650/700000: training loss=2.5172, dev loss=2.5165\n",
            "27700/700000: training loss=2.5215, dev loss=2.5209\n",
            "27750/700000: training loss=2.5185, dev loss=2.5198\n",
            "27800/700000: training loss=2.5157, dev loss=2.5186\n",
            "27850/700000: training loss=2.5178, dev loss=2.5154\n",
            "27900/700000: training loss=2.5123, dev loss=2.5180\n",
            "27950/700000: training loss=2.5114, dev loss=2.5177\n",
            "28000/700000: training loss=2.5137, dev loss=2.5147\n",
            "28050/700000: training loss=2.5161, dev loss=2.5183\n",
            "28100/700000: training loss=2.5186, dev loss=2.5219\n",
            "28150/700000: training loss=2.5132, dev loss=2.5131\n",
            "28200/700000: training loss=2.5171, dev loss=2.5174\n",
            "28250/700000: training loss=2.5180, dev loss=2.5195\n",
            "28300/700000: training loss=2.5114, dev loss=2.5143\n",
            "28350/700000: training loss=2.5137, dev loss=2.5189\n",
            "28400/700000: training loss=2.5157, dev loss=2.5160\n",
            "28450/700000: training loss=2.5127, dev loss=2.5177\n",
            "28500/700000: training loss=2.5122, dev loss=2.5152\n",
            "28550/700000: training loss=2.5133, dev loss=2.5112\n",
            "28600/700000: training loss=2.5129, dev loss=2.5127\n",
            "28650/700000: training loss=2.5113, dev loss=2.5169\n",
            "28700/700000: training loss=2.5152, dev loss=2.5130\n",
            "28750/700000: training loss=2.5119, dev loss=2.5141\n",
            "28800/700000: training loss=2.5130, dev loss=2.5098\n",
            "28850/700000: training loss=2.5132, dev loss=2.5147\n",
            "28900/700000: training loss=2.5139, dev loss=2.5137\n",
            "28950/700000: training loss=2.5140, dev loss=2.5127\n",
            "29000/700000: training loss=2.5094, dev loss=2.5118\n",
            "29050/700000: training loss=2.5091, dev loss=2.5108\n",
            "29100/700000: training loss=2.5104, dev loss=2.5144\n",
            "29150/700000: training loss=2.5121, dev loss=2.5131\n",
            "29200/700000: training loss=2.5092, dev loss=2.5143\n",
            "29250/700000: training loss=2.5128, dev loss=2.5137\n",
            "29300/700000: training loss=2.5058, dev loss=2.5082\n",
            "29350/700000: training loss=2.5122, dev loss=2.5102\n",
            "29400/700000: training loss=2.5106, dev loss=2.5084\n",
            "29450/700000: training loss=2.5111, dev loss=2.5128\n",
            "29500/700000: training loss=2.5126, dev loss=2.5133\n",
            "29550/700000: training loss=2.5099, dev loss=2.5119\n",
            "29600/700000: training loss=2.5074, dev loss=2.5088\n",
            "29650/700000: training loss=2.5049, dev loss=2.5154\n",
            "29700/700000: training loss=2.5086, dev loss=2.5115\n",
            "29750/700000: training loss=2.5082, dev loss=2.5112\n",
            "29800/700000: training loss=2.5091, dev loss=2.5128\n",
            "29850/700000: training loss=2.5073, dev loss=2.5092\n",
            "29900/700000: training loss=2.5069, dev loss=2.5054\n",
            "29950/700000: training loss=2.5056, dev loss=2.5102\n",
            "30000/700000: training loss=2.5069, dev loss=2.5079\n",
            "30050/700000: training loss=2.5061, dev loss=2.5144\n",
            "30100/700000: training loss=2.5075, dev loss=2.5150\n",
            "30150/700000: training loss=2.5004, dev loss=2.5099\n",
            "30200/700000: training loss=2.5064, dev loss=2.5090\n",
            "30250/700000: training loss=2.5013, dev loss=2.5118\n",
            "30300/700000: training loss=2.5051, dev loss=2.5119\n",
            "30350/700000: training loss=2.5075, dev loss=2.5075\n",
            "30400/700000: training loss=2.5041, dev loss=2.5122\n",
            "30450/700000: training loss=2.5047, dev loss=2.5126\n",
            "30500/700000: training loss=2.5038, dev loss=2.5108\n",
            "30550/700000: training loss=2.5004, dev loss=2.5068\n",
            "30600/700000: training loss=2.5033, dev loss=2.5108\n",
            "30650/700000: training loss=2.5070, dev loss=2.5093\n",
            "30700/700000: training loss=2.5028, dev loss=2.5080\n",
            "30750/700000: training loss=2.5044, dev loss=2.5091\n",
            "30800/700000: training loss=2.5022, dev loss=2.5088\n",
            "30850/700000: training loss=2.5043, dev loss=2.5071\n",
            "30900/700000: training loss=2.4990, dev loss=2.5077\n",
            "30950/700000: training loss=2.4980, dev loss=2.5042\n",
            "31000/700000: training loss=2.5016, dev loss=2.5084\n",
            "31050/700000: training loss=2.4998, dev loss=2.5062\n",
            "31100/700000: training loss=2.5004, dev loss=2.5062\n",
            "31150/700000: training loss=2.5005, dev loss=2.5079\n",
            "31200/700000: training loss=2.5011, dev loss=2.5082\n",
            "31250/700000: training loss=2.4965, dev loss=2.5062\n",
            "31300/700000: training loss=2.5000, dev loss=2.5023\n",
            "31350/700000: training loss=2.5002, dev loss=2.4977\n",
            "31400/700000: training loss=2.4965, dev loss=2.4994\n",
            "31450/700000: training loss=2.4981, dev loss=2.5069\n",
            "31500/700000: training loss=2.4992, dev loss=2.5050\n",
            "31550/700000: training loss=2.4984, dev loss=2.4988\n",
            "31600/700000: training loss=2.5002, dev loss=2.4984\n",
            "31650/700000: training loss=2.4963, dev loss=2.5028\n",
            "31700/700000: training loss=2.4963, dev loss=2.5021\n",
            "31750/700000: training loss=2.4965, dev loss=2.4973\n",
            "31800/700000: training loss=2.4928, dev loss=2.5030\n",
            "31850/700000: training loss=2.4942, dev loss=2.4986\n",
            "31900/700000: training loss=2.4938, dev loss=2.4969\n",
            "31950/700000: training loss=2.4971, dev loss=2.4998\n",
            "32000/700000: training loss=2.4932, dev loss=2.5004\n",
            "32050/700000: training loss=2.4961, dev loss=2.4999\n",
            "32100/700000: training loss=2.4944, dev loss=2.4967\n",
            "32150/700000: training loss=2.4939, dev loss=2.5006\n",
            "32200/700000: training loss=2.4916, dev loss=2.4972\n",
            "32250/700000: training loss=2.4950, dev loss=2.4988\n",
            "32300/700000: training loss=2.4902, dev loss=2.5037\n",
            "32350/700000: training loss=2.4927, dev loss=2.4966\n",
            "32400/700000: training loss=2.4902, dev loss=2.4960\n",
            "32450/700000: training loss=2.4921, dev loss=2.4995\n",
            "32500/700000: training loss=2.4941, dev loss=2.5002\n",
            "32550/700000: training loss=2.4920, dev loss=2.4930\n",
            "32600/700000: training loss=2.4898, dev loss=2.4966\n",
            "32650/700000: training loss=2.4950, dev loss=2.4981\n",
            "32700/700000: training loss=2.4903, dev loss=2.4913\n",
            "32750/700000: training loss=2.4907, dev loss=2.4978\n",
            "32800/700000: training loss=2.4919, dev loss=2.4929\n",
            "32850/700000: training loss=2.4907, dev loss=2.4977\n",
            "32900/700000: training loss=2.4865, dev loss=2.5012\n",
            "32950/700000: training loss=2.4906, dev loss=2.4944\n",
            "33000/700000: training loss=2.4901, dev loss=2.4954\n",
            "33050/700000: training loss=2.4873, dev loss=2.4938\n",
            "33100/700000: training loss=2.4884, dev loss=2.4971\n",
            "33150/700000: training loss=2.4917, dev loss=2.4939\n",
            "33200/700000: training loss=2.4890, dev loss=2.4968\n",
            "33250/700000: training loss=2.4882, dev loss=2.4973\n",
            "33300/700000: training loss=2.4893, dev loss=2.4989\n",
            "33350/700000: training loss=2.4878, dev loss=2.4978\n",
            "33400/700000: training loss=2.4885, dev loss=2.4929\n",
            "33450/700000: training loss=2.4905, dev loss=2.4938\n",
            "33500/700000: training loss=2.4878, dev loss=2.4912\n",
            "33550/700000: training loss=2.4834, dev loss=2.4944\n",
            "33600/700000: training loss=2.4851, dev loss=2.4943\n",
            "33650/700000: training loss=2.4852, dev loss=2.4949\n",
            "33700/700000: training loss=2.4841, dev loss=2.4929\n",
            "33750/700000: training loss=2.4856, dev loss=2.4887\n",
            "33800/700000: training loss=2.4877, dev loss=2.4944\n",
            "33850/700000: training loss=2.4872, dev loss=2.4948\n",
            "33900/700000: training loss=2.4881, dev loss=2.4922\n",
            "33950/700000: training loss=2.4875, dev loss=2.4919\n",
            "34000/700000: training loss=2.4853, dev loss=2.4931\n",
            "34050/700000: training loss=2.4900, dev loss=2.4930\n",
            "34100/700000: training loss=2.4828, dev loss=2.4882\n",
            "34150/700000: training loss=2.4838, dev loss=2.4903\n",
            "34200/700000: training loss=2.4861, dev loss=2.4908\n",
            "34250/700000: training loss=2.4856, dev loss=2.4915\n",
            "34300/700000: training loss=2.4900, dev loss=2.4959\n",
            "34350/700000: training loss=2.4816, dev loss=2.4971\n",
            "34400/700000: training loss=2.4850, dev loss=2.4941\n",
            "34450/700000: training loss=2.4850, dev loss=2.4876\n",
            "34500/700000: training loss=2.4818, dev loss=2.4867\n",
            "34550/700000: training loss=2.4854, dev loss=2.4939\n",
            "34600/700000: training loss=2.4826, dev loss=2.4970\n",
            "34650/700000: training loss=2.4853, dev loss=2.4941\n",
            "34700/700000: training loss=2.4802, dev loss=2.4888\n",
            "34750/700000: training loss=2.4788, dev loss=2.4848\n",
            "34800/700000: training loss=2.4852, dev loss=2.4895\n",
            "34850/700000: training loss=2.4790, dev loss=2.4875\n",
            "34900/700000: training loss=2.4799, dev loss=2.4919\n",
            "34950/700000: training loss=2.4836, dev loss=2.4891\n",
            "35000/700000: training loss=2.4803, dev loss=2.4908\n",
            "35050/700000: training loss=2.4792, dev loss=2.4858\n",
            "35100/700000: training loss=2.4763, dev loss=2.4924\n",
            "35150/700000: training loss=2.4836, dev loss=2.4884\n",
            "35200/700000: training loss=2.4786, dev loss=2.4919\n",
            "35250/700000: training loss=2.4814, dev loss=2.4961\n",
            "35300/700000: training loss=2.4839, dev loss=2.4919\n",
            "35350/700000: training loss=2.4829, dev loss=2.4873\n",
            "35400/700000: training loss=2.4822, dev loss=2.4875\n",
            "35450/700000: training loss=2.4797, dev loss=2.4910\n",
            "35500/700000: training loss=2.4838, dev loss=2.4902\n",
            "35550/700000: training loss=2.4834, dev loss=2.4918\n",
            "35600/700000: training loss=2.4825, dev loss=2.4894\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-9451e1964cf6>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;31m# Eval dev DS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0mXb_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYb_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBLOCK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m   \u001b[0mlogits_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXb_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mYb_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m   \u001b[0mrunning_loss_dev\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_dev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-85-750cae2ac2a2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, targets)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxemb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_logit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, T, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-78-dea6106337be>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mtril\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtril\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B, T, d_head)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(lossi)"
      ],
      "metadata": {
        "id": "jjTgq78BCCRd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "1dcb4b95-79c0-4663-ac61-62802874e262"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f88603bd990>]"
            ]
          },
          "metadata": {},
          "execution_count": 89
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(lossi_dev)"
      ],
      "metadata": {
        "id": "c6QgPaXDCHha",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "a57ac7f5-cdc8-4895-d282-d9eb7a3a5911"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f88603f3fd0>]"
            ]
          },
          "metadata": {},
          "execution_count": 90
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "fY3YqGX2TMmv"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample the model"
      ],
      "metadata": {
        "id": "7kTWwn5Q8x4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g_gpu = torch.Generator(device=device).manual_seed(2147483647) # for reproducibility\n",
        "\n",
        "# 0, which is new line char, is a reasonable start (seed) char\n",
        "idx = torch.tensor([[0]]).to(device)\n",
        "new_idx = net.generate(idx, 1000)\n",
        "\n",
        "print(decode(new_idx.view(-1).tolist()))"
      ],
      "metadata": {
        "id": "CGiFf5b280CQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54f0e0cb-d2f5-4259-e1ca-18b32e01c6a6"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ORGANISy ea gialy muit qeeprs aurantherarre co mue cond\n",
            "ICofagitmamathed fadeifou, see oskghe.\n",
            "Ofr\n",
            "Gome f ame,ts, pusp; wof wing hare\n",
            "Fownthalo, ros rove omirtk demareikuir alathake love n inkins.\n",
            "KOL ds g?\n",
            "IULoe:\n",
            "An teind ir a, olit.\n",
            "BCO:\n",
            "AWhourlertin, thors KK:\n",
            "Thy?\n",
            "\n",
            "AROn w thad wis elde ldeaboupr tonor and, oran fofout.\n",
            "\n",
            "CETay t,\n",
            "\n",
            "Anigeso brtlle, of achase te pe seeangelougheat thouts is ft Whe'of dig wno hethesuleherke vey thay, were\n",
            "Whes MHE'sp, pre\n",
            "NUUA:\n",
            "Thay m med donke wnvefome ar me hingnd subuche, d, by, dso IThin\n",
            "Fo sede hthal tsthy chais.\n",
            "Hator fewereat bowake by, jowiny iksenef norefing t'le woungndrang. w ind!\n",
            "Toulorer:\n",
            "\n",
            "UAnithepere, crn f hot my o' whe we y,\n",
            "Vo t e anor touse s m! k, Fel\n",
            "Whateeg, kanth y ffugrarsthour's e nisulon my Wh w he, ore ad d:-n t IN ke, ondorgare m iero o\n",
            "Tod tacome\n",
            "BI:\n",
            "Whe thend tariveg itouces,\n",
            "Hend muu I ndil, tor alkis ESin co pumaldeell h, dugomor:\n",
            "\n",
            "Por bdoo thed.\n",
            "\n",
            "Fry teen.\n",
            "KOimandongesity colinours be.\n",
            "Alfoft\n",
            "IISind Cola\n",
            "IBEThof werll, ha\n"
          ]
        }
      ]
    }
  ]
}